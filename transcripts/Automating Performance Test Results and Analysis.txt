Two to three minutes and then we can get started.
Okay. No problem. It's starting to fill up. Yeah.
People are joining. Thanks, everyone, for joining. Just wait for two more minutes and then we can get started. So we have Joey with us today to discuss about automating the performance test results and analysis. So just wait for two more minutes and then we can get started.
Thank you.
Those who joined you, let us wait for one more minute and then we can get started. So we have Joey with us to discuss about automating the performance test results and analysis. One more minute, then we can get started. Thank you, everyone, for joining this week's Clubhouse meet. Let us get started. So before that, I have a couple of announcements. This session is being recorded, and you can watch it in my YouTube channel today or tomorrow. And also Clubhouse has a new feature called Replace. So this session again, it is getting recorded within Clubhouse. Anytime you can go to this room, and then you can replay the conversation. There are two ways you can listen to this session. And this week's sponsor is Red Line 13. So if you want to run a high scale, affordable geometry test or other test, you can leverage the Red Line 13 platform. So they have been supporting us since the beginning. And there are other couple of sponsors have got. So eventually I'll be announcing them. And Redline 13 is conducting another conference. So if you're interested in speaking in that conference, please let me know I can help you out. And then you can present your performance topics in the conference. And last week, I don't know how many of you are under Grafana Con. So in Grafana Con, there are a couple of updates with respect to K six, so probably we can discuss in the next week's club. I can invite someone from Griffin, a team to explain the features. So those are the couple of updates from my side, and without any further Ado, let's start with this week's title, automating performance test results and analysis. So we have Joey with us. Hi, Joey. How are you?
I'm great. Naveen Kumar. I'm great. How are you?
I'm good. Thank you. Thank you for joining us. Actually, I've been meaning to contact you for the past couple of weeks. Thanks for agreeing for this session.
No problem. Always happy to join these kinds of things, share knowledge and talk about performance, even on a Saturday evening. Thank you for me. It's an evening.
Where are you based? Out of?
I'm based out of the Netherlands, to the south.
Okay, what is the time there right now?
It's 05:00.
Okay, so I'm ruining you a Saturday evening?
Yeah, we've got new lockdown rules. So there's not much to do on a Saturday evening nowadays.
Okay, thank you. So why not you introduce briefly about yourself, and then we can go into that performance testing concepts.
Of course, thank you.
Yeah.
I'm Joy Hendrix. I'm based on the Netherlands. I come from not around Amsterdam or Denhage or Utrecht. I come from the south, so very close to Germany. I live there with my girlfriend and I work for APG, which is a very large bench and clock glamorant. And I am a performance engineer there, and I solve performance engineering problems, help ABG make their applications faster. One of these things that I've been tackling with, and it's also the subject of my pack presentation for the Performance Advisory Council hosted by Jacinte. I think last month that was there also one of the other podcasts that I did with test Guild Joe Collanto, and we talked about automating result analysis. So what I usually always see is that when people automate result analysis, they either use, like a very basic check. They either just push through SLAs or have a bunch of SLA set up that they get defined by the business that meets those criteria. It's good to go. It goes straight to production, or they have some kind of average check where they just check if the average doesn't change too much from the previous test to the current test, and then they'll just pass the production, which is okay. But it's not that great if you really want to be able to make decisions in the pipeline. So as a ways and any kind of requirements that you put in your test, it's obviously very helpful, but usually they are defined quite higher than they are actually needed because of that. Stuff can slip in and performance can degrade even if the service was usually 50 milliseconds on average. Now it's 100 milliseconds on average. It has an estimate of 1 second. Nobody cares. But we did just slow down with 50%. If you move to 50 to 100 milliseconds, usually it doesn't really matter, because what happens with the impact you look at how your users behave. A human contrary measure 50 milliseconds. You can't feel that. But what usually happens is in the grand scheme of things in the entire application. When performance changes, it's not very nice when it changes for the worst and you kind of want to capture changes you don't want to capture. Actually, you don't want to just steer only on SLAs. You kind of want to see when stuff changes, when stuff gets interesting, when you want to look at it, and when stuff starts to change and start acting differently, you kind of want to see what's happening here. Is this bad? Is this good? And that was kind of the moment for me. I need to automate my result analysis, and I need to do this better than just set some SLAs and check if the average is okay. So I started to dive into statistics and to see what is there in statistics to actually use to properly automate an analysis. So statistics. It's difficult if you studied it in school and you have to do exams in it. I envy you such a it's difficult topic, but it brings so much value to the table, especially for performers engineers. And I don't even think that you're going to be tackling very heavy statistics, mathematician stuff within the performance engineering context. But in that field, there's so much goodies that we can use. And we already use a lot every day. Most performance engineers, you hear the terms like the 99 percentile, the 50th median average standard deviation. They're all variation. They're all words that you have seen floating around. Other performance engineers might have said it. Performance engineers always use a certain amount of statistics. So before I started diving into this topic, I was like, yeah, I know what the standard deviation is. I know all these things, the basics. How hard could it be to learn a bit more? Well, pretty damn Lord, there is a lot that comes into play when you go into statistics. So I started researching and I found a couple of values. So in the statistics, they have the sinkle. Statistical distance. It's a way to measure how two populations, how much distance there's between them, or how much change and two numbers or two metrics that I really found very interesting was the wasrstein distance and the Cologne of smelloff distance. There are two distances. And if you watch my back presentation, you get a more visual feel for it, because the KS, I'm going to short them down a bit. The KS, the kilogram of Smirnos distance and the Waspstein distance. They are quite hard to understand if you don't have a visual example in front of you. But it's not impossible, of course. So if you want to have a bit more context, I would recommend to check out my GitHub, where I also have all of this documented or check the recording of the Performance Advisory Council from earlier this month. Sorry, last month. And what we have is, for example, and I'll start with the wastewestein distance. It's basically a metric that gives you the overall change between one test and another test. So if you have the baseline in the benchmark and you want to compare those two, the amount of difference there is between those the Watson distance represents that in a number and represents quite a logical number from zero being absolutely no difference. So if you would calculate that number from two same samples, it will provide you with a zero. So you want to be as close as zero as possible. And that's great, because if the higher that number becomes, the more distance has been introduced, the more has changed. And if we then classify and categorize that number, saying like, zero to one is no change, that's all. Okay, one to three is this amount of change, four to five is this amount of change. You can classify them, and then you can put an impact label or letter rank attached to them. So that's great if you want to classify the overall amount of change, but just having the overall amount being able to quantify how much the overall amount of changes is not enough. You also want to check how much absolute maximum change is there in the test. So it could be if you snip out some of the outliers, you can measure how much is the maximum absolute distance between your baseline and a benchmark. And that's the callagram of Smirnos distance. And if you go on my GitHub and all kinds of see the recording, you'll see that I always make these graphs and they go as an exponential from left to right all the way to the top. And I plot two lines and the biggest amount of difference between those two lines that's usually the maximum distance, aka the Smrdoff value KS value. That's also a great value because it tells you exactly how much the farthest point from point A to B. And together with the Watt print distance these two values, they can actually tell you how much difference there is, and if it's a lot if it's a little bit. But it doesn't tell you if that distance is going to be positive or negative, I just want to be able to detect the distance. That's what I do with my performance test. We run our performance tests. I use Neil load for a bit of context. I run my Neil Load script. It exports my data as raw. Very important. If you do anything with statistics, the data has to be raw in a raw format. So the raw data. If you have the raw data out of your test, then you can calculate the values out of it. I do that using Python. So Neil Lord exports the file. I eat it up with Python. Calculate those distance and I ranked them from S to F, and I'm using the Japanese grading system, which if you're familiar with how the Americans do that with A, it's super good. B same goes with the Japanese one with S being like super excellent, a good and so forth until you reach F, which is absolutely horrible. And those letters don't represent if it's good or bad, it just represents the amount of change for me. And if I have an S, then I know those tests are completely identical. I don't care about them. This can go straight to production, but if I get, for example, a C and there is a significant difference between them, then I want to look at them. Then I want to hold the pipeline and I want to manually look at them and see what's happening here. If it's in my degrees of normalisty, I don't want to do anything. I want to just let it go and it goes straight to production without anybody looking. But once it becomes interesting, I'm going to look so you can imagine if you can put labels on these things and say for example, I have an E, and in my case, I want to manually look at them. So the pipeline holds. I manually look at the test and I say, That's good. We had a performance improvement. We were expecting that. Copy the result, an email, congratulating them on the good work. We managed to improve the system performance, but if it's a slowdown well, then I'll create a defect, do a bit of preemptive analysis on it, and I start contacting the teams and starting to look into the problem, and the pipeline is halted at that point. So that gives me a lot of control in ACIC pipeline on that point. But you can imagine that if you have these letter ranks representing impact, different or change, or how you want to look at it, and you can actually just say like, okay, if I have A-C-I am going to hold the pipeline, I'm going to fail it. I'm going to create a defect that's going to be automatically assigned to these teams. I'll analyze the data a bit further. I'll see. Okay, I have these transactions that are changing the most. I know that's part of this team. I'll make sure that the defect gets created in their name, and I send all the test results with them, and I start contacting that team automatically. That's going to be all done automatically. If you use Python or whatever to just automate that process. You could also say if you have a B, for example, a little amount of change that you could say like, okay, I have a little bit of a to change the benefits for leasing it into production outweigh the impact that I have performance wise. So I'm just going to go to production and I'll solve it in production. I'll just let it go. I'll take the hit. So this gives me this letter ranks to give me the ability to take risk because I know how much risk I'm taking. I can clearly qualify that and say, like, okay, you're taking an S. You don't even have to think about it, or you're going to have a beast little bit amount of regression. Our users aren't going to notice it, so you can just deploy it to production and we'll solve it in production, giving us the benefits of actually releasing the new code and all the new features that are in it, which we usually want, because that also is a benefit in itself. Because of that, we can take risks and solve it in production and monitoring and production, but it all depends on what you want to do with it. You could also say like, okay, if you use the waste time and the chaos value to determine how much distance to it, if the distance is too much or too little. But you can also then use an average to just check if the average is higher or lower. I have a performance improvement. I'm going to take way more risk if I have an F and it's a performance improvement, I can say deploy to production or I can say hold. Wait, I want to look at it app. That is a performance improvement, because how God's name can we have such giant improvements in our response times and still be completely functional and okay. So you can also catch the weird stuff happening. So with these two values, you can really see OK, I have the KS value and waste and I can classify them and you can create something which they call a heuristic. It's basically performance test results are complicated and systems that we are testing are complicated and heuristic can make us make sense of complicated data in a complicated environment very quickly. So we can take rules of thumbs. If we know that these values hang around these kinds of numbers, we are going to take these actions. If the error rate fluctuates above 20%, we are going to take action and fail to build. If the true proof suddenly changes from our test, which shouldn't happen, then we're going to fail the test and we're going to look, why is this happening? If we suddenly have a longer duration of test, then we're going to fail the test and look what happened. So you can set all kinds of actions in these heuristics and you can create basically like a task list that needs to all be verified. If all the tasks go green, it goes to production and selfishing stays in the degrees of normalcy. It just goes to production. And if something happens, that is different. We detect difference with for example, the Washington and KS distance, then we're going to see what are we getting ourselves into? Are we releasing Foster software? Are we releasing a B rank that's faster or we're releasing a B rank that's slower. It completely depends on what you want to do in terms like also what your business and the team, how you want to manage your risk completely depends on how you want to tackle that, but it's a great way to do this, but ranks they show a degree to tell you like it's an asterisk between zero and one. So that leaves a lot of room for error and stuff to still slip in. So you can also use these values together with 99 percentile or 95 percentile or whatever fancies you. You can also add those values into your heuristic and score them. And you can use a score from zero to one. And the more boundaries, for example, are broken by a Washerstein distance or kilogram of distance. The lower the score becomes. The score is quite complicated one to speak about it. But if you visit my GitHub and go through the codes that does this, it's quite clear what it's doing, but with the Waspine and the quantum of metrics that you want to add to it to make it a little bit more tailored to your situation. You can also create a score, and with that score from zero to 100, you could have a more delicate number that you could use to define it more properly. How much is too much for yourself or for your business? And when you want to take risk and when you don't want to take risk, that's one way of tackling it, using it. Using basically a score, or you could use letter ranks. I personally use both. I use a score for myself for myself and the technical teams, and I like to use a letter rank to communicate to the business because I just have to give them a letter. They all understand that it's easy to understand. It's easy to take risk with it, but we're under the hood. We're using the score to make decisions because it's a more accurate number, but the score can we know which letter roughly comes across with which score because we calculate both. And then we can also say you're taking an S or you're taking an A, and you can steer on that kind of a way. So it gives you starting to use more advanced statistics like the waste and calorie move and to add stuff like error checks, like how much errors you are having in your test, the true put duration, all kinds of extra checks that you can add. On top of this, it gives you more control over your test results and allows you to make smarter decisions in very little time, allowing a foster flow to the market to allow a better control on how much risk you're actually taking and quantifying it better instead of and on top of all of this, could all still be the SLAs. If they break, then it's anyway failed. But it does allow you to have more control over what's happening and more impact on what's happening. That's a bit of the gist of automating result analysis. So I went through pretty quickly. If there's any questions, feel free to ask them.
Thank you for explaining your whole GitHub repo content.
That's also what I tried to add at the end. It's a complicated topic, and especially through audio. It's pretty difficult to explain it, and I don't want to bore you guys, but if you go through my GitHub repository, there is more graphical material that makes it a little bit more easier to understand and to see it all in action. So I was really assure you, if you really want to get a bit of grip on this topic to check out my GitHub to see how it's working. Yeah, sure.
I have a couple of questions. Let us start with the discussion, of course. So we have beginners. We have intermediate and we have performance experts in our club. So I want to diversify the costings. So if I am starting as a beginner in performance testing, and why do I care about automation? Because whenever I run the test the tool will give you the results and then I can just analyze with the APM tools, whatever the tools I'm using, so I just compare it and then I can present my observation. This is what typical engineer will do.
Right?
So why I should automate and why not leverage the existing features in the tool?
Good question. I'm also a bit critical about the analytical capabilities of load testing tools. So first, why should you automate? I think there's one word that people really like to use to explain that, but automation makes your life easier. Performance engineering is difficult. It requires a lot of communication back and forth with the teams if I can automate as much away as possible, that's great. I also still have ad hoc projects that I do. I do them once and I never touched them again. So I also run them in your load. What I don't do is I don't use any of the analytical capabilities in any load test too. I generally only look at it when the test is running. If you're familiar with Neil load, for example, and also they're there in LoadRunner when it's running the test. Right. I don't look on Abdomics or Dynatrace or whatever APM I'm using to see the load happening on the system and I'm testing or I check it on Neil Load itself. You see the response time and how the load is flowing from Neil load, so that's as far as I look at the analytical capabilities of the tool, the first thing I usually do is export the data. I export it straight to a file and I either uploaded to a database and I have some automation around that to make my life easier in that aspect. And with that automation, I have it in a database, or if I don't throw it into the database and just have the raw CSV file on my desk. In both cases, I always connected by tool to it. These tooling they're made for graphing, and I'm probably the least sophisticated user of these bi tools because I create scatter plots and some analysis work on these kinds of graphic tools which allow me to really control the data better. I don't really have to start writing something in Python to snip data away or to throw some outliers away. I can do this really quickly. You can also put it in Excel, obviously, but it's a little more complicated. So I always use tableau to really speed that process up. And for me, it's always been a very big asset to create first of all, beautiful graphs, which also speaks a bit to business owners, and I can better visualize what I'm trying to say here and what I always see when we share the grass that it would, for example, get out of Neolad. It would be not at the same quality as what you would get if you were using Power.
Bi or.
Tablet is my favorite from those two tools. It's not the same, however, it's also very okay. It's a low test tool. It's a low thrower, and I want to use a low throat too, and I want it to be as easy as possible. The load I always find is great in that because it makes my life easy in creating a test script fast, so I can just test stuff. And then when the data flows out and load is on the system, the real job begins and trying to figure out what's wrong with the system. And so I want to keep my hands off to load the stool as much as possible. I just want to get load on the system and do that in the best way possible. Obviously. So realistic is done. The analytic capabilities are usually okay, but nothing great, and that's completely okay, in my opinion, for the Lotus tools, so I get five vendors. I'm not really busy with that, but combining with tableau or low runner with tableau is amazing because the graphs that you can push out and the customizability that you have, it's really great.
So I have pinned your profile in the club for those who want to check out, please check the link. It's there in the stage in your clubs app with respect to since we are talking about DevOps and automation, keeping our hands off the process. So if you worked on Jenkins, for example, so they have a feature called trending Performance Trend Report so it will automatically compare bills and then it will display it in a table view with the green and red color arrows. So why can't we leverage just those to see how my performance is being there between bills?
But unfamiliar with what you just mentioned. Also, my knowledge about Jenkins probably doesn't reach that far. Yeah, I can't really answer that question properly, but what I always think about when I use other tooling, for example, than Jenkins, then right. If they do some automated analysis, I don't think they're going to use any of the raw data. They're not going to take the data from my performance test and analyze that and what I always want. I want to be in control because I want to understand what's happening under the hood. I want to know how it's calculated, because if I know that, then I can also keep that in mind how it's calculated. But I see a lot of stuff is aggregated or averaged out. If you go to my get the profile and go to the automated performance analysis, I forgot how the repo was called, but if you scroll down, there's also a big ass gift file and it just shows the average response time and then the scatter plot gets overlaid with it and that overlay on that scatter plot. It's really great because you can see the impact of averages. And that's always what I was thinking when I use out of the box tools or stuff that would automatically analyze it. It either is aggregated. For example, you get like, an average out line, which it's not really that handy to have that because it kind of hides away how your application is behaving. And you can see that in that gift as well. It's a huge gift. You can find it immediately. My page. I think I've spread it around LinkedIn also a couple of times, but it really shows the impact of averages versus raw data.
Yes.
So I just opened the link just going through it. So since you have been using the algorithm, the statistical distance.
Right.
So there are many algorithms out there again in the Python library, we can see a lot of algorithms where we can do these kind of analysis. So why do you pick the statistical distance? So what is the significance of it? Why is that algorithm?
It's not really an algorithm that I'm choosing, right. It's more or less. I'm using an equation I used to waste, mainly because I was looking on how other fields do this. So I saw, for example, how they do this in medical or just in a regular hypothesis testing. And I saw that they're using the cologram off to check the difference in two populations. And I found these two the first. So I found these two. There are a lot more of these kinds of calculations that you can perform. One of these is Kramer distance test. I believe it's also a great value that you can also use. I think you also have Kulbach Leiber convergence test, which is also quite handy, but it all depends on how you want to what you want to do. I tested a lot of these equations out on my data, and I always noticed my data, how it reacted to it. It reacted the best to the waste, however, energy distance, which is very similar to the Kramer for miss distance. They're also great distance metrics to really show the difference between two populations. And also these metrics to answer a question a bit better is also that these two metrics, they are meant to do this. They're meant to represent the amount of distance between them. And that's also what we are doing all the time. We're trying to find how much difference is between one test and the other test. That's a bit of the gist that we've been doing all this time. And statistics kind of offers us out of the box. This is one. This is still very simple statistical stuff. And when you see the cream of a mist, and it's also quite difficult to understand that. Yeah. It's the same reason that you could use a simple students T test if you've paid attention in high school or remember this from statistical classes in College, a student Ttest is also something applicable here that you can use. It uses the median more, which I don't really like because the median again, just as a median variance check, I believe. And it also tells you how the test is doing is also a great way to check. Is it the same? And you can do hypothesis testing on this, too. What? I've noticed most of the statistical distances, the actual methods for the real mathematicians used to know a lot more about the stuff than me. They use this for measuring if there is a significant difference between two populations, but populations of data. So in our case, a baseline and a benchmark performance test. Good. Okay.
So suppose if I have three populations, three runs. So how this.
Works? Yeah. So first, in a sense, how it works on my GitHub, it can only compare two. But what I do is I normalize my data. So I take set A and I normalize it, and I take set B and I normalize it. In sense, you could take your baseline. So if you have three runs, two baseline runs and one benchmark run, then you could compare merge them together to have a more statistically accurate baseline, meaning that we have two stable tests. So whatever numbers come out of the baseline test, they're a bit more accurate because we have a larger population. Same goes with the benchmark. You could make the benchmark more accurate by adding more benchmark runs. In a sense, if even if I have four runs, two benchmark, two baseline, then I could merge them together, make bigger groups out of them, and they'll become more statistically stronger in their comparison in my case. So in reality, what I usually do, I merge baselines, I create bigger baselines, and then I have only one benchmark, and that's how I usually do it. I think there is some improvement to be made there that can be done a bit smarter, but that generally always did the trick for me when regarding three different tests. So what I'm saying? We're tested one against version one and one against version two and one against version three. Then it kind of goes out of the window. There are like hypothesis tests that you can do for this. Like the ANOVA kind of tests that can tell you how much difference there is between three tests. But I kind of felt usually that doesn't really work. In our case, I tried that, but it never really worked for me. If you have three different tests, three different versions, and they're all different individuals in themselves, then this kind of doesn't work. So it only kind of works when you're in that baseline benchmark situation and comparing two populations, unless you're merging populations to become more statistically relevant.
So we have to take baseline and one good run.
That's it.
Yeah. And since that's usually enough. And it also depends on the traffic, which you're getting right. If you're getting absolutely nothing worth of traffic, then the population goes below how much it is. The same goes for transactions. So in your test, your transactions stuff that you're doing in your test. If you have two, for example, 15 transactions, 15 actions in your test. But one is one of those wonky actions that you can maybe do 2040 times in your test. So you have very little measurements about them. And then on the baseline end, you also have, let's say, 200 measurements. This could be a very heavy service call in your test, right. If you're testing on the API level, it could be very heavy service call that we need to have in the test. And it happens, for example, 200 times in the duration of an hour, very low throughput kind of a test. 200 measurements are quite low, quite low for any statistical stuff, it's doable. But it's a low amount. So compared to, for example, a transaction that has 15,000 measurements, they're going to be more accurate. They're going to be more accurately representing the performance in regards to a transaction with 200. So what I usually also do is I calculate scores and ranks on every single transaction so I can see and tell. Okay, listen, this transactions, these groups, they failed, they're in a bad shape, and I'll tell teams that, but I only will do that once the overall calculation fails. If I see a significant amount of distance or change in my overall run, then I'll start looking at the transactions and only then I do the calculations and save that the information. But I only will trigger and start acting on that information once the overall run sales. And that's mainly because there are transactions in there that could go a little bit wonky over time and act silly because five outliers could really skew how that transaction handles. And it depends on the context of what you're testing because it could be that, in my case, 50,000 food or 50,000 an hour is a lot. But if you work for a telco or a big Corporation that has a lot of traffic, it could be that you were like, oh yeah, 50,000. That's like three minutes of traffic for us.
Nothing.
So it really depends on the context as well. That makes it also a bit more difficult or harder to manage your data if you have a lot more or low. Truepoon.
Like me, when you say population, what does it contain actually timestamp, response time.
Usually it's what you get out of Neil load. If you hit the export button in Neil loads, you get the action.
First.
You get the time stamp, obviously. So you get like, either an epoch timestamp or one of those logical string ones, and then you get the response time, or you might even get an ID with them. So like, this was measurement one, like you get an Excel too, and then you get the action that you performed and then you want to have a point after that was a failed or an okay transaction. Did I get an error or not? And then it kind of comes up to what your low test tool can do. Gmeter is great in this because you can basically completely customize this. You could add information to those data points saying like, I want to also see the customer number or I want to see the Http status quote that got back, or I want to see the response and I want to save all of that information and I'll get it into a database and it's in there.
All right.
And then when you hang, for example, a tool like tableau on it, you could visualize and even start seeing which customer types are generating higher response times than other customer types or how many 500 our codes that we get and how many 429 or four or fours that we get, et cetera, or how many customers from a particular type got an error. And it allows you also to do once you can include information into your raw data, like, for example, a customer number, it allows you to do also a lot of cool stuff. For example, hey, I see my error rate go up for a particular group. Something might have flown by functional testing. Maybe I should hold the test and report this, and those decisions can now be made. If you have fourth poor access to the raw data and are automatically analyzing your data, there are all kinds of little extra functions that you can start adding to these groups.
Can I use this solution for any tool?
No, it really doesn't matter which tool you use the script that I wrote, then it's just a Python script. It's a clock that takes two power meters and two arguments, and it wants your baseline and one your benchmark. It wants them as an array Python language at once. Two lists with numbers and those two numbers, they're going to be your response times. And based on that, it's going to calculate the distance, the difference between them, and then it's up to you what you want to add next. And once you have access to all that data, if you have this loaded into Pandas or whatever you want to use, you can start making decisions on it or add your own stuff to it to make it more tailormade. This is not the distance measurements. It would work probably for a lot of applications, but if you want to do, you're going to be forced to customize it a bit if you want to add your own stuff to it, because for me, for example, 2% error rate could be as great as normal. But for you, 2% error rate is probably pretty bad or also normal or absolutely horrible. It depends also a bit on the context of what you need to add. I think for the distance because the distance also uses trust balls to rank and score the test. It could be that if you have an exotic application that behaves a bit more different, the test results will also act different. So when we talk about that, it also happens a lot like this for me. Also, with the project that I started and on GitHub, you can also download one of my test sets and just try it out, see how it works, downloading your own data, and then start tweaking numbers to make it fit in it all. And then you can start building more stuff around it to improve your heuristic, your check or automated quality gate, or however you want to call it or whatever buzzword you want to attach to it, and it allows you to automate it into a pipeline. But if you're using Neola Load Runner got Lin K six or whatever you want to use, it doesn't matter as long as you have access to the raw data and you can eat that raw data up with programming language like Python or R or whatever you want to use. Because these calculations are quite generic, you can look them up how they work. You can start automating them. So the solution is completely independent of whatever load test tool you're using. It's kind of up to your imagination what you really want to do.
Thank you. Let us take questions from the audience if someone has any questions, so please raise your hands so that I can admit to the stage. Thank you for joining. Yeah, please go ahead.
Hey, thanks, Navina. Thanks, Joy, for explaining everything. I have.
Two questions.
Joy, like, is your tool from data? Will it also plot a template kind of report? There is a one question and second thing, it's related to automated like DevOps, whatever you call. What type of tests you compare?
Like.
Regression test only compared or what are the tests like? Let's say any soak test you compare from baseline to benchmark. Yeah, I kind of follow the same preserve baseline to benchmark. But whatever use you implemented, the two I'm implementing through Excel. That's manual process though. So that is the second question.
Excellent question. To start with your first one. I have two types of tests that I've been with you. I have a peak load test. So we just run peak load through my expectation environment. And I have a stress test that I compare that's it all the other stuff. If I do a capacity test, for example, I compare them manually. But those are those tests that I'm going to do ad hoc one's in a blue moon. That's not something that I do automated. So the stuff that I do often regression testing, which is mainly my peak, my stress test. Those are the ones that I have automated and also have automated analysis around. You had another question around. Could you repeat another question again?
Is it your two will plot any result?
It does some stuff because it has some the graphs that I made on most of the graphs, by the way, that are on the GitHub repository, they're generated through Python because I needed to generate a lot of graphs. It would be better to just automate that quickly so it could save me time. It has some capabilities in it to generate graphs, but there's nothing in there that really generates an overall test report from that analysis. It's actually a great valid point that could be added to the project. However, what I do in my daily work is that I have my test results stored in the database and I connect Tableau to it, and that's what I use to really make those dashboards that I also share with the business and I use with the teams analyze them. I generally use them only for myself and my team, but the Devils teams don't really see those dashboards unless there's something wrong and I screenshot it and put it in an email. They generally only have the score and the rank that's all subjugated into one number for them.
Then how your match? If stakeholders want to look into the results, you cannot say that. Yeah, based on your ranking, it is good to go. Where is the proof? I mean, don't take me negative, but if somebody asks, where is the results? So that's where I was asking about the results template. And also I have another question raw results into Tableau, so I never heard of that. Is there any way we can upload the role into Tableau?
Yeah, tableau is just a bi tool that you can download on your desktop. So if you have a CSV file, you can just open it up and I have automation in place that uploads data from your load into an Oracle database, and then I connect with Tableau to an Oracle database and I just eat the data out of there a tablet bi tool and it costs money to have that tool. I think it's $80 a month or something along the lines of that, but you can't quote me on that. But for me it's always been a very big benefit to make these things come again.
But not for our log.
Our connection is dropping a little bit.
Less audible.
Actually.
I think I have four Internet connection all but now.
Yeah, it's fine.
So what I was asking is also tableau for production monitoring and stats, but we never uploaded any CSV file and I never find the way we can upload and plot the graphs over there. So that was one question I.
Built that myself. So I built wrappers around Neil Load and those wrappers around NeoLoad. For example, when new load is done, it will automatically export results in a raw format, and it will then process that file and upload it to a database so that's all automated on my end. So Neil Load, our load runner, is not going to do that out of the box. For my automation purposes. I've wrote Wrappers around it to answer your first question, which also asked about the business and stakeholders want to see my test results. My test results are always open, so if the business owner wants to look at them, they can look at them. Generally, the business trusts me to make that my decisions are good and that my calculations are in order. We also validate in production and we validate and test. So if we see any issues in production that we haven't seen and test, then we're going to either roll back or we're going to solve the issues. So we have a lot of control over what we're doing. And generally I've been never asked to show any data for any client or my current employer to show test results. Like, I want proof that what you did as well. I've always had to trust that I always was trusted on my expertise on that point. But yeah, they're always open.
So if anybody off say.
Can I have the results so I can save them and keep them as test evidence? That's always there. They're all downloadable so the teams can reach it, but they've generally never done it. In my opinion. Then in my case.
One last question, but not least.
So how about.
Approvals in that your organization, the two which you wrote, do they have to go some security checks and approvals and everything?
Any big enterprise. It's always that security like to sniff through your stuff and see if it's good or not. For my case, security checked my code, asked me, hey, you have some passwords in there. How do you manage that? I do that properly and that's it. So that's something typical that happens all the time. For me, that was not a hurdle. It was more like a little bureaucratic tick to get in the box. For me, there was no issue. So any type of security Department, if they would hold you back on that, that would also be holding back a lot of other DevOps stuff and automating your processes.
Yeah, I'll ask one last question.
Obviously, how you run a test anyways? We run any release. So every time we're going to release software, it goes through the pipeline, we test all our builds, and generally it also runs the tests every night, even if there is no new build. So I have multiple testing environments. They all spin up and it's all automated. One is used for daily bolts. One always keeps the current latest production version. One is on the next version. That kind of stuff. So we run basically every time we have a relevant opportunity to test, our tests are running and comparing against the latest production version to see what the difference is. So that's how we do.
That.
So we are currently always testing every day in the night these tests start running and teams in the morning can come back and they can view these results. Like I said, they're all downloadable. Also program some Python stuff that creates reports for them, so they can download those reports and then look into the test results. Generally, they don't really do it. They can only look at the scores and the ranks and they steer on that. And they have access to profiler information from what happened during the test. And they can look in up dynamics to see the tests and debug on that. So that's generally how that works. It's quite a typical process. Yeah.
Thanks a lot, Joy.
No problem at all. Thank you, Sid. Good questions. Hi, Berwin.
Yeah.
Hi.
Good morning, Navin. And good afternoon or. Good evening, Joey.
Thank you, Pavan. Hi. Well.
It'S been a pleasure hearing you with respect to the deep ended automation work that you have actually done. And thanks for sharing with us, because these concepts called scaling testing or measuring the distance between two hubs or notes, and then analyzing and churning out the results. It's something new to us, especially because we are traditionally aware of little laws or Brooks law, the capacity planning laws. How do you plan capacity on the interruption CPU and Q theories and all those things are pretty much understandable. I have quite a few questions. So the first question, what I wanted to know is in this typical Scrum Agile model.
Right.
So in the CI CD pipeline model, the purpose of that is to actually reduce the meantime to release and meantime to deploy. But however, when it comes to the developers, correct. So first of all, there is something called as Aptex core that you are aware of, which basically tells when you compare between different hypothesis of different fields within the release. So the Aptx score basically gives you an insight of whether this particular build is actually deployable or not. So that's a thing. I mean, that's actually globally agreed and globally followed standard. So that's number one, what I was trying to say that is actually two Agnostic and Stack Agnostic.
Okay.
So that's my perception that's number one when we actually bring the same from the contracting view perspective when I actually talk about the computing of what you explained, right. The Kalma Garage Mirror distance and some other distance stats, which probably I'll have to take a look at it later. But my question is, how do I basically explain this to a developer, given the complex of the calculation that's number one and number two. I also see that you created a different index, right? For a given particular event, whether it's a critical or not critical, and what is the action that needs to be done? So my question is, can we take this as a global standard that can be actually implied or applied to the existing performance testing delivery framework? So that's my second question before. So I think probably I will take it further depending on your input on these two questions.
Good point. Yeah. What I've defined and I get the propositor is nowhere close to a global or standard that would work on 99% of all systems and 99% of all contexts. That would be naive of me to say that. However, I do think that how it's defined on my guitar page. It would be applicable to a lot of situations.
But in the end.
It'S also that I said a couple of times, really, it all depends on your context. The thing is that the technique of doing it right. And maybe the numbers that you use that's going to be global. That's going to be everywhere. So it depends on how you want to do it. It's completely up to you if you want your test results to be sure if you want to make decisions on certain actions, it all depends on what you want to do. The numbers and the calculations are going to be always the same. So I would recommend if you're going to implement this yourself is to check how it works for you. It completely depends on your context. Like I said, it's all imagination. You can make it as sharp and as strong as you wanted to have. Like I said, the two calculations that are in there, you can add the error rate to it. You completely define it. You can start. And if your business context, for example, my contacts, I have groups of customers. And if I want to see how a particular change of impact to the group of customers, I can calculate that. And if the business would like to know that impact, I can provide them that it all depends. And when I talk to developers or when I talk to developers, they're usually not really interested in the actual load test. Unless it fails, they're usually more interested in stuff that's in their own sphere of influence. So I do have profiling stuff that also happens during the test and on unit test level, and that's stuff that they look at the most.
But.
For how you want to implement it up to you, it completely depends on the context of the organization.
Yeah, I totally agree with you. Basically, you have given us a high level, probably an in depth skeleton, and it's all up to us how to leverage this as per our requirement. The other question that I had is you said this is not a global standard, but this can be a kind of, I would say, baseline to begin with.
Right. So.
The question that I had is basically this particular calculation. Does this also take the network latency and the number of points of data packet and then the number of nodes and all those things in the future or irrespective of that is just the distance between two nodes or two nodes just takes end to end response time.
Because in the end, that is what matters the most. That's where our customers feel. So I do look at secondary statistics like CPU memory. I have thresholds defined around that as well, but they do not define how I define user experience, because that's the only thing I care about. I care about user experience and giving our customers the best user experience that I can and secondary, I try to create the best user experience possible with the lowest consumption of hardware. That's my secondary point. But first, I want to make sure that our customers have the best experience on our platform as they can get that's my absolute goal. Nothing comes true to that. If I have to keep more CPU towards it, I'll do that. But the customer cannot feel any pain of that. And that's always how we at least define it. And that's also why I only care about end to end response time and everything else is secondary. I still on it. So if I have an increase in CPU usage, suddenly I'll create a defect for it, but it still goes to production because it's not impacting my end to end response time, and I'm going to validate in production as well. If that's not impacting it, if it's impacting, it gets rolled back and fixed. Otherwise the customer. My main goal is to keep the customer happy and to make sure that we as an organization don't feel the pain of performance problem or an outage. And if that means throwing more CPU at it or more memory at a problem temporarily, I'm completely fine with that. Okay.
And my final question. I know that there are other audience waiting, but I would like to just put a very brief question to you. How do you see this integrating or maybe this particular statistical distance integrating with the front end performance testing, especially when you basically look at when you consider tools like Lighthouse or any other available tools out there. So can this be resonated with them or it's just that back end and Http level?
I think it's more of a back end for sort of thing the front end. So when I converted to my own experiences, right, usually the front end and a lot of use cases. It can be tested basically on your own laptop. You can test how your browser is acting towards it and every browser is going to act the same. It might be different if you use Chrome or Firefox or Internet Explorer or Edge, but generally how the browser is going to download that data, it's all going to be the same. So I generally only focus on my back end because that's where I can make the most difference. And if I get an option to do front end stuff that's usually very well already managed by the developers, usually whatever feedback I can give is usually already processed. It maybe because I work with a lot of talented front end developers and they always managed to do very great stuff with that. But I mainly focus on back end and also this mainly only focuses on back end as you need a baseline and a benchmark to compare to which is difficult if you contract the changes. But in fact, if you could figure out how much response times you have from your fund and you can also compare it, it's just math. It's up to you in the end, but I generally only use it for back end. Sorry. Api testing, which could, I think, is generally what most people nowadays are testing.
Okay, great joy. I have followed you and I'll connect with you on the LinkedIn as well. Thanks a lot for your time.
If you have any other further questions, shoot to me on LinkedIn.
Sure, I'll do that. Thank you.
No problem.
Thank you. For one. You can go for next.
Hi, Naveen.
Hi, Joey. How are you doing? I'm very glad to be here again.
Thank you for joining.
First of all, Big Shout out to Joey.
I think you bring up a very important topic of bringing statistics or more advanced statistics to the table of performance testing. I think it's lacking today the comparative analysis, the feed testing that you've been talking about.
Awesome.
Big Shout out to you.
I think it's awesome to bring it up as a topic.
Now.
I'll continue with a sort of experience that I had and I would like to get your reaction to this.
So.
I did some testing to a load balancer. So we basically wanted to test whether the balancing is done correctly and ultimately means that the load is being distributed to the different back end servers equally or as equally as possible. And of course, my way of doing that was to collect, first of all, the information of how much request each back end server got. And then I use the Gene coefficient is used usually in Macroeconomics to measure income inequality or wealth inequality.
That's what it's used for.
And I also use visualization of the Lawrence curve. I'm sorry for the technical stuff, but the point I'm trying to get is when I presented this to colleagues and decision makers, they kind of gave me the look like a madman like, I'm crazy and bringing big stuff that they are not really familiar to, and it's difficult for them to get a hold on this.
So I would like to ask you.
Joey, if this is something that you relate to, if this is something that you have experience and what do you think would be an appropriate way to deal with it?
I feel yet at the same problems. First, my team that I work with, right? They know what they're talking about, and there was a certain amount of respect, obviously to me. So they know what they when I say something and they listen to it and they accept it and they like, okay, cool. That's smart. And generally I don't go to a stakeholder with this. Generally, when I go to a stakeholder, I say I have this equation. I'm not going to explain how they don't really also care, but they hire me for a reason, right? They hire me to help them solve problems. So I have their trust. If I don't have their trust anymore, probably out of a job. The thing is usually that I tell them I have a score and this score is generated like this. If you want to know how it works, I can explain to you it's complicated, but just trust me on my judgment and my team's judgment that this is correct. Usually my property owner knows how this works and they can put in a more political terms, usually to defend this a bit, also at the upper management. But I've never had a point where management wouldn't say, okay, cool. We trust you. We don't trust you. We want to know exactly how it's done. Management always is like, yes. Okay, cool. So this represents this disrepresents this and you've tested it and it's been reviewed among my peers within my organization. So that's how we roll with it. And I completely understand if you can start giving a presentation about all those fancy words that you just mentioned, you're going to have the exact same reaction from most business people. What they mainly always want is a number one number, and they want to know, how does this number scale? That's why they love averages. They love a median or 99 percentile or standard deviation. Consider a large majority of people already know and accept them. It's in their reach of understanding. That's also what I have. If I start conveying numbers to management like a product donor, I am talking averages. I am talking median 99 percentile. But once I'm talking to a developer and where I can talk a bit more loosely to, I'm buffing out the scatter plots and talking with these people because I can talk on a different level with them. And when I talk to peers, I can become even more technical, a bit more geeky on those kinds of topics. But mainly what I always feel is that teams that are trying to dumb it down to the business, so they get a letter rank that is qualified and they're going to be numbers attached to that. If they want to look it up and review it for themselves, then they can do that. So it's always documented for conference or something like that. But generally I try to dumb it down for the business and also decision making to dumb it down. And it's a letter or a score that they use to do that that way, instead of using it, making it too complicated. The same goes for it makes it more understandable for the business. I would say in the case of what you've done with the load balancer, it sounds like really awesome work, but I would try to dumb it down for the business, try to come up with something like a scoring mechanism or rank them, and then tell them this rank just means this. If you want to know how it works. Here's the page. It's all documented. Or I can shoot in a presentation and board you for an hour. If you want to know how it works, it works. So that's been my take on it. All right. Perfect. Great offer, joy. Yeah. Thank you. Thanks.
Yeah.
Thank you.
Great. Thank you. All that.
I think 70 has some questions, I guess.
Yeah. Thanks.
So, yeah, I don't have too many questions, but thank you, Joe. I've been following your blogs and I'm in a network in Linguine as well. So I would like to know how did you. So you started with certain limitations in the existing load testing tools, correct. When you initially started analyzing your results, that journey. So what are the skills which you think would be immensely helpful? Say you identified a problem. And how did you start with coming up with your own solution framework?
I think I'm a bit biased. I talked about this in the past. I've been a Python developer, so I've done Python development, and when I made the switch to performance engineering, it was like, okay, time to go do performance engineering and doing a lot of manual work, and I'm lazy. So I don't like manual work. So I started automating things, and then I started finding challenges. For example, I really get bored by opening that sheet every single time, looking at it and then like, okay, no problem. Close it. That's how it kind of started for me. And then I grasped at the tools that I know best. So I grasped that Python and my development background to start doing that. You can use any language to do this. So if you're comfortable in R, do it in R. If you're comfortable in Python, do it in Python. If you're comfortable doing it in Java, do it in Java, it's completely up to you. You could automate it in Excel. It really doesn't matter for the skills. I would say would be required to do this the most logical way. I would say start learning Python. Check out the statistical packages that they offer, see how they try to learn as a skill. I think if you learn a program like Python, R as an extra skill on top of being a performance tester or a tester or testing in general, it's a great extra skill because it just allows you to automate. Get rid of a lot of toil and boring work so you can boolean out of sight and you can start doing the things that really matter to you. And for me that was working together with the teams and solving issues with them. Instead of doing run test check results, there's nothing wrong done. Spend an hour doing that. And now a program does that for me. And for me, the skill set would be Python or any program language that you're comfortable with to start doing this and then just start from the beginning. Start first, automate the execution of your test and then start the storage of your test results. Always in a raw format. And then you start moving towards statistics and doing stuff around that part. It's a process to go through. And I think learning a programming language would be a great first step to start this. Yeah.
Thanks, Joey.
I think that's it. I guess nobody has any questions. Thank you, Joey, for your time. So how we can scale up if you want to expertise and test yourself analysis any inputs?
Of course, for the absolute beginners, I would start learning a bit about statistics, right? I would first start the basics that we all know and love. I've mentioned them a lot. The median, the standard deviation. Start with that. I think most people coming out of high school will notice, but you just learn what the numbers mean. Start learning a way to automate it. Maybe check out Python, check out or find a way to automate those tasks and start coming up with clever ways. You can read the stuff that I wrote for the pack. You can watch the recording where I explain it with my example of a roombra vacuum cleaner. And you could try to get away into that for the absolute beginners, I would first start understanding the basics around load testing and start working your way up to the other the commonly known stuff and then start working towards the more advanced level stuff that we're doing with automated analysis. But for me, it's always been. The takeaway is learn how to automate and make your life easier so you can do more with two hands. Sure. Yeah.
Thank you, Joey. That's it for this week's club. Thank you for joining. So this session will be available in my YouTube channel and also you can check it out in the clubs up. Thank you for the sponsor Atlante 13 and see you next week with a different topic. Thank you, Joey. Everyone, have a good day.
Thank you, Navin Kumar. Thank you all. Bye bye.
