Thanks for joining this week's clubs meet. So today we are having Lloyd from Neocortex. We'll be talking about Global last mile load testing. So last week we had Leandro about this new book. We interacted and by Monday I will be announcing the winners. So who is going to get the book from Leandro? I will announce in the LinkedIn. So now let's get started highlight how about you?
I'm just great. Thank you very much, even for inviting me to the club house.
Thanks, Lloyd. Just a quick reminder this session is being recorded. Once the session is entered, I will publish the recording in my YouTube channel. So please check it out. Now let's get started so loud. I would like to know first thing about the today's title. So what does Global last my load testing. It's kind of a new term in the industry. Or has it been there for a while? Could you please explain the term?
Yes, of course. And happy to give a little bit of history of the idea. I would say that this is an idea that has been around for some time. There was a company maybe ten or 15 years ago called Gomez that did an early version of what we are doing on PC. So the idea with Gomez was they would use compute power on people's PCs. And so they created a network of load generators, I think all over the world using the compute power of people's PCs. And I believe that they had a reasonably successful business in the early 2000s up to 2013, something like that. And then they were required by a chain of companies. But ultimately, we don't really know about Gomez anymore. That's sort of an ancient history. That was the origin of the idea of using people's computing devices to provide load generators to simulate load on website software. So that's the general idea. What Neocortex is doing, which is similar is we're using the compute power of people's Android phones. So with that in mind, let me describe just a little bit about what Neocortex is. And then you'll be able to see how it is that we came to be working in this space. I didn't conceive of Neocortex originally as a load testing company. I conceived of it as a as a brain like computing company that would use the power of the world's 2.5 billion cell phones. So it was intended to be this massively scalable computing architecture that would use this huge compute resource 2.5 billion cell phones, that's 92 TFLOPS of compute capacity, $60 billion a year worth of revenue at charging at Amazon EC two like prices for that amount of computing. So it's a huge amount of computer incredibly valuable potential recurring revenue if we could harness the power of that computing for real world applications where we could find markets for it. So I told the story on LinkedIn of the thin layer of gold dust covering the Earth. That's one way of looking at what neo critics is doing. All of the cell phones covering the surface. The land surface area of the Earth represents an enormous economic resource, kind of like a thin layer of gold dust. And the trick here with this company is, how do we monetize that? How do we find a way to bring all that compute capacity into a useful state? How do we make it useful and accessible so that it can be applied to real commercial applications? And one of those first applications that we came up with was load testing. It would be a perfect resource for testing the resources of for testing the performance of people's websites, but it would be using the power of real world cell phones. And one of the byproducts of doing this with people cell phones, not phones in data centers, not phones that we purchased, but phones that we rent time on from real phone owners, then is that these phones that would be doing load testing would be located in people's homes. That's an interesting property. That means from a computing point of view, they're not the highest performing computing application. Data centers have extremely high bandwidth trunks that bring enormous amounts of traffic into high performance data centers. And then inside the data center, they have a huge amount of bandwidth for internal communications. Here we're using people's residential Internet connections and WiFi networks to communicate with them. So they're not the most high performance network of devices. What they are is an extremely realistic network for simulating actual consumer traffic. So that gives us this unique resource load generators based on mobile devices in people's homes, in real places where people actually shop from. So that means that's where the name global last mile load testing comes from. We're able to use these devices to do load testing, but they are distributed evenly throughout the world proportional to where the population of the Earth is. So that makes it a global load test, at least potential to be a global load test and a last mile load test because the devices are located in last mile locations. People's living rooms, as Tom Luna Bos told me in 2017, he was the founder of SOS, which is a one of the oldest load testing companies. Tom said, Hi, you can do something we've wanted to do for a long time, which is you can tell us about the response time and performance to the living room in Oklahoma, not to some data center in Oregon West. Right. But to the living room in Oklahoma or a coffee shop in Boise, Idaho. You can tell us what the real end user response time and experience is, which is different than what you would measure as a performance engineer to the nearest high performance data center.
I.
Was there.
Yes. Apart from the load generation use case, is there any other things we can do, like, can we run some small website or some instance using this approach.
Well, it can be done. However, we haven't. So you're talking about using neocortex devices as web servers, for example. Yes. Yes, it can be done. We have not particularly focused on that market, partly because the website storage website server function implies also persistent storage. You're going to store the content that you want to serve on your website. And these phones are not like enormous disk drives that would be associated with a cloud service. You know, Amazon, for example, is not just Amazon EC two. That's not their only service. They also have Amazon S three as one of the basic pillars of their service. So storage and computing are the essential elements of any kind of service like that. What we have fundamentally and initially is the computer aspect, like devices with CPUs that can do a certain kind of computing or agent operations. However, in order to create a network of servers out of them, the persistent storage, we would have to add a new abstraction layer, basically, that would provide redundancy and resilience and persistence, so that if people unplug their phone, for example, or are disconnected, that there would be redundant storage kind of like bit torrent or some of the other distributed storage like file coin, for example, distributed storage that operates with redundancy across a number of different devices, which may come or go. So we see that storage capability and service as a roadmap item that we are coming towards. But that's one of the beauties of last mile load testing is that it doesn't require persistent storage. You're using the devices to go collect data from the server under test by hitting it with a request and measuring the response time. So you're kind of creating data out of the response times and sending that back fairly quickly. But you're not storing it forever, right? You're sending it back.
Got it. Okay. So when it comes to cell phone, people definitely care about their Privacy, correct? Yes, of course, when it comes to near Cortex platform. So how the Privacy of a cell phone user is being they can give yes, of course.
That's a fundamental question. So we have spent an enormous amount of our effort in making our system secure in two directions. So when I say two directions, what I mean is there there are, in principle, two parties that are accessing the phone when our device is in use. In a commercial case, there's the person who owns the phone, and then there's the customer who is trying to do work on the phone. So perhaps using this device as a load generator and a load test, for example. So we have the owner of the phone, and we have the cloud services customer who is using the phone. And, of course, we have to have complete isolation between to the people that are involved in using the processor and the memory. We've spent an enormous amount of time building a secure container that we install onto the phone, and that makes it so that the owner of the phone cannot see what's happening inside the container. That is there's process and file system isolation. So they cannot see what processes are running inside the container, and they cannot see what data is being stored internally inside the container. So that protects the customer from being snooped on or having their data manipulated or process is manipulated by the owner of the phone. And then, of course, we have to protect in the other direction. I think this was the concern that you are implicitly raising, even as we also have to make sure that our customer doesn't do anything to harm the owner of the phone. So for example, they should not be able to break out of that container and be able to Snoop on the content of the phone, to be able to look at the owners videos or access their microphone or camera anything like that. We have to isolate them entirely inside the container. So we have to have two way isolation with both the owner of the phone and the customer unable to see what each other is doing. And we've achieved that. But that was one of the biggest investments we had to make as a company. It took us about two years, about a million dollars to build a full secure Linux container that would be isolated in both directions.
So every cell phone is a Linux container or in neocortex platform.
That's right. That's right.
So what are the limits you set when it comes to CPU or memory? For example, Pixel Five has a Octacore and GB of Ram. So what is the maximum CPU limit you will be utilizing from the end user?
I.
Believe that we've set it at maximum 95 CPU. We don't allow the container to completely take over the phone and disable the UI. For example, that would be bad if it did that. However, one of the things that we haven't mentioned yet, which is pretty important here is that we only use the phone when the device is plugged in to the charger and when it's connected to WiFi. So the intention is that we're only using the phone when the phone owner is not using it. Now, there's two scenarios when we expect that the phone owner would be allowing access to the first one that comes to mind is the idea that when you charge your phone at night, when you're sleeping, you're not using your phone, and that's when you're connected to your own internal WiFi network and you are plugged in to the charger. So those two signals plugged into charger and connected to WiFi are an indicator that the phone would be available and could be used by a customer process. And so we structured it from the beginning that that's when people would be paid, phone owners would be paid when it would be available, when the phone would be available to be used for a compute instance in a service similar to Amazon EC two. Now that's not the only time, though, that the phone might be available, not the only scenario. The other scenario is we recognize that people have more than one phone. Often people will have their previous cell phone and maybe two or three generations of cell phones in the past, sitting in a closet or sitting in a bedside table drawer under utilized or not used at all. So we allowed our providers to have not only their main phone, which would be available typically 8 hours a day at night when they are sleeping. But we also allow them to have their spare phones and up to five phones total available to us. And now those spare phones are particularly valuable to us, and they are particularly valuable to the phone owners. Here's why we pay proportional to the amount of time that they are available to us, connected to WiFi and connect the charger. Well, the main phone for those users are available one third of the time, whereas the spare phones are plugged in all the time. So that means that the people make three times as much money because they're being paid 24 hours a day for their phones at the same hourly rate. So they love those spare phones because they get paid so much more for having them. And we love those spare phones. We really love those spare phones because those are the ones that have high reliability. They are unlikely to be unplugged after being available to us for 8 hours. They're available 24 hours a day, seven days a week. So they become our most reliable service. And we've measured we actually measure the device availability rating, which is essentially if the device has been available for the last 48 hours, consistently never unplugged, no breakages in the service, just always available without ever disconnecting. And so that's how we can offer data center like reliability on devices which may come or go. We keep track of the ones which are coming and going, and we give them a low score and the ones that get the high score. Those are the ones that we prioritize when we're giving devices to customers.
So.
If a customer comes to you and they want to test with multiple regions last mile, right. So they can easily upload the test plan, and then they can easily test correct that's the whole idea. And suppose if a customer wants, I want to test four G or five G. So whether the new text has support for that.
So no, we don't. Because of the characteristics that I described earlier, we want to make sure that we're not using up people's data plans. And so that's one of the things we had to give up was that we do provide measurements of response time to last mile, but through residential networks and through WiFi. At least we don't currently allow people to use their cell phone data connection. So they're four G, five G, three G connections. So you could receive that as potentially one limitation. However, we still think that that is adequately described as a last mile response time because it truly is measuring. If I was shopping. Now, in the scenario that I'm in right now, I'm at my home connected to WiFi. This is in fact where I do most of my shopping is at home connected to WiFi in these scenarios. But anyway, to answer your question, no, we don't currently offer that. We offer only WiFi connections and through the residential data network. We have had people ask for that, though I think it would be fair to say that it's a potential roadmap item. If we had customers asking specifically for that, we would be willing to enable that additional service. Phone owners have occasionally asked us. He said, Basically, I've got a all you can eat data plan. It wouldn't cost me anymore. Don't worry. You wouldn't be running me out of data for my data plan because it's all you can eat. So I wouldn't mind offering my service that way. So for those people, we would be willing to enable them to make money all the time and therefore enable measurement of real cell phone networks. But we didn't do that initially.
Got it. Okay, we have one and find it. Hi, you guys, if you have any questions, please, you can interact with Slide.
You can.
In this.
Hello? Yes, I can hear you.
F.
One, please go ahead.
Sorry. Okay, so thanks Navin again for bringing me into this room and introducing me to this wonderful set of people. Hi, all, I think we are communicating on the LinkedIn and Congratulations for you probably bringing in this technology and performance testing platform solution provider, which could be a game changer down the line, but Interestingly for the first five minutes, even though I was a part of this room, but I could not hear the conversation properly. I thought the conversation did not begin but had to quit and rejoin the room back again. So whatever the understanding I have currently based on the description that you just provided, I just had maybe two or three questions, you know, currently talking about the typical classic load up no testing solution provider, which are in the market currently today. Either it could be place meter or five or any other provider. So what would be the cost of ownership and the return of investment? Let's say you know about 500 users. Let's take a base example of 500 concurrent users and you give us a clear description of what are the unique features, the comparison between the distributed last mile low testing services that basically these now the cloud Ops load injector providers here in the this industry for the past, maybe a decade compared to the Cortex service provider. Yes, that's one thing that I wanted to check because since we are working in a service company. What bites down to us is the return of investment and how he would be for us when we compare it with owning any other service provider I want to check. And the second thing is, you explained us about mobile load testing with distributed load injector cops located across different demographic locations. So I just wanted to check because, as you clearly pointed out, the realistic production mobile testing. It's all about testing on the mobile data, but not exactly on the WiFi that you are aware of. Then you clearly said that that would be the future that is added. But however, you know, the challenges of, you know, the Latin and ISP providers and challenges across different geographical locations. So what I was probably to the impression that, you know, we are using maybe mobile phone, probably there could be a satellite driven load generators or something like that is obviously not WiFi related. So that was my impression, the impression that I had so that's one clarification that I that I wanted from you.
And.
The third question I had is actually, I will definitely visit the website of the probably reach out if in case anything or any requirements down the line. But what are the key differentiators of cloud native and cloud enabled performance services and integration? You know, Iration areas with respect to observability or with respect to deep diagnostics of the APM side rail reliability or to engineering that Neocortex can offer. You know, where I mean, are we talking about any interesting plugin play a kind of solution in here?
Yeah.
So these are the three questions that I want to.
Very good. Very good. Okay. Thank you very much, Arjun. So, yes. But I'd be happy to address those three questions. So the first one is about costs. To phrase it as a total cost of ownership. The the way that I normally think about it, its just a direct comparison between the costs for a given test for comparing between using the Neocortex network of devices versus using the more common scenario of a single large server or a small number of large servers in a data center like AWS, Azure or Google Cloud platform. So we're comparing the cost of using many small devices on the Neocortex network versus the cost of using a large a small number of large powerful devices in data centers. So there comes down essentially to the cost of instance time on the Cortex devices versus the cost of corresponding compute capacity, instance time on data center devices. And for that, we devised our pricing strategy originally in a way that was intended to be that we would price our devices at approximately half the cost of Amazon. As for a given level of compute capacity and the way that we set that pricing and benchmark it was by doing a computing task. Originally, we did this in 2016. We benchmarked the best devices that we had at the time. Galaxy S Six was available at that time, which was one of the first Octacore processors with a very high performance.
A.
57 Arm processor. So it was a leap forward in processing power, so it made it a good candidate to be a good server. And so we benchmark that on a rendering task against a PC and against AWS servers and set our price. Basically, there were some fairly complicated benchmarking processes. We found that Disarm device could do rendering at approximately a third the speed of a full PC using a Quadcore I 70 C. So we took all that into account, and we essentially set the price of our best Gold devices. Gold ranked devices at five and a half cents per hour. And with the expectation and the belief that the performance of those devices was comparable to an Amazon, I believe it was a C Four not large, which was priced at $0.10 per hour approximately. We set the compute capacity for one half the price, at least at that time of an AWS device priced at $0.10 per hour. So that was our original intention was to set ourselves at a price discount relative to the comparable amount of computing that you could get at AWS. And the reason, of course, there is that we wanted to be less expensive than the the leading provider AWS, so that anyone who is considering trying us would experience a discount approximately discount to switch to us or to evaluate with us. And then the other aspect of the economics, then, is that then we would have to pay a fair price to the owner of the phone. And so our intention there was we gave ourselves margins. So essentially we would take whatever we were paid or five and a half cents per hour, split that evenly with the owner of the phone. They would get two point $0.75 per hour, and and then we would get the other two and a half cents. So essentially the phone owner gets paid. We get paid equally for the time on the phone and the customer gets a discount. And the only everyone wins. The only party that doesn't win in the scenario is the large data center provider who may lose a small amount of business. But as far as we can see, that's a drop in the bucket for the huge data center providers. So it looked to us like a win win situation. And the pricing strategy was one that could appeal to all the parties. So it was enough to pay the phone owners to make them interested, to do it, enough to give us a chance to stay in business with reasonable margins and enough to attract customers with a reasonable discount. So that was the general idea on pricing. Since then, the devices have continued to evolve. So as offers over time better and better capabilities for the same price. So they aren't a stationary target. They get better and better over time, but so do we now. A Galaxy S Six is no longer the best device around. In fact, it's a pretty much obsolete device. It's five years old. It is now a Bronze category device, and a Galaxy S 21 is the new current gold standard from Samsung. And the device performance of that is over double what the performance of an old Galaxy S Six was. Basically Amazon and Neocortex have both benefited from Moore's Law or the new incarnation of Moore's Law, where price performance is still improving every year with better core, better frequencies, be better processors. So that's our first answer to the pricing. It's the price question of the cost question, but it's not quite what you asked. You asked about total cost of ownership, and what I'm talking about is an incremental price or a price for a given job to be done. So, you know, 200 devices. Let me just be specific. 200 devices at five and a half cents an hour is $11, and with 200 devices with 40 virtual users per device, that would be a load of 8000 virtual users. So that's a reasonably large test. It's not huge. It's not the biggest that we can do, but 8000 virtual users is one size and the cost of running that is $11 on our devices.
So.
If you would be able to compare that to as whatever number of instances you need with whatever capacity you would need, you would have to price that out and find out what that would cost you to to achieve that load effect from AWS. But then also you would have to compare the value of having it done with the last mile capability versus with the estimate of performance that you get from measuring directly from a nearby high performance data center, which is going to be under estimating the response times relative to real world response times. Finally, to the question of total cost of ownership, that's generally something I think of when people are buying buying devices, like, for example, if you are provisioning your own on Prem devices and actually buying them and paying upfront for the cost of the servers, cost of the incremental cost of the air conditioners in your data center, the maintenance people to run them, and so on. That's generally how you calculate the total cost of ownership of these devices, which may be used for many jobs and over a long period and amortized and depreciated over a period of time. You get your total cost of ownership of those devices. Whereas Neocortex is not doing that. We're renting devices over time and only paying when we are using them. We're only being paid when we're being used. So it's kind of like saying, what's the what's the total cost of ownership for you in taking a ride on Uber? Not exactly. I don't think the way that we think about that the driver of the car in the Uber scenario certainly would think about his total cost of ownership, his or her total cost of ownership. But the customer is just renting, renting time and having the experience of riding in the car just for that one ride. So if you want to discuss further about and elaborate a little more about how you like to think about the total cost of ownership, I'm definitely open to doing that, but I think of it more as a price per price per hour, five and a half cents per hour comparable to $0.10 per hour for AWS. Okay, so I'm going to pause there just to see if I've reasonably addressed your first question.
So basically, Neo Cortex is more affordable than any other players in the cloud computing. Correct.
That's how we've intentionally priced it. But I will give you this full disclosure, honest caveat that we set that pricing essentially in 2017 based on a rendering task. We didn't do it based on a load testing task. Right. So it's possible that for load testing, the economics might be different then for rendering for a compute bound task, it could be that an additional re calibration would be needed. So at the moment, I the only real way to do that. And I guess this is something that we need to do now pretty soon, because it's becoming this is our bread and butter. So we really need to do the comparison, spin up whatever device would be possible to use for an 8000 unit virtual user test on AWS and actually price that out. What would it cost to rent that for an hour versus our 200 devices at 40 virtual units per device? Now, there's a reason that we put a limit of a rule of thumb 40 virtual users per device because we found in really analyzing the scatter plots of the data that if we overload any of our phones with too much processing running JMeter, for example, that we find, then that the response times become artificially large, not because of the actual transit times, but because the phone would be overloaded. So it's too busy handling the requests to be able to accurately measure the response time if it's in the overload condition. And of course, we don't want that. So we found that it's safe if you've got 40 virtual users per device, and there is a reasonable pacing, as we've seen with our customers, that there's a certain amount of think time, you're not just blasting them at full speed for each user. So if you have reasonable, reasonable scenarios, we find 40 to be a reasonable way of doing it reasonable limit. And in that case, then we're safe and we're not overloaded. But in order to do a fair comparison with AWS, if you're going to have a single server Simulating 8000, well, that's actually probably a pretty high performance device. Then I would expect that that's not a ten cent per hour device that would be able to simulate 8000 virtual users. So that's where the pricing comparison would come in. If it would be. I think it still remains to be seen if it's if that high powered device is still twice as expensive as the larger number, the 200 smaller devices all carefully not being put in the overload state.
Go on. What was the second ghost? Im sorry I lost track of it.
Well, the the second question I had was compared to the load of low testing is providers like place meter. The first answer seems to answer that when we talk about the one interesting aspect that probably I had to add in to that question was, you know, the the distributed load testing was to I'm not wrong to reduce the carbon emissions, a positive impact on the climate change, an important aspect of load testing and close ups and all that stuff. If you are using more of mobile phones to get that, I kind of have a bad impact on changes or carbon emissions, that is the other thing that I want to bring. And I believe a Cortex architecture more about data, and even given the kind of an architecture of resilience is what I talked about is more of a liability and a specific putting together. And then what I was under impression that it's not a mobile data driven testing but a WiFi. So why was the impression that maybe homes are available to to satellite mode or something? That is the question that I had a final question I had is.
I.
Kind of lost the final question.
That is, the final question was about observability of.
APM. The question.
Sorry, I muted.
Yeah.
So the final question I had too, was, you know, when we talk about observability the performance engineering aspect, how good is context respective integration to all the other hybrid tools, the other race engineering the three takeaway.
Okay, if I may, then so shall I focus on the second question, which in the rephrasing of it has brought in these bigger issues, which I didn't catch on the first one about climate change and global impact? I'm not sure that I've really got the gist of your question. So I'm going to say what I think I'm going to try to answer. And if I've got the question wrong, then I'm going to give you a chance to correct me. So it sounded like your you are asking me to comment on the global impact of this distributed network of mobile devices, how it would compare strategically to the conventional data center approach. Is it better for the world that we are using people's phone or as compared to using large servers and data centers? So is that the nature of the question? S global impact on climate and the environment?
Yeah, that's correct. Lid, I resonate with you on that. So that's precisely my question was because if you are using mobile phones and we are more exposed to all the ways and all the stuff. But so that's the thing, the question that I had.
But anyway, shall I answer that then?
Yeah, sure.
Sure. Okay. So that's the question that I will try to try to answer. We generally look at this as as a kind of a green solution that is very much advantageous and preferential to to the climate and to the environment. The reason being this actually is a case where it does make sense to think in terms of total cost of ownership, not just dollars, but in terms of environmental impact. Here, if you compare the cost of what a company like AWS has to do, which is when they want to increase their capacity, they buy new servers and they put them into their data centers, concentrate them into a single location, their high performance devices, they pay twice for the power that they consume. It's a large amount of power for these devices that has to be brought into these data centers. And then because they're concentrated in one place, they generate a lot of localized heat, which has to be dissipated using air conditioning. So they pay twice to heat up their facility with the computing, and then they want to do that, and then they pay a second time to air condition it and cool it down. So that's the way that they are doing it involves buying new devices and powering them in an inefficient way, in a heat inefficient way and having to air condition it to keep it cool. Whereas what we are doing is using devices that people already own. There's no purchase of a new device. There's devices that have already been bought and serve a useful life in a consumer sense, we're re utilizing something it has already been used for its original intended purpose. So in that sense, we see that as kind of like recycling or reusing a product. So we think of that as the ultimate in a green solution. Re using devices that have already been purchased. And then from a heating point of view, because the devices are not concentrated in one localized place. Yes, you pay for the power that they consume to do the job, but they don't heat up your place so much that you have to change the air conditioning in your in your home. It's partly because the device is so inherently efficient. These are Arm devices which have been made to be battery power. So the industry has put an enormous premium on making these devices power efficient, so they don't run down the batteries when they're used as normal phones. So they're efficient to begin with. So they consume less power per compute than the intel servers that are commonly found in data centers, and because they're not concentrated in one place and therefore have to be cooled, the they're sitting out on a table and have normal air flow around them. There's no need for specific air conditioning to cool them. Specifically, they fit into people's lives where they're already in the environment is already being air conditioned. And this adds virtually nothing incrementally to the air conditioning needs of a given home. So we think for two reasons for those many reasons that our solution is more environmentally friendly, lower carbon footprint, and better for the environment than the highly concentrated data centers that's my answer on that. Does that address your question, or is there something else that I should also address that I missed?
No.
I mean, I resonate with you on that. And thanks for putting it in an elegant way. So I'm okay with that.
Okay. Great. Now I think there was another part of your question that you mentioned about DevOps and integration with other tools. And how does Neocortex compare in those ways? So shall I address that question next?
Yes, please.
Thank you. Very good. The first distinction that I would like to make a clarification. But what is it exactly that we offer? For example, we don't think of ourselves as a as a tool competitor. For example, for example, we don't compete with Micro Focus Load Runner or Tri Scents. No load, for example, or K Six or Gatling. We are not producing a load testing tool that competes with those other load testing tools. We're not making the software we're providing the instances they run on. So to the extent that we are competing with anybody, we're competing with AWS, Google, and Microsoft Azure for providing instances to you for running your software. But we are as agnostic as we can be about what software you choose to run. So we make it possible so that you can run load tests on our devices using JMeter or no Load or K Six Locus Gatling. And those are the ones that we currently offer. And in fact, we've got other capabilities that go more into the observability part, which you have mentioned. For example, an integration with Dyna Trace, an integration with the JMeter PerfMon plugin. So those are for correlating the performance that you may measure in a load test with the CPU memory and network metrics that you can measure. So you can find out the load test is telling us that the performance of the device is slow, under of the server is slow under load, and we can see that the CPU is becoming limited in when it's getting loaded. It's a CPU problem, or we can say, oh no, it's a network problem internally, or it's running out of Ram. A garbage collection is the problem. So those observability concerns we address as not as a device provider, but by integrating, allowing integration between your load testing tools and integrating with Dyna Trace, PerfMon and other Observability tools. In addition, we also do it. There's a different kind of observability. This is more like synthetic user monitoring, where you can also use our devices not just to do load testing but to do front end testing. So things like Puppeteer and Lighthouse. We've had requests for those additional capabilities so that you could do a load test that measures the response times to the real world user end user locations. And then you could have also devices in those locations that are not so much delivering load, but they're measuring the render time and the actual response time perceived by the user. You could, in principle and in practice, have the most thorough measurement of end user experience possible, more thorough than could be done any other way because you'd be measuring the real render time on real devices under load that was created by last mile response time load generators. So you could have literally the best of everything, the most thorough end user response time measurement possible using our devices. Now, I don't know how many performance engineers are that ambitious. They really want to measure all of those things. When I speak to most of them, I find that we're that's like getting your PhD in high performance end user experience. And what I find is most most engineers in practice are using a single data center, a single device in a single data center that is very close to the website server under test. It might even be inside the same data center. So there's a wide continuum of ambitions in how much of the end user experience to measure. And the most common scenario is to measure the least accurate estimate of end user experience by just using a single data set. I find that there's a sort of a long tail of distribution with a much smaller number of engineers that are willing to consider measuring end user response times and then an even smaller number that are also wanting to measure render times and combine all these into a full measurement of the total distribution of end user experiences. Now, if I may, I'd like to just kind of make that there's a key point there. It's about the fundamental purpose of performance testing it's to measure the collective distribution of end user experiences, to make sure that the end customers of your website services are getting the expected performance, and that should be in unloaded conditions and in loaded conditions. So we want to make sure not only that the devices don't crash, the servers don't crash on Black Friday. We want to make sure that they don't slow down excessively. Also on Black Friday, I've heard the phrase that slow is the new low is the new down that if the device becomes so slow that it's unacceptable performance while shopping, people will abandon the shopping cart just as badly as if the website crashed. So it's important that we measure the response times fully under load, including the render time and including the response times all the way out to where people actually live and shop.
So.
Performance engineers are doing either a what they're used to or a what is convenient that actually is creating a problem in the industry because the performance engineers are the people who are tasked with determining whether the performance is acceptable to the end user. The CEO of United Airlines is not independently measuring that. It's going to trust that the company that he hires, you know, Cognizant or Infosys or Cap Gemini or more tech. Any of the companies that are doing the performance testing that they will make an honest, good faith measurement of the end user experience. And if those companies take the easy way out and just use a data center nearby or even in a server inside the same data center as the device under test, then they will be dramatically underestimating the response time that an end user will receive. And so now we can set up this horrendous disconnect, something that is harmful to the industry, not just to the customer but to the industry as a whole. It's has a big consequence if we have performance engineers underestimating the response times and signing off and saying, We've done our test looks like we'll handle it on Black Friday Friday. Everything's fine. Response times will be good because we measure it in this false way with an under estimated response times. And then Black Friday really hits. And now you have people sitting on the couch saying, Why is this website so slow? Because they're measuring the real response times with the real rendering times and the real path out to where they live, the living room in Oklahoma and their WiFi connection. All of that adds up to response delays that are longer than what the performance engineer measured. So now we have a disconnect. We have a conscientious performance engineer who doesn't realize that he's underestimating the response time. So he confidently says performance will be good. And because the measurement has been flawed in this optimistic way, it's confident but incorrect. That is the worst possible scenario for a performance engineer. It's sort of a doomsday scenario. Performance engineer says performance is good when it's not in practice. And it's because, in my opinion, it's because the industry has gotten used to measuring it in the convenient way. It was the only way that was possible to do it in the past. That's a problem. That's the problem that Neocortex fixes. It allows you to measure it accurately. And it gives the performance engineer the bad news that the performance as perceived by the end user is worse than what you get if you measure in the data center, it's important that not that the performance engineer gets bad news. It's important that they get the truth. They should get the truth of what the real end user performance is so that they can optimize the performance correctly. And now this is a key point that the performance engineer is creating an optimization loop. The performance engineer doesn't just do a load test that sort of. Technically, the performance tester does the load test. They get a measurement of the response time or the distribution of response times. But then the performance engineer has to optimize based on that. So if the performance is perceived to be too slow, like it doesn't meet the service level objective, then they must improve the performance of the server software and perhaps the hardware. Maybe you have to install more servers or allocate more devices or fix that bottleneck in the database or do whatever they have to do. But they have to go to work and optimize the performance. And then once they've done it, they think, okay, we've just saved 400 milliseconds off that server response time. Now let's do another load test and see if the performance is now acceptable. And so they do a second load test. And hopefully what they find is that okay. Now it's acceptable. If it is, then they stop optimizing because they now meet their service objectives. So this question of when do they stop optimizing? When do they terminate their optimization loop is the key thing. If they have an underestimate of response times, they will stop too early. They will think that it's Miller time. It's time to go for lunch. We've optimized it and everything looks good measured here in our data center. So they stop optimizing. Whereas if they had truly measured what was being perceived in the living room in Oklahoma, they would say, oh, we've got to improve it by another 1.2 seconds to account for the actual end user last mile response time. This is why last mile load testing is so important. You have to know you have to take that extra 1.2 seconds or worst case into account. And it puts a really high burden on how much more optimization you have to do at the server level to actually get great performance perceived by the customer. So not terminating your optimization loop is the key. And it's not that we're giving you bad news. We're giving you the truth so that you can truly optimize to get the end result for the consumer that you're actually being the performance engineer is being tasked to do. Okay, I'm going to pause there. That was a long answer is very important.
I agree with you, Lloyd, because as a performance engineer, right, we tend to focus on one thing whether backend performance means people focus only on the back end performance. There could be numerous reasons, I mean, management or time constraints, and then it will not be in scope for them. So there could be a number of reasons where we can expand on all the aspects. But as you mentioned, we need to expand our visibility into all the aspects of the application is I agree with you agree that phrase that you use about focusing on the back end.
That's one that I've heard other very well respected performance engineers have used that very senior people in the field said, Well, when I debate with them about the importance of doing this, and that's an answer that I often get I prefer to focus on the back end. Okay, well, you focused on the back end. That means you are not focusing on the end user experience. You're not measuring it. And that is the job that is to be done, the end purpose of the job to be done. So in this sense, I consider that this is addressing not just a concern for a given customer, the job that's being done for that customer. This is a problem that affects the entire industry. The entire industry abdicates the responsibility of measuring the real user response time. The industry as a whole suffers. And I actually, personally, I believe that we are seeing effects of that. The performance engineering and performance testing industry are experiencing a kind of decline because it is losing its relevance to its true purpose of making the performance good. Optimizing the performance for the end user. It's become a specialized niche industry of improving database performance times and improving performance as measured locally at the server. And that disconnect between the end user is making it something that it's losing its relevance in the business sense to the funding sources. The CEO are wondering, well, is this relevant? But I'm paying all this money for performance engineering, and people are still complaining that the website is too slow, that disconnect is being perceived. I think implicitly by the people who do the funding and the way out of this industry wide problem is to raise our standards. Basically, don't do the thing that is convenient. The thing that is easy. Don't look for the keys under the lamp, look for them where you dropped them. Right. That old joke about where you dropping your car in the parking lot. But I'm looking over here where the light is better to try to find them. You have to do the hard work of really measuring the end user response time. Yes, it's more work. Yes, it takes more time to do it, right. But it's the job that must be done. And if one engineer doesn't do it on that job, that job is weakened. But if all performance engineers don't do it on all jobs, the industry as a whole is weakened, and that is a big problem. I personally see the effects, the corrosive effects of this collective behavior or not a behavior, but of a belief system that is affecting everyone in a subtle way. And the only way around it is for each individual performance engineer to commit to raise their standards and really measure end user experience. Their performance will become better. Their company will be raised, buy it. The performance engineering industry will increase in its value. People's jobs will come back or have greater longevity. The pay for being a performance engineer will go up as people raise their standards. That's the kind of initiative that I perceive is needed in the industry. Okay, anyway, that's my soap box on that. I think it's important.
We have a couple of folks, I 70 and get ID. Thanks for joining.
Thanks.
Please go ahead with your question. I think you will be waiting for a long time. Please go ahead.
Yeah.
Thanks, Li, for a brief explanation and everything. Let me put my words. What I understand. So these automation tools, like any load testing, like a geometer or Neil or any tools, came into picture when it was not possible to do the real load testing. The term you called at the end load testing.
Yes.
It was not possible. Now you're making that as a possible. Right.
Right. We enable it really for the first time. So that it's an enabling technology? Yes.
In the tools came in the picture when it was not possible. You are making it possible to bypass the tools. So that's fine. That's good. And one more thing to bypass.
Not to bypass the tools, to actually allow them to serve their true purpose by being run on the right machine in the right places, not bypassing. We're enabling them to do the job right now.
True.
True. I put some pessimistic. So there you go. Oh, and one more thing. I have a question like I thought of asking more in terms of monitoring perspective. So when it's so how you help me understand here you're scaling your device, then you are allowing them to test on your device. Right. Scaling your devices like you're allocating that you're saying that my device is five octacore and this much Ram, so 40 virtual users you can test on it.
Yeah, that's right. True.
So that means no need to have any monitoring like CPU memory. You're already setting a Max CPU of 95% that's initially used, right?
Yes, that's right.
So are you saying directly, no need to monitor my end real user in terms of resources, at least not in terms of response time or rendering anything.
So I think I see what you mean. So you're saying are you saying monitoring monitoring whether the load generator is in an unloaded condition or a satisfactory handling condition? Is that what you're saying?
No.
No. I'm thinking in terms of end user device. So now it's the real user. Right. So are you saying no need to monitor this one in terms of apart from the response and rendering times.
I.
Think Sid was mentioning about the devices, CPU and memory, then the test is in.
Yeah, but the low generator device, your phone.
Okay.
So that is what I was trying to say. My understanding is that most load generation tools, most load testing tools have the ability to monitor the health of the load generator itself to see if the load generator is in an overload condition. I think that that's what you're asking about doing that. Okay. So. So for that, we haven't yet had a customer ask us to do that. And there's every reason that we should be able to do that. But so far, no customer has said I actually want to verify that you can do that.
So.
I can't point to direct experience with having done that for a customer. What I have done is I've run many load tests and in enough different conditions. So I and my team have run it, and we've pushed the limits of having more than 40 virtual users per phone. We've had, for example, weaker phones, where we've put hundreds of virtual units users onto a given weaker phone. And what we've seen there is that the response times become unexpectedly large and so that's an indirect indicator that the device is in an overload condition. And so we're not measuring true. We're not really in that case, we're not measuring end user experience. We're getting a false pessimistic reading it's worse than what a real user would experience because a real user is not doing hundreds pretending to be hundreds of users on one device, so we should not overload it. So that's how we came to a kind of empirical rule of thumb that about 40 is safe and 40 on a good phone gold phone and 200 an old bronze phone from 2017 is unacceptable. So we've kind of measure that anecdotally and empirically, but actually measuring it directly by reading the CPU characteristics and performance. It's not something that we have yet done, and I'll give to the additional caveat. It's something that we can do. In principle, however, our lead engineer has told me that in order to do that, we have to get access to some Proc files deep inside the Linux subsystem, and it will be some engineering work to be able to do it. So it's not something that I can just sort of enable in a second, I would guess that would be potentially a week or maybe even a little bit more time to enable that capability and be able to get that diagnostic. So the best I can say is really doing that measurement would be a roadmap item that we could do in a reasonably short time if it was requested by a customer. We haven't yet had that request by anyone for implementing the function yet.
Yeah, thanks a lot. That makes varies sense. And I think if you name it as re testing real performance, I think that would be more suitable than what your name? And one last devel question. So if I have a powerful device which could be used for you.
How do you approach me as a company power progressive if you have a big gaming machine or something like that? So at the moment, that's a good question. That's another roadmap items for us. We currently we support only Android devices. I'll make the initial or this small detour. People often ask us, Can I run phone Paycheck? It's our app that pays people for renting time. Can I run that on iOS? But I run it on my iphone and the answer is no. We don't support iphones or ipads iOS devices because we don't believe that we would be permitted to run the necessary background processes. Persistent background processes on iOS devices. Generally, that's not allowed, except for audio players and map applications. But otherwise, applications are blocked from using persistence background devices. And of course, we need that to provide reliable service.
So my question was like, even if it's on a device. So how do you approach end users? So we will run our tests. How do you convince the approach to customers?
So how do we how do I don't quite understand the question. How do I persuade the customers?
No, the set you are using their real mobile device when they connect to WiFi. It's somebody this phone phone, right? Not your company's phone number, any phone, any phone. So if you want to use my phone as part of your company, how do you okay.
I think maybe it's just how do you sign up to provide your phone to our service? Is that what you're saying exactly? Okay. Sorry. I guess I had all that implicitly assume that people know about that. So the way that people sign up their devices is they install an app that we have created onto their phone, so they're volunteering to do it. And in an exchange to get paid for doing so. So they install phone paycheck. So if you go to the Android Google Play Store and you search for phone paycheck, you'll find this lovely little app. It's rather plain user interface. It just looks like it shows you kind of a bank balance, shows you how much money you've made from the point of view of the owner of the phone. It's an app that pays you for doing nothing. It pays you in US dollars through PayPal for literally doing nothing. You just renting out your processor with no effort from you. So it looks a little bit like Airbnb. You're renting out your home, youre renting out your processor. But it's not like Uber, where you actually have to drive your car. You've got a job that pays you dollars per hour for being a driver. There's nothing like that. You're just renting out this asset that you own. So that's how you access it by installing the app on your phone. And then there's another key step, though, which is that we then have to activate your device. At first we activated all devices as we were virally growing. But then we hit a limit of how many devices we could afford to pay for before we had paying customers ourselves. Remember, this is a shared economy business. So the providers of the compute capacity are the phone pay check users who are getting paid. And in our initial development phase, we have to pay them even though we don't have customers. So it's kind of like what Uber would have to do at the beginning of a launch in a new city. They would have to pay drivers to be driving around on the roads even before there was any riders. Right. So there's a little bootstrap process that a company like Uber would have to do pay a bunch of drivers, get them out there and promote to the riders and get the riders out. And eventually you get a balance between the number of riders and drivers, and then you can kind of grow it organically then, and you can stop subsidizing the drivers. Well, we're just on the cusp of that right now. We've been subsidizing our providers. That is our phone paycheck users. We've been paying them even though we didn't have a substantial number of paying customers to us. So that's cost us it cost us, dearly. Actually, we've been doing that for four years now since we launched in 2017. I can tell you that we've paid over a million dollars to pay people to make their phone processors available to us, even though we didn't have anyone paying us and that's personally come directly out of my pocket. I put two and a half million dollars into the company and over 1 million of that has gone to paying people to bootstrap this process of having an available resource that we could then develop on and used to demonstrate to our potential customers. So if you evaluate Neocortex Load Test now as a load testing service, you'll be running on devices that we have been paying for four years. But your this is your cost of ownership. Your cost of ownership is extremely low, right. Because my cost of ownership is very high. I've been paying all these people even though I didn't have a customer coming. It's an investment that I've been making in creating a two sided shared economy business. But that upfront investment is substantial to have a viral growth on the supply side. Now we need the viral growth on the demand side, the customers. And the good news is we're just about to get that. Now. I'm seeing good uptake in the industry, but we have to balance out our supply side and our demand side. And what we need now is neo critics. Load test customers come and try it and start actually using this amazing resource, increasing the demand for devices. And then we can grow from the 3000 devices we've got activated to the 90,000 that are on the waiting list. This is a phone. Paycheck is an extremely popular service. People all over the world love to get paid for doing nothing and paid us dollars. By the way, we're not doing some crazy coin thing. This isn't neocortex coin. There's no crypto currency aspect of this. We're paying us dollars through PayPal all over the world.
Suppose I'm running a Raspberry Pi cluster at home where I use for operates by hole and all. So can I enroll into the program?
No, you couldn't. Unless this Raspberry pie was running Android right now, we're supporting only Android. In principle, it could be done, but it would just be an additional interface that we need to support on our roadmap of additional devices. The roadmap that we have is continue to support all flavors of Android and new security enhancements. Google makes it harder and harder for us. Every year with every new version of Android, we have to do something to make sure that we can still run properly and don't have our processes killed and stuff like that. But then we go from there. Extending to PCs would be next. Not because we need more devices. We're not running out of devices. We're never going to run out of devices, just two and a half billion Android phones. We're never going to run out of phones. We're limited on the demand side, getting people to use our service much more than we are limited by running out of device. But the reason that we would have a roadmap item to go get some PCs, it's not because we need more computing, but it would be good to have even more powerful devices. A PC is three or four times the performance of the best phones that we've got. And then if we could augment that with not just PC CPUs, but with with Nvidia GPUs, then we could have a credible entry into the deep learning market, for example. So our general purpose compute capacity, we would look more like a real competitor to Amazon EC Two, because we would have devices that reach up into the high end. Right now, we're limited to competing only gets below end. So we have a roadmap to address that, but. But it's not our biggest concern at the moment. Getting customers for our load testing services our primary objective right now.
Sure, we have one Hi 70.
Heaven in hell at have you on the call?
Yeah, it seems like is here. So it's not a question, but I would just like to provide my review to other performance engineers here. Yeah, I have thoroughly evaluated their product and we have partnered with them as well in your context. So I have at least a complete information of end to end. What are the questions which most of them have been covered here? So just a few review points which I would like to give a Loya already stated it supports most of the most of the available load testing tools in the market. Be a meter, new load or Gatling or cases whatever you emit. We are supporting it. And the best thing which I have observed is like how you have your Blaze meter and other portals where you could do your JMeter testing. Neocortex is equivalent to that, actually with an additional benefit of doing this last mile load testing. So definitely it is a winwin when you have to compare with any other cloud portal load testing tools which are available. So yeah, and I've seen the tool. I think one of the best things which I have observed is the support if you need certain feature, I think Lloyd and his team are very quick enough to incorporate which I haven't seen the support kind of support in evening commercial tools where we pay a huge amount for their support and maintenance. But if you have to get any feature added, it really takes number of months or even years. So that's one of the great things about the tool. And finally, it's like you get best sophisticated reports like any other tool, and they have few additional benefits of the reporting feature as well. So yeah, other things. I think one of the Lloyd was highlighting the other aspect where apart from load testing, we could also do certain website monitoring, probably using a Lighthouse. So if you have to run your Lighthouse across different locations in us, it's very easy to do on this compared to your EC two or any other VMs. Yeah, those are the white sand behind which I thought of. I find you very nice.
It's great to have you on the call, Servant. To mention those things. I think most people know that Servant and More Tech have been incredibly wonderful early adopter partners with us. We've announced the More Tech Neocortex partnership about a week and a half ago, having savant really not just to evaluate, but really exercise, really push our our capabilities. Over the last several months. It's been so valuable to have such a highly qualified early adopter. It's pushed our service beyond what I could have imagined that we would have been able to do a few months ago. And having such an experienced and knowledgeable adopter early adopter has allowed us what Sean said about providing support for new features. Yeah, we went through half a dozen cycles there with a turnaround time of generally about two days to when Servant would request some new feature that you felt that we needed to have and we would work on it like our lives depended on it. Basically. Okay, we got to do it. And Demitri and Michael would drop whatever they were doing to make it happen and turn that feature request around. So yeah, I can confirm we are indeed responsive to customer requests. Servant is the proof of that. And our newly launched product, Neocortex Load test with the convenient browser interface. The origins of that were in feedback from Servant and from other members of our team who are essentially saying it's pretty inconvenient to ask the performance engineers to have a Linux machine lying around to be able to use, and we need to be able to provide a convenient access and reliable access. Basically, Necrotic should take care of that. We should have a controller that runs on a highperformance data center. That part does run in a high performance the controller to our last mile devices. And so we responded to Servants request for that. That one took a few weeks for us to fully build because it was a whole new type of service, but yeah, it's been great to be able to respond quickly to a customers request. We're honored to be able to do that. And servants pushed us like a drill Sergeant. Give me 50 push ups.
Alright.
Yes sir. Let's do it. Let's make this happen.
Ah, thank you so much.
As a CEO, you have to listen to everybody's.
Sorry. Recent deliveries. Is that what you said?
Everybody ought to listen to everyone's life right other way. We cannot improve the product.
Of course, we're very happy to have early adopters. Servant has really been our first very strong one. However, there have been others as well to going back even earlier. Navin Kumar himself and her ender Sera and Paul McLean were the first people over a year ago who tried the new service out when we were just offering the batch runner capabilities and all of them provide additional feedback and input to us on how to improve it. So we've been improving steadily with input from all of these experts in the field who have been helping us with their feedback and the latest incarnation of that. The most recent one was a really strong engagement with more tech and with Servant and the lead on that. So I appreciate that from all the people who have tried it and and others as well too. There are other people who took our certification program maybe six months or eight months ago. Everyone has contributed in some way to us improving the service. We're keen for the feedback and we welcome anybody else who wants to give us a try. Now anyone who tries it now gets a a very thorough, thoroughly baked and convenient interface. Now, basically with two clicks, you can launch your first JMeter test. No need for a Linux machine, no need for need to know Linux at all. Just a convenient browser interface. Select a preset hit launch. You're running your first sophisticated the Pet Store demo, full reporting and then you can get the files and modify them. We've made it really easy to use. And again, that was thanks to the really expert feedback and guidance from Savant on how it should work in these advanced scenarios.
Thanks line.
Thank you.
Do you have any questions?
Yes. One last question. I yeah, it's not a question. It's a doubt. Like how do you decide the Go uses loads like it should be running from the South America or North America or the world UK. Is it a requirement should come from the end customers or you decide?
No, you can request that. If you look at our demo. If you sign up for your free account, go to the Load test button and Go to launch a test and you'll see that there's a field there. You can request how many devices you want and you can request what regions you want them to run it. So you get to decide that if you leave it by default, then you leave it up to us and we choose at random devices all over the world and wherever they may, we have them available to us. But if you wish, you can specify sub regions, for example, USA only or India only or Asia Pacific or Russia, South America. You can ask for whatever devices. And then what we show you is as you make your selections in real time. Then we show you how many devices we have available for your test. So if you say USA, we have most of our devices or single greatest country is us. So if you ask for us, you'll see 90 devices currently available with unique locations and 374 devices available where they may be colocated. So there's a distinction there. We assume that you want devices in really unique locations. But remember, people are allowed to have up to five devices. So if we're not careful, we might give you two devices from the same little farm in somebody's house.
So that being said, all the five verses should be active. Always need to be connected. How you notify them? Like there is a requirement of your location loaders you connect to your device and make it sure it's connected. The WiFi is that that way? It should be all of the five device. It should be active every time.
No.
Any device can be brought down or disconnected. So if all five devices are available when the time comes to launch a test, then they become eligible. And and if we're asking for non colocated devices, then we may choose one of those five. But if we're asking for non co located, then we won't choose all five of them or even two of them, because we want the devices to be in unique locations with different IP addresses and different actual neighborhoods that they come from. But we treat them as available devices, but with the grouping based on their location. And if we can, if you ask for a reasonable number of them, then we can provide you with devices that are guaranteed to not be in shared locations with each other. Non colocated.
Thanks a lot.
By the way, that was also a request that effectively came in from one of servants colleagues because he noticed that when we would draw the map of devices, we were getting colocated devices. And so you would ask for six devices, but it would look like five points on the map. And that's because sometimes two of the points were in the same house. So it only looked like five points, but it was actually six with two colocated. And so we had to spend several days modifying our device selection algorithm to select only unique locations. Turns out that's hard to do using programming in PostgreSQL easy in Python. But try to do that with Postgres SQL requests. Turns out it's kind of tricky.
Hopefully you are not entered with each coordinate is a unique location.
Each location. Now we are treating that as a unique look.
Got.
For example, we do have scenarios where there are several people in the same city, but we don't treat them as colocated. It's only if they're in the same household. We treat them as colocated.
Yeah, thanks.
We have so doeth NKS for joining us. If you have any questions you can ask, I think he left. That's fine. Okay. Thank you everyone for joining this. I think he's back.
Yes.
Hey, if you have any questions you can ask lid actually in this session I don't have any questions. One information provided by I am fine with it.
Ok.
Cool. Thank you together. Thanks to everyone for joining this club house meet. So quick update on October 6. Please check it out. Load Test all. Com conference by Red Line 13, which is free and 100 percentage virtual. I'll be speaking there. And also there are a couple of folks like Leandro DA welded, so they'll be there. They will talk about various topics about performance. So please check it out. Load test world. Com and with that said, we can wind up today's session session is being recorded. Once it is available, I will publish it in my YouTube channel.
I mean, sorry. So before signing off, I just had one question to live. Sure, this will be very quick and not taking much of time. And also maybe to slant as well because she has used this product for quite a while now. First of all, I just wanted to give my appreciation to not for the for the insights and he hit the nail on its head when it comes to, you know, why the performance testing and engineering field has declined now with respect to the scope and various business objectives and all the other factors. So he is absolutely right. Second thing is regarding this business model of using plat formation and more consumer centric of offering the solution seems to be a very good aspect for us. Also, maybe the question that I had is there a is there a technical capability deck or tool wise comparison deck or any I would say white paper or anything which which can I basically use to to probably once I go to the create an account and do some proof of concept and then probably showcase to the you know, to the management or whoever it is.
A.
Servant asked me for that about two months ago. So I created a slide deck that was intended for a performance test lead or performance engineer to to discuss with their management about the virtues of the tool and perhaps recommending it. We need to arm you with some collateral. You can convey the advantages. So yeah, I can share with you the deck that I gave to servant for that purpose. In fact, I guess we should just put that on our website. This is something that an early adopter performance engineer will need. You have to then get budget approval to start using the tool. So you have to persuade people that the virtues of the tool are so would that address your question if we share that deck with you?
Yes, absolutely. Because I'm kind of now working on probably prospect. I mean, we have as a company, we haven't gone to any vendors for an out and out licensing, but it's all customer based licensing on demand. But in case if something goes well, we do have our own sales Department for Tooling, probably if technical debt goes well. And if a proof of concept goes well, someone reach might reach reach out down the line as well. The other final question I had it's not again, which will be very brief, you know. I mean, we have our Indian stack hosted on AWS EC Two cloud, which is hybrid, and then with whatever understand with the testing tool infrastructure, the Neocortex brand, right? I mean, you know, as I understand, it's all mobile handset based, with whatever the app that is installed and the agent that runs to spin up the browser interface or whatever to connect to the easy two instances. So how complex is that to run a test behind the firewall or behind the Nat on AWS EC two instances should should all the thing U, should that be a DMZ zone, or should there be a security group interference? How firewall or not driven testing could be done? That's what I want. I mean, should there be it of interference at both the on the test infrastructure side and the test tool infrastructure side as well?
I.
Don'T think so. First of all, there was something you said in your question that I'm not sure if I understood right. It sounded like you're thinking that we might be running Neocortex software on EC two devices, like inside a data center. That's not something that we're doing. We're running whatever your testing software is. If you're running JMeter or NoLoad or Catlin or six, we're simply allowing you to put your load generators on these mobile devices that we provide for you. You get to run your testing software, load generators on our mobile devices that are people's real world, last mile locations where the real load would come from. On a real Black Friday with real users, we're letting you get your load to happen from there. But there's no Neocortex software that we are trying to run on an EC Two instance or inside some other data center. We're simply acting as an orchestrator. We orchestrate all of these devices to make them available to run.
Store or load generator software. Whatever tooling does that answer your question?
Yeah, fine. I mean, I think I will get in touch with you offline. We can keep it very short, but I got your point. I mean, that's what I was trying to ask me. It's not that we'll be running the Neocortex apps on the instances of EC Two. I got it. So that's the reason I put it into two different things. You know, one is all our test environments or the servers are in the AWS EC two within particular region. And the other thing, what we differentiate is what we call it as normally as a test tool infrastructure. So the test tool infrastructure will be the load generators or load injectors along with the testing tool. Whatever the testing tool that we use. But I will certainly connect with your offline back to you.
Very happy.
Thank you. Thank you, everyone. That we can line up the session. Please take it out. Youtube channel for the recorded video. How are you? Thank you. Have a good weekend.
Fantastic. Thank you. Come that's really enjoy being on your panel.
Thanks.
Yeah. Thanks. Thanks. Like thanks, everyone. See you.
Thanks, Nevin. Thanks, Lord. Thanks, everyone.
