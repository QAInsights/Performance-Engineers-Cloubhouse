Thank you everyone for joining this club session. So this is our ten club meeting featuring Chaos Engineering by Karthik. So thank you for everyone telling. Now we have around 350 members in this club house and this session is being recorded. So I'm using an application called Backstage, so it will be recording everything, including the transcription and other details. So I'll be posting this link in my YouTube channel and also in GitHub repository. Also, you can check it out. So this session. So I'm going to divide into multiple sections, basically ten to 15 minutes segments, and then I will try to keep it within 1 hour and for every ten minutes or ten minutes we can take around two to three coatings and the Catholic will be answering that. We are not limiting the number of questions just to proceed the sessions further. We are making this kind of arrangement, but anytime you can ask any questions to Karthik and Cathy will be very happy to answer your question. So now let us begin. So let me give you a quick introduction as Karthik is a CPU of Chaos Native. Chaos Native is Chaos Engineering solution. They have a couple of products, Litmus, and you can check it out the website and let us go over to Karthik.
Hi.
Karthick.
I even thank you for joining us. We are very excited to listen about evolution of chaos testing and engineering. So over to you.
Yeah. My pleasure. Even at the outset, I'd like to thank you for reaching out and giving me this opportunity to come and talk about Chaos Engineering am quite new to this club house experience. So I must say this is great trying to learn something new through all this. Thank you so much. Once again, just briefly introduce myself and we can get started with the topic. My name is Cathi. I am one of the maintainers of the Open Source Chaos Enginering project called Litmus Chaos. I think I have a fellow maintainer with me today in the audience age and I also happen to be the co founder of an organization called Chaos Native, which is one of the prime sponsors of the Open Source Project, amongst other organizations. So really glad to come here and speak about Chaos Engineering to give you a little bit of history. We'll start with history and we'll talk about what Chaos Engineering is, how it evolved. A lot of you here in the audience might already know what CS Engineering is. You might already be practitioners. You might be using different tools out there. Some of you might not be practicing Chaos engineering, knowing it is called CAS engineering. You might be doing it as failure testing, might be calling it by a different name, but you might still be following it in principle. So Chaos Engineering is a discipline that was formally brought about a decade or so back, I think. Netflix, Netflix, Salesforce, Amazon, some of these organizations which created some very good resiliency practices because they were actually providing platforms or they continue to provide platforms that are consumed by millions of people. Resiliency is very key, of course, to all of us, but more so for those kind of services which are public facing. Sometimes infrastructure software important as it is, gives you a little bit of scope for sort of fixing things. You can take some time. But when it is publicly consumed software, let's say Netflix is streaming videos, and thousands of users are doing it at the same point of time to ensure that it is always available. Organizations like those came up with this discipline called chaos engineering. They put together all their resilience practices, and one of them happened to be doing failure testing and doing failure testing in production and doing it in a particular way. That is why they do not call it failure testing. They call it chaos engineering. Chaos engineering sounds like a very interesting term. The first time I heard of it was three, four years back. I also did not know what chaos engineering was. So if you look at the principles of Chaos org, which is the set of principles or tenets that these organizations came up with, there is a single page website backed by GitHub repository. You'll see principles of Chaos or they talk about what cars engineering is. They define it as the discipline of experimenting on a system in order to build confidence in the system's capability to withstand turbulent conditions in production. It's a fairly heavy wording definition. Let's think of it like this. You all must have heard of Fastly Cloudflare, some of these big CDN providers. And recently Fastly went down for an hour and I took down several websites along with it. And there are people who lost a lot of money outages when they happen, they cause a lot of money, the lot of dollars are burnt. That is what people try to avoid. And the way they avoid is trying to simulate those failures, different kinds of failures in production in a controlled way and see how the system responds. And then they go and build some mitigation within their deployment practices or their infrastructure, the application, business logic and ensure that they can ensure the impact is not much. They know how to handle these kinds of failures. Do they know how they can probably recover within the SLS that they promised to people to look at? Azure and GCP and some of these cloud providers, IaaS and SaaS platforms? They tell you something like 99 point 95 availability. You must have heard of this term 5969 the availability percentages that I will promise, as in order to ensure that this is upheld, people go ahead and do some control failures and see how the system behaves and actually plan on the right mitigation strategies. They ensure that failures that they observe don't turn into business outages. That is what chaos engineering is about. So going back to this term as engineering in the history about it. Netflix, it is believed, moved from using an onpremise data center into running everything on AWS after they faced a database corruption and they had an outage for three days. There's a way back, maybe in 2008 twothousand and nine time. And once they put everything on AWS, they just did not go to a more reliant platform and let it be. They crafted a lot of tests that actually verified whether the applications were resilient on a a lot of those tests for a period of time took the shape of tools, whatever was done manually as a practice turned into tools and frameworks. Must have heard of Chaos monkey. That's when this term called chaos and chaos engineering just came into being. You can say about any of spec. They created Chaos monkey and a suit of other monkeys. The reason why they selected monkeys was how the monkeys very restless. It can basically cause a lot of havoc and very unpredictable. So they created a Chaos monkey initially took down easy to instances, and over a period of time they created other monkeys. Janitor Monkey two, clean up unused resources, security Bunkie to verify if they were security loopholes conformity monkeys to verify there was policy violations, all these kind of things, right? And they made it a scientific discipline. The so a lot of things from the Ops world mean time to detection, mean time to recovery, and all the best practices associated with managing and deployment environments or production environments. They took all of that into account. They brought about validation mechanisms for a lot of those things, created the tools to cause failures, and then even constructed the approaches, and of course, made them public to carry out these experiments. The way you need to carry out these chaos experiments was also sort of created by them. They call it game days. Game days. When a group of people get together, mostly folks in the upstream and they bring the right support personnel on board, they get sign off from all the stakeholders. Sometimes they keep their customers involved and let them know what is going to happen. Then they go and file something in production. Maybe they started small with killing two instances. They moved on to taking down availability zones, all sorts of things. And they verified how the system behaves if there was an issue. Of course, when they went ahead in this game day, they also had mitigation plans, rollback plans ready. If something did fail, this is what they would do to regain optimal operational characteristics within this much time. So that is the idea. They would go with have light, observability infrastructure to be able to get a fine grind view of what's happening in the system. And this failure is being carried out, go with the proper hypothesis, and then the hypothesis would be proved or disproved. Most probably the hypothesis would be around how the sellers would whether they would continue to be maintained or if there was a deviation. What is the extent of the deviation and whether the health field work, etc. There's a lot of things you would anticipate happening when you do failures. So they went with all that hypothesis and they said this is how we would do Game days. They constructed some templates. Some of them are available on the Net today. Different people have customized it based on the kind of applications and services they provide. They have their own version of Game Days. And you can find a lot of material on the Internet, especially by folks from Amazon with Amazon Prime and all these things, how they carry out chaos engineering and how they maintain resilience, etc. So this is the whole practice of chaos engineering. The people from the Ops and then the people from this group called Saris, you might have heard of this word called Sari, site reliability engineering or site reliability engineers, something that Google popularized. You must have seen the Google SRE book, which is a very popular book. Google crystallized all their best practices around how they manage the collection environment. And then they wrote that in that book. One of the very interesting things that they mentioned was SS are not pure Ops folks. They are actually developers and they treat you are environment deployment environments as they would treat an app. So everything that you would visualize as being done to an app or the way you would visualize an app life cycle being constructed, something similar happens to their deployment environments. Also, right amongst this big group of organizations that practiced and advocated CAS engineering, Google was one of them, Netflix, Amazon and sales force and a lot of these people and they brought in this concept and later it became a designation. And you see a lot of SREs today on LinkedIn. The Saris were the people who carried out chaos engineering, and that's how it has been done for a long period of time. Many folks and in organizations with multiple layers with different silos think as engineering is a very, very restricted practice that's been followed and implemented by a core group of people, but not something that is accessible. Information is or as a activity to a lot of other people say developers testers QE engineers, performance engineers, and the like. It was being done by a closed group who would do it either in production or they would do it in environment that closely mimicked production may be pre proud or staging environments, as you would call them. And this also was adopted by not too many organizations, but by a few. And the organizations that did it often had differences or very subtle philosophical differences or departure from the QA methodology of testing failures as QA or, you know, as a developer or QA engineer, I'm sure a lot of people would be doing failures. We have had a lot of things in the past system tests. We would call them. We would call them as kitchen sink tests. Go and remove the power plug and bound support on the switch or, you know, just basically report a node. This is not something new. This also forms the core of what is done as part of CAS engineering as well. But there are some certain differences in the way a person would approach a chaos experiment, as they would call it in CAS engineering versus a failure test in a QA environment. For starters, the environment in which these steps are conducted in QA are often very sanitized. Environments may be clusters or test bits brought up just for that purpose, either manually or as part of some CIC pipeline on automation. There's not much dynamic, there's not much Dynamism or whatever you call associated with that environment. It is generally very clean, running workloads specific workloads, and you go to the test. You see the behavior a lot of times. The focus is on how the application works in QA, whereas when you do chaos experiments, it is often a setup that has been running for a long time, used by multiple people with applications and services, having gun upgrades for a long time, running a lot of load. Hey, there. This is production that I'm talking about, or staging environments that one would try to mimic and keep very close to production. So you have things fester on for a long time, give opportunity for failures to build up a system utilization to sort of go beyond a certain point. And then you do chaos experiments there. So there is a difference in the environment. The test Wed for one. And then there's also the change in approach or what you are trying to look at. The chaos engineering is a lot about the bot rather than the how. So you're not really interested in which service is behaving exactly which way or how it is doing that. What is the underlying mechanism of functionality that's been written to behave in a particular way? Rather, you are interested in the end view that is expected out of your service by end view I'm talking about. So you might have heard of the terms SLOs and Slis as performance engineers are very familiar with these terminologies. When a service is built or architected in response to a request for the service, you come up with some SLA service level agreement. Right. And then those agreements are broken down into so that probably correspond to individual pieces of the service, the largest service that's being provided surveys. In this case, I'm talking about you can think of it as the service that is being provided, the functionality that is being provided to an end user. Then that is broken down into several component services, the actual technical pieces running on your info, and you have an SL against all of them. And you have some service level indicators that tell you whether the solos are being met or no, these are generally cut off against some lies that you might have. Let us say you are supposed to be operating under 200 milliseconds latency, average latency. So the average latency here is the SLA and the 200 milliseconds is probably the cutoff and the Slo contributing towards the larger SLA, which then translates into that 59690f your eventual platform or service. So chaos engineering is a lot about monitoring these end goals, not really looking at how the system is accomplishing it rather than whether it is able to accomplish the end point end goal. So this is the slight differences between failure testing and chaos experimentation, though you could use the same tool to accomplish either view it's about what purpose you go with when you go ahead and do this failure. You can make it a chaos experiment, or it could just be a failure test to run as part of your CSR pipelines. And there's also another thing. Yeah, I think there was a question.
I have one question. So CYA engineering. Right. So where does it fit? It fit in the ship right. Methodology. If it is a shift right, how can we move that to shift left?
It's an interesting question. As engineers or as developers or a performance engineers, we often hear this word shift left. We made very popular with Agile and Scrum and all these prints and all these concepts. You would like to basically ensure that you're testing things earlier in the cycle the moment you have something, some deliverable piece, not leave it towards the end, as you would with the traditional waterfall model. So this is the shift left philosophy that we have, but chaos Engineering sort of evolved out of a shift right revolution. There is a Twitter account called Copy Construct, which I generally find get fascination for, and it's backed by a person called Cindy Sridharan who writes a lot about resilience, etc. There is a very good shift right article that you might find there proper share the resources after this call. So chaos engineering sort of started as a philosophy because of the shift right. So it said that nothing can come close to what happens in production. And the way your system responds to something failing on production is very different from how your system would behave when something failed in a very sanitized environment with not much load or not much dynamism, et cetera. So you basically need to end the dependencies that you encounter in production are far more. For example, for starters, the infrastructure upon which you deploy your in services often is very different from your test environments. Test bits. So you really don't know what is going to happen until you inject that in production. So production is the farthest right that you would go. So chaos engineering started as a shift right philosophy, and that's how it has been for a long time. So to answer your question, and even chaos engineering traditionally started with shift, right? They said production or the Environment's closest production is where you do chaos, but it's changing in the recent times. It's good that you brought this up. That serves as a lead for what we can talk about next. So there is this big. So I think I sort of gave an overview of what engineering is and why practicing it is important and where it is done, how it was brought in as a concept, how it differs from failure testing, things like that. Now that we've set the background for this testing, and also said that Shift right is where CS engineering is done. Let's talk about what has changed in chaos, enduring the last, say, two or three years. And that's where the shift left is coming in. And we'll talk. So before I do the other, any other questions?
Yes, we can take a couple of questions and then we can move the topic.
Hey, Karthik, in this is it. I have a basic question. Like one of my questions is asked by Navin. But before that I have a few questions. Like, oh, my question is like, where does performance engineer or test engineer fits into the gas engineering? That is the one. And as you mentioned, it's not limited for SREs, which Google created. And the second question is, who are the decision makers are over the product owner, who is a big person or architect who design, who give the target this kind of as testers of gas testing has to be done. And the third question is, there is no limit of the test cases like kind of a test or what you call CAS engineering use cases you can implement like that. As a performance engineers, we may monitor a lot of lot of counters, like any level of counters, either it infra or OS level or anything. So every counter could become a test case in the case. So is that right?
Yeah. Great question.
I do have a fourth question.
But I'm already losing track of them, so I try to answer this.
That's fine. You can go ahead with three questions. I'll ask the fourth question later.
Here is the first question was, how does performance testing fit in? That's a great question, and I think very important considering our target audience. Also, performance testing. Performance evaluation during chaos is extremely important. And when you perform chaos or practice chaos experiment, you would do that in let's say when you're doing it in production, you would not be running additional load to what is already there because you expect the system is already under significant duress. When the experiment is performed, the fault injection is carried out. But when chaos is done in pre proud or staging environment, that is where it becomes necessary for us to make the effort to simulate the kind of load that you would do in production. And often times that happens using performance tools. Performance load generation tools. There are classic cases of people using locust Asics Vegeta, a lot of such tools in order to generate load on the system and generate enough traffic while the Chaos experiment is performed. Now, there are two aspects to this, as performance engineers. Like you said, there are a lot of counters that you look at. There are a lot of aspects of performance of the system is characterized by several parameters. Let us say there are hundreds of parameters that you are looking at that describe your system's performance, and when you're doing chaos, it becomes necessary for you to sort of look what is changing when you do this. But the load that you are generating can either be a very specific profile that simulates a user condition. The user workload, as they say, which sometimes involves I think there are some TPCC benchmarks and things like that which even simulate user delays, user time, thinking time. So you try to keep it as real world as possible when you go ahead and do chaos engineering just to sort of simulate conditions. When you are trying to a prep for Black Friday, for example, there is an organization which is going to, let's say, flip card with Amazon. There's a sale coming up, big billion day Black Friday, things like that. You know, the traffic is going to hit the roof. So your intent on stressing out the system to Max possible. You load generators are running those kind of profiles that you want to run that kind of load. So that is one thing that happens usually and monitor all those counters as the failures happen. Then the other way of looking at it is you also try and simulate the user traffic patterns. Maybe we can nights, afternoons. There is a specific waxing and waning pattern of traffic that you anticipate, and then you are using your load generation tools to generate those profiles, and then you do chaos. So benchmarking and performance. As I understand it, I am not an expert, but the way I understand it is you look for specific workloads and look for, you know, how the system is behaving you do absolute and later benchmarks under a different kind of conditions system in all component working state system in a degraded mode, et cetera, et cetera. That ties in very much with chaos. Though the kind of profile that you would run during the course of the injection might really vary depending upon the intent behind the experiment. So performance tools really plays a big part in Chaos experiment. Performance and benchmarking tools. Load generators play a big part in chaos engineering and chaos experiments, or fault injections may be used as part of a performance distinct suite as well. If you're trying to get what I'm saying as part of an experiment and as part of hypothesis, you might be using the load generator as an aid. But on the other hand, if you are building the performance test suite, then you might use some Chaos tool, some fault injection platform, let's say Litmus or Rambling or Chaos measure whatever it is to actually cause a fault in your system and sort of give you the right knowledge about how your system behaves under a specific workload with a certain failure. So it's there very much hand in hand. I would say the way in which the Chaos tooling and performance in Smart tooling is utilized during the course of an experiment sort of decides, you know, what is the inference that you try to drive? I hope that answers the question on performance with Chaos. The other question was about who makes a decision around the chaos engineering, what test should be done, who should do it when it should be done, what environment it should be done, etc. That again, is changing. I think for a large part of the decade it was adopted in mature organizations which had well build resiliency practice. It's often the Sari. It lets the head of site liability engineering or some such personal, which would actually say that we need to sort of have the Chaos practice. And then you would actually expect the site reliability engineers to come up with the Chaos scenarios and then they do it in game days with all the right people involved. So the decision making would probably be from the top down from the, let's say, head, but off late, it is changing. It is becoming a little bit more bottoms up. Developers are also doing chaos engineering. That's the part that we will talk about next. And this is the change that has happened in the last two or three years. But before we go there, I also like to point out how the general adoption pattern happens for Chaos. People don't want people are generally very apprehensive or hesitant to do chaos engineering because if you are going to do chaos engineering, you're implying that system could have some weaknesses. Nobody likes to admit to having weaknesses. So people are very hesitant about doing care engineering. They leave it to the application delivery teams, the test and the development and performance engineering teams to save you. Please. Still, as the false and gives us the finished product, we'll ensure it always runs and we monitor and we observe. We do patches and upgrades. If something goes wrong, we'll get back to you will rise as one, and then we will fix it. Right. But they're very touchy about, you know, doing failure injection in actual deployment environments, preproduction and production runs. It's like very antimony us to them. They don't want to touch it. But one way the chaos engineering philosophy and practice spills through is through doing fault injection on Observability in from this is a low hanging fruit. So in all organizations, the Observability infrastructure is very critical. You might have an array of tools that you are using different APM solutions that you might have employed in order to see how your system is behaving, and it is really important to have them running for you to be able to constantly monitor, get the right alerts, notifications and everything, but somehow they are not in the direct path of your services. Let us say, as someone providing services platform services to a user base, your Observability in replace as much important role as your actual application business logic, but it is still a secondary thing. It is helping someone on this side by the side. I'm talking about folks maintaining your infrastructure than the other side, the people consuming your applications for them, the application and the availability of the application is paramount, and you are the folks who are trying to ensure it is available. That's why Observability is so important. But people who try to adopt chaos do it on low hanging targets, and they typically tend to do it. And this is based on some experience that I have had in speaking to people who started chaos engineering adoption because we were trying to advocate it and push it and eventually is left within the or so they basically trying take down their Prometheus, for example, or their alert managers or whatever service they are using for Observability. And then they see what is impact, and then they go ahead and see what is the benefit they sort of derive out of this. Then they take it to other services that probably you can say, second, fringe services, then they take it to the main services. It sort of goes from there. The decision making for this often rests with the Ops team, the sort of DevOps or SR had. All these kinds of designations are personal and percolates downward to the team. That's how it has been for a long time, but it sort of changed. I would say in the last two, three years. I hope that sort of answered your question. I can take the next set of questions once I talk about cloud native chaos.
Yeah, pretty much, but I would like to request would you also give us a practical amount like a real use case which has taken and also the mitigation which they follow to come across this kind of monkey store wide?
Sure. Yeah. I'll probably talk about the adoption stories of Litmus. That's the tool that we are building and how some people are using it to actually conduct chaos and getting a prop that might give us some games. Sure said I'll do that towards the end of the discussion. Going ahead, we talked that we said there are things changing in chaos entering over the last two, three years. We've been telling this in the last few minutes, but what it is really you are all aware of the cloud native paradigm that has come about. When you say cloud native, you are implicitly reading it as Cuban etes native because Cuban Nates is really substrate upon which the entire cloud native world is evolving. Today. The first graduated project so to say within the Cloud Native Computing Foundation, which is body within the Linux Foundation, that is the home of a repository for a lot of open source cloud native projects. So what is cloud native? The term itself is I'm sure a lot of you already know about it. You might be building cloud native technology your Sims. Just for the interest of this discussion, cloud native means a lot of things. It is an approach. It is a technology. It is a set of principles. So many things. You must have heard of the twelve factor app and you must have heard of the term called micro services. Containerization. People used to host services and physical servers. Then they went on to hosting it in virtual machines, and then they made it containerized Docker became big. And after it was containerized, people needed an orchestration mechanism to manage this containers. And that's when Cuban Tes was built. There were no made in mesosphere and few other orchestration systems that were there. But Cubans became the defector orchestration platform. And why containers are important is it is probably the best way to run micro services. And micro services again are another paradigm where you used to have something called monolith applications, which were self contained, had everything sort of residing in one VM different functionalities or services or pieces of an application where sort of built together and shipped as a binary. Earlier, popularly, this was known as a monolith approach. And this got then broken down into several loosely coupled services. People started calling it micro services, and people started developing and releasing these micro services independently. And there was this whole revolution about shipping fast shipping independently and integrating loosely. And that's the whole concept of micro services. And people figured out the best way to host these micro services was to run them in containers, because that gives you the same nest of environment. Operating environment. Open to container is the same. Whether you run it in your laptop or by sea, you run it on a so it's also a very easy way to test it. Ship it, package it. That's when continues became very useful. Container runtime became very popular. Docker came first, followed by a lot of other things, I, container. The CRA. Will container do a lot of things. And then you had orchestration platforms to manage the availability of containers because containers where designed to be ephemeral nature. So you basically want some systems to monitor how the containers are deployed and they are brought up and they're available all the time. And how do you manage the life cycle? How do you allocate to sources to them everything. So Kuber etes became very popular. So now this is what the cloud native world is all about. A lot of organizations are re architecting their applications from becoming from knowledge to micro services architecture. They are undergoing a change in the way they deploy the services for people to consume. They might have done it on their own data centers, they might have moved to Cloud. They might have been hybrid for some time. They are now going to Kuber Nates. Some people are running onpremise Kuber Etes. Some people are running managed services on GK and EKS and is your AKS everywhere. And not only are they running their application services in a micro services way, their applications in a micro services way, but all the supportive tooling that people used to run DNS Observability storage is also now become micro services. If you go to the CMC landscape, you need a magnifying glass to look at it because it's all crammed into one page into several subcategories. And there are different kinds of touring available there. So when you run your modern day application and provide for your consumers, you are most likely running it as a set of containers running as pods managed by higher level controllers like deployments in stateful sets and demons its incubates and a lot of supportive tooling that you might be running service meshes contain raised storage providers like Open EBS or Cluster or Set or any of these things or monitoring like Prometheus and Grana or Dina Tries or any of these things. Maybe you're running Core DNS or Cube DNS. Maybe you're running Kafka or Rabbit in your message because everything is running as different micro services. So now because you are running everything in such a loosely coupled manner, everything is a set of micro services which are undergoing their own patches, their own upgrades, their own releases. And because they are running on Kuber Nets, which is itself a very, very dense platform with a lot of moving components. You have different set of control plane services running in Cube nets prior to which inhibits the working of your own app and your own services. So you are at the mercy of a lot of hundreds of moving components in your deployment environment to ensure that 99 95, 99 99 availability. So cars engineering becomes much more important because the surface area for failures is that much more so. This is the reality of chaos engineering in the cloud native world. So what then did this influence in terms of practice? People who used to advocate that things have to be shifted right? Run on production or three fraud environments now started saying, I have done a lot of changes, boss, to my application. I am running it on a new deployment environment. I don't have the courage to run it in production right away. I cannot adhere to the golden principle of chaos engineering. To run chaos in production, we need to take a step back and start doing a lot of failure testing. A lot of scarce engineering in preproduction staging environments. Go further left. Run it as part of ICD pipelines and most probably as part of your CD. You have your staging environment into which you are deploying, and then you immediately trigger chaos experiment as a Chaos test as a sanity test, people would be running much simpler sanity tests earlier, but now the norm is to run chaos experiments. Part of your sanity validation once you have done upgrades, once you've deployed your new image into your production, and the process of upgrading into your production environment or staging environment is through entities called getups controllers, things that keep your source and deployment environment the same. This is true for infrastructure, the infrastructure as code, as also for your applications maintained as em in your Git deployment. You have things like Argo Helm, Flux Heal, a lot of these tools which ensure that your applications or infer changes are reconciled onto your deployment and with Mint, and you want to validate how good the change is. You run failure tests there. That's where Chaos tooling is now being adopted, much, much more. And I would say chaos engineering is a practice which was very esoteric, very limited to. A group of people like SRS and Ops became much more ubiquitous and democratized, and it has started seeing usefulness started seeing usage by a lot of people who are not really only a series that is developers and QA engineers and DevOps functions, people who are building and maintaing pipelines building release engineers. A lot of people have started using chaos storing to validate the effectiveness of changes that they are shipping to validate whether things are good. And this is where the lines between a failure test is done by QA earlier, and the Chaos experiment test done by the typical SRE is sort of blurring. Of course, SLOs continue to be validated, but the validation of the SLOs also is automated and built into the experiment. And you basically take the hypothesis that we have and the chaos intent that you have and burn them into an experiment which is then carried out in an automated fashion. Let us say we take the scenario of doing a network class. I'm causing a black hole attack 100% network packet drop. I'm causing one of the services. Let's say a balance application. Let's say one of the services that provides the a balance reader in a payment app. Right. I'm going to inhibit all the network traffic to that app. Then I am going to see what is going to happen, whether there is middle were intelligent enough to route it to a different replica or a different a different standby service, or whether it fails. This validation is built in into the experiment using different techniques, different things called probes, as they call it today in the different chaos touring world. And there is also a huge set of downstream applications whose availability we can monitor. And you build in all the hypothesis into the experiment. You define both the chaos intent that is to cause hack a drop as well as the validation intent. You put everything in a declarative way in a YAML, and then you let your GitHub controls take over, push this manifest a manifest or experiment on your cluster, and then there is some controller or reconciler that actually carries out. This business is running as a service on the cluster, maybe itself running as a pod, which will then carry out this chaos experiment, then give you results and probably push the results as matrix into some observability infrastructure, which you can sort of use. You can probably instrument your dashboards with that information. You can glean it through reports and alerts. And whatnot this is how chaos engineering is sort of moving today. And the load testing and performance tooling also plays a huge part in this. When people are doing everything in pre pro and non pro environment, the importance of performing storing in chaos experimentation becomes much more, and those also are burned into the experiment itself. So you have one of the experiment probes launching a performance tool, even as the fault is done, then there is another probe which is actually monitoring for what happens when the part is happening, whether your hypothesis is proved or disproved, then there is something else that is actually pushing all these metrics into an observability platform. So all this is happening in an automated way. So this is the change that has happened in chaos engineering over a period of time. And this is where the decision making process of how to consume chaos also is changing to be a little bit more bottom up people are, and especially because of the proliferation of Chaos tooling in the open source space. It used to be a chaos monkey, of course, was some open source. But there was a restriction that Chaos monkey can be used only with spinnaker. When Eric actually created it, and a lot of people who were not using Spinnaker were denied the benefit of using Chaos monkey. And there are a lot of tools sprang up in the open source space. People started adopting it, developers started using it. I started using it themselves and talked about its benefits to their team members. They advocated it within their teams, and eventually it sort of became a policy. Right. So that's the bottoms up adoption of technology that you would see with other things, probably, let's say someone released a new editor, the first consumer of that would be a developer, and then slowly becomes the tool of choice for a lot of other people, and then becomes a part of the organization, a list of tools to subscribe to or buy licenses for, and they start adding that as part of their culture. Something very similar with chaos engineering is happening today. A lot of bottom up adoption is happening there in that space and coming to the subcategory term called cloud native CIOs engineering. So this is where I sort of like to bring in what we are doing as a company, as a team, as a project. So sometime around 2018, in a company called Maya Data, we were trying to provide a containerised storage solution for Kubert, basically trying to sort of make use of commodity storage available on service and pull them together and create some virtual volumes and provide it. Attach it to containers using some external storage provisions and CSI controllers and things like that. That's what Open EBS was about. And a lot of people were using opens on different cloud platforms. A huge section of them were using it on Hetzner. Cloud is a popular cloud service provider in Europe, and at the time, people were running into some issues there, and they said that I'm seeing my pots get killed. They basically move off to different nodes. Storage takes a long time to reconnect their my applications are facing timeouts. How are you guys testing resiliency? And if you are testing something, why don't you share that approach with us? Open as in open source. Why don't you share some approach with us so we can start using what you guys are using in our own deployment environments. That's when Litmus Chaos was born to test the resiliency of open base. And we started off doing different failures on Kuber Nets network losses, latency injections, taking down Kuber Nets nodes and pods, sending different kinds of termination signals, eating up resources, memory and CPU and disk stressing disks, taking notes down for maintenance, causing evictions. A lot of these kind of failures we were trying to create, and we built a project around it, also open sourced it. But over a period of time, we saw a lot of interest getting generated for Litmus. And we sort of started building road map dedicated to Litmus Chaos and not being so very closely tied with openings because we felt any open source, I mean, any application or any service running on Cuban it stands to benefit from Litmus. It broadened its horizon and created a separate roadmap of its own. It's a wide adoption, and it saw contributions coming in from organizations like Red Hat into it and a lot of others who started using it. And then we ate. So we donated its Chaos to CNC. And it is a sandbox project today in CCF. And we've applied for Incubation because we've seen a lot of growth in the last two years. And we recently released the two or of its to provide EndToEnd Chaos engineering platform, not just a tool which can help you run experiments or run faults. Their layer intent was to make those faults available to people off the shelf, readily usable experiments in a declarative way. People were using it on Kubert. So we wanted to give folks the same user experience that they had with Kube Etes already. Everything is Ml, everything is declarative. Everything is a resource, and there is a controller to manipulate that resource from actual state Besides state, whether it is a native resource or custom resource in Cuben Tes. So we also created Chaos custom resources to define the CIOs intent in a declarative way. And we wrote an operator that basically reconciles the Chaos resource and carries out the fault injection business logic that must let us one X. But over a period of time, as Chaos enginering adoption increased, as we spoke to the community learned a lot got newer requirements. People wanted to manage chaos across different clusters in their fleet from a centralized a centralized, single pane of glass sort of management plane, rather than install custom resource definitions and operators in each clusters that they were operating. And then people also needed some kind of analysis on the results that they were getting from his experiments. People wanted this validation to be built in, he said. People are no longer sort of peering into their APM or Observability dashboards as they do the fault. They want the system, the Chaos touring, or the framework to sort of validate a lot of this hypothesis that they have bring back that results, and they are free to define a set of constraints and you have results against each constraint, and then an overall success factor for that experiment that they carried out, and based on which they can compare the resiliency of their application or services against a particular scenario, then they compare that score. The success factor that they get across environments may be Dev, staging and production, or maybe across releases. Then see how, whether improving or they're going down in terms of Brazilians et cetera. So these are some of the capabilities that we built in into it as to to and then released it recently. So that is where we are now. Litmus Chaos is the open source platform. As Chaos Native. We were part of my data that's well, Imus is born, but with popularity of Litmus, we decided that it needs a dedicated focus, resources and time. So we spun off from a new organization called Chaos Native, just solely focused on providing enterprise support to Litmus Chaos and also continue to further and develop the open source LMS Chaos as well. So that is what we started doing as Chaos engineering people. And this term called Cloud Native Chaos was sort of coined by the maintenance of this project, and it was well received and sort of taken too kindly by the overall Cloud Native community and the Cloud Native Foundation computing foundations that have published some blogs around it. And that's where it is available now as a subcategory. Or rather, it is recognized as a valid sub category within Chaos Engineering. And the other option of Litmus has been going on for some time. And as I asked a question on can you give us a real use case of how somebody is using Chaos Engineering? I will probably take the example of it must tool how Red Hat is using it. So what they're doing is they have a platform called Open Shift, which is which is basically a Cube notes at its core, but with a lot of enhancements and a lot of management tire built on top of Cabernets Open Shift control pane is being tested using Litmus where people are actually trying to take down some of the core control plane services of Open Shift and validate whether they are able to handle it. Where their workloads running on OpenShift continue to work despite taking down the control plane services. Have they built enough redundancy in there to be able to continue running the workloads and provide user services? That's one popular use case and there are other organizations. For example, there is a telco operator called Orange and they are slowly running their network functions on berets cloud network functions as they call them earlier. It used to be virtual network functions. Now they have started running into functions on cloud incubates. We firewall something as simple as firewalls more complicated telco services. They are trying to kill or inject different kinds of faults onto their system. They are running Cube nets on open stack and they are using Litmus to do impairs an open stack and do failures in Kuper notes for their CNS and validate whether they are continuing to achieve the so that they have. Mostly it is testing whether their high availability works with certain constraints. They make use of something called it as probes in their experimental definitions. And those are some of the use cases about how people do mitigation. I don't know if I was able to answer that part convincingly with what I just told people go back and fix either their deployment practices. Maybe they add more nodes, they have better autoscaling policies. Maybe they have more resources provided they provide the right limits for the CNI as to how many IPS or how many parts can run on a particular node. They ensure that people have the right observability frameworks. Sometimes you are okay seeing failures, but you want to be detected. You want those failures to be detected quick enough and you want to be able to ensure your incident response is good. Chaos engineering is not only about testing self field is also a lot about verifying your readiness and incident response capabilities. Sometimes you are also checking people resilience through chaos Engineering. So how are you able to go and identify root cause based on the notifications that you are receiving and how quickly are you able to fix it? How quick you are able to get system back to operational state, but that's also something that people focus on and add as part of the Mitigation agenda. So this is how people have been using gas engineering, specifically Litmus, and how people plan for getting better on how they do the Mitigation and all these things. I hope that provided enough overview.
Sorry to interrupt. So sounds like the chaos engineering is meant for more beneficial for the cloud for providers. Obviously Google, they have their own cloud factory they can provide. So how do you think? Lets me see as a Bank, I want to go to Benets. So why don't Kuband the CAS engineering they and say that yes, we have come across all the CAS in generic cases cases. Now you are good to take it. How does this sound correct? I feel like one or more like the cloud providers, they have to adopt and test all these gas engineering practice and give the service to the utilizes to a great extent they do.
I think the reason why a lot of people are moving to managed Kube Nites on these cloud platforms is because they are quite resilient and they are already doing the chaos engineering practice. But you would still not leave things to chance because the cloud is and Cabernets is only providing you an infrastructure to run things to an extent they can guarantee availability. Maybe they have things like, okay, you run a deployment and one of your part goes down. They can spin up another one. But that's not as simple as it is generally because let us say you are running your own database. Let's say you are running Postgres on Cube nets. You have a leader follower configuration that you set up in one of these pots. Go down. Let's say with Kafka or something that you're going down, one of your Kafka brokers goes down on AWS manage to service right now depending upon the CNI plugin that's being used on Cubans, the networking stack that is being used, depending upon the storage that you are providing, whether it's SSD, whether it is normal disk, depending upon the amount of CPU and memory that you have resource. Your cloud instances with the time taken for failover of, let's say of your postrace or your Kafka brokers might differ. And also your application might be very complex. You might be triggering off leader reelections. Your failures might not be very straightforward. There may be a lot of concentrations. There may be split brain scenarios that can still happen within your app context. Kuber nets and cloud providers give you a platform to host things, and they ensure a basic level of resilience. But on top of that, there are a lot of things that can still happen depending upon what your requirements are. And let's say you had a 62nd timeout consumer time out on your Kafka consumers and you're running it on as. And when you actually ended up running your chaos experiment for doing broker failures along with the standard benchmark, let's say you're using Locust to create some topics and run some data. You see that because of the combination of, you know, different services that you're running, because of the type of infra you're running and because of the choices that you made in your deployment environment, you end up seeing 75 seconds fail over time and not 60 seconds. So that is a learning. You might still end up facing outages or some level of user degradation. Sometimes you might be as a bank. Let us say we talked about the balance, let's say a payment gateway. I just have a console ebanking application or something is there? I'm able to make a deposit and I can also make payments digital payments. Now when I go make a digital payment, has to read the balance services to see how much amount I have left in my bank before I'm able to make the payment. Now, if I go ahead and inject 60 seconds or let's say 60 seconds is just too much, it will time out something like 2000 or 5000 milliseconds latency on the balance rate of service. Now there is going to be your front end. Are we speaking to that guy to find out if there is enough money and that operation is going to be stalled by this much time that might actually bubble up to your end user with much higher latency. So you have some latency inject into your backend service that sort of builds up across several layers. And by the time as a user, you see what's happening on your screen, you're seeing the rotate button run for much longer than you anticipated. The way the more complex your fries, the more network complexities, especially your Fry, involves, the more the impact of fault. And it's cannot be left to the cloud provider to guarantee resilience under all circumstances. For all scenarios, they will provide a certain level of resilience in hosting your services. But how certain faults propagate to the end user is something you have to find out. And if I were that good, a lot of people will still not be doing chaos engineering on cloud platforms. But the fact that they are doing it today sort of reflects this reality. In fact, as in the recent Reinvent launched their Fires Fault Injection fault injection service, it is a paid service or as to do for chaos engineering. They are encouraging their users to do chaos engineering because they believe there is a lot of dynamism that is brought in by people who are deploying their applications and they need to do chaos engineering. Air launched the Azure Chaos Studio just on 16th August. So cloud providers are encouraging people to chaos engineering, and I think it is still going to be needed. It cannot be left to the provider to ensure that everything is going to be up all the time, so they will make that possible to a good extent. A lot of people find it very difficult to run things in the cloud all the time because it eats up a lot of money. So they have on premise infrastructure. You build your own private cloud, OpenStack VMware or you have some hybrid model. You're running some high tech services on the cloud, maybe, and your other services are running in house in your own data center. So it becomes necessary for you to sort of run chaos engineering in places where it moves more. And also you have people running Federated clusters across several availability zones or regions to maintain availability. So sometimes there is a a lot of latency is creeping in. Let's say you're running a synchronous service, synchronous replication between your services and your service is running on different regions because you want high availability. Then we're going to add a lot of latency. And now even if you say .1 X latency is injected there, it bubbles up into 100 X by the time it sort of reaches the customer and the user experience is degraded far beyond what you injected in between. So you need to have a lot of intelligence in your middle where and chaos engineering is a lot about learning. That is also one place where it differs from regular failure testing. You learn a lot. There is something called unknown unknowns. So when you build your hypothesis, there's something called known known, known unknown. And then there are unknown unknowns. When somebody builds an app, build a service tool or platform, you account for an unlimited bandwidth, zero latency, a lot of compute resources, unlimited storage. But that's not often the case. You might run out of capacity, your storage might get disconnected is there might be media errors on the storage, and your discrete and rights might become very slow. There are so many factors that can creep in which one might not have accounted for. And those things can be sort of seen and found out when you do your experiments and you learn a lot out of it. And then you go back to the drawing board, fix either your infrastructure, your deployment practice. Maybe you are going to run more replicas, maybe you're going to put a better load balancer, or maybe you're going to fix actual bug in the application software. That is something that you might do as well. There are different ways in which people might mitigate and CAS engineering sort of continues to be important, even in the so called cloud environments, where generally things are more resilient because they are doing some testing on your behalf already.
Thanks, Cath. Can we take a couple more questions, Cathi?
Sure. Yeah.
Real. We have any question?
Yeah, thanks. I I got it from one time to Hi. I wanted to understand, how frequently do you since you have been in this space for quite some time now, how frequently do you think these experiments should be done? And should the experiment experiments include multiple parameters, or should one parameter be tested at a time like you mentioned about networking?
Right.
Should that one experiment include only networking, the next one only storage like that? What is?
Yeah, that's a great question. You are right. How frequent is it? I think it depends upon the the organization and depends upon how mature their applications are. There are people who conduct game days, monthly game days, and things like that. But there are also people who two chaos experiments every day. I would recommend having chaos experiments run in an automated fashion as part of your delivery pipelines and make it very frequent, especially if your micro services oriented and you're putting things on Cuban Tes because there's a lot of change happening there. And if you've set up automated builds, and if you're in the practice, if you have the practice of deploying or upgrading your dependencies, often it becomes important to do chaos engineering that much more frequently. So I would say not too much is better than lesson. How do you actually run the test? Do you usually target a single variable? Do you target multiple? Good to start with single. In fact, if you look at the principles of chaos that Netflix and the team and folks sort of put up initially, the thing they said was minimize blast radius. So you are trying to make the failure as controlled as possible. Chaos engineering is a very scientific discipline. Despite the name chaos, you make very controlled failures and see how it behaves. So you start off with single parameter single variable changes and observe behavior. Once you have built up enough knowledge on how the system is going to behave to a certain fault, you do things in combination. One of the things we start one feedback we received as we did the rest open source project was we moved from doing simple experiments to workflows. It must. One X was all about experiments, and it has two. X is a lot about workflows where you can string together different faults in desired order. You can do false and parallel because when misfortune happens, it doesn't happen singly in real world. So you also do multiple faults. But that is done after you sort of understood how things behave to single component variables. So you start off with single variable fault, and then you sort of move to multi component failures and observe how the system is behaving. I think that's the process. It would start frequently with the test environments and CICR pipelines. Then you graduate to doing it in prep and production. When you go to production, obviously, it becomes few and far in between. It becomes more periodic, often at the larger intervals rather than doing it very frequently. It also depends on the environment that you are doing it in. So in production, it would, of course, be, you know, with enough gap between two game days, as they would call it. Whereas staging pre, prod and test, you tend to do it more frequently. Start off with a single component graduate to multicomponent values.
Thanks, Garth Shilpa, do we have any question?
Yes. No. And I do have first of all, thank you so much for arranging this session. It's very insightful. My question goes, how is chaos different than performance testing and engineering? Since we have different types of testing types or scenarios, which we usually do in our performance testing as well. So what the additional feature we have in chaos engineering?
Okay, so performance testing, I think, has a lot of objectives as you do them. One of the things that you would end up doing as part of a performance test or benchmark test. I load the system really high. It just saturated with requests. That is also a form of chaos. You can say in principle, philosophically, that is also inflicting chaos. But traditionally chaos involves failures, whereas it is not something that is a mandatory thing. When you do performance tests, you benchmark systems. You run performance tools. You visualize how different parameters or come through and how they change in relation to each other. I come from a storage background a little bit. The performances that we used to do on our storage systems involved running with different block sizes. For example, run 4K eight K to 56K one M kind of block sizes run sequential and random workloads run for different active durations. Cash warmed up without the cash warmed up. Right? Cash right through all sorts of things and see how the system behaves. You understand a lot about how efficiently on, how quickly you are able to serve requests and multiple dimensions of that, how quick there is latency there is throughput and IOPS that we used to view as the three pillars of storage performance. I'm sure that it differs depending upon the target or object against which the performance test is being performed. It's a lot about how quickly or able to sort of respond to requests and how efficient you are able to do it. That's generally the goal of performance testing, but often as part of that, you also take the system to the saturation point and see whether it is able to work or no, which is also sort of a chaos test. But traditionally, chaos is associated with going one step further and degrading your system by failing some component. You would be injecting a network class. You would be taking down a disk. You will be taking down a compute instance and let your system would optimally work in a four node cluster load balanced across four nodes, but you reduce it to two nodes and see really what is happening, and you're probably creating noisy neighbor conditions. Let us say your system is performing at a certain level. You introduce some rogue processes and eat up resources and leave your actual process with very less CPU cycles to work with or with real system memory to work with. So these are the deliberate fault that you're injecting in your system, even as you're running your load to see what happens. Right? Chaos engineering is involved to a great extent, these kind of this injection of faults, along with actual performance profiles being run and system behavior being observed. That's how I would sort of look at it.
Okay. Thank you so much for the answer.
So Chic, I have one more question. Is it possible to take out the entire region by doing chaos engineering?
I think those those kinds of tests are practiced. I have heard of there's a Chaos King Kong or Chaos Gorilla or some such thing that Netflix provides, which actually enables you to do things like that, take down availability zones, regions. I have not personally experimented with that. So let us started off as chaos engineering with an occupants context and then on starting to do queries against a non. I'm sorry, starting to do chaos engineering against non Kuber etes infrastructure while still running the business logic within Cubes. So that means you have the Litmus control plane running as a set of services or micro services on Kuber nets will be making use of the AWS SDK, the VMware and the GCP Azure SDK to sort of use their APIs to go and do out of band. And in the instance failures we are working on providing those kinds of capabilities simulate as failures and things like that. But those kind of tools do exist. We are also in the process of expanding our suite to include things like that. But that is also a practice. But that's something you wouldn't really try unless you're really sure of your applications and how they are working.
God, thank you. We have races. He races. We have any question?
A thank you. Thank you for pulling me up. Hi. Thank you so much for the insight. I got a question and I'm not sure you have already covered this. So there is a failover testing. And now we are talking about the chaos and testing. So what is the main difference, like how it is very different to the regular fail or testing, and also, is it something that we use for the disaster recovery as well?
Yeah. Thanks for the question. Rakesh, So there is a slight difference to testing and chaos experimentation, but it's more of a very subtle philosophical difference. Rakesh, Failover testing is generally a part of the chaos in letting agenda for a lot of people in different companies. So there is the tool that you're using to inflict this failure can be built in which way you can use it to perform a field over test, if you would like to look at it that way, or you can use it to perform Kiosk experiment. What is the difference between these two? Many times when you do testing or the traditional fall over test or system test, you have a predefined hypothesis, which is mostly about the how of the application, how it is behaving. You may or may not be really looking at the Los as part of the process of running the test. The chaos experiment is less about the how, and it is more about the what. And there's something called exploratory testing that we might all have come across in our journey. People say, okay, there are some test suites that we all wrote up our alms, and then we go and test it and fill up results and do all that. But then there is this exploratory testing that we do things we sort of did not document really, but we gave ourselves some time to sort of understand the project. Try out something with sort of idea of this is what might happen. But let us see what happens because we are learning something new. The chaos experimentation is false. The traditional definition of chaos experimentation falls in that latter category. You're seeing the end behavior, the user behavior. You're often peering into the silos. You might have set up three holds on your dashboard. You are seeing whether that day is breach or no, and if he is breached, then how quickly is it coming back? Mttr is very important. Does yourself feel really work? So those are the kind of things that you would look, but from a tooling point of view, you would use the same tool. You can bend it any which way you can use it for doing your regular testing. You can use it for experimentation, all these kind of things. That would be the actual difference. I think you had a second part to the question as well. Could you please repeat that? I think I just lost it.
Yeah. So I did mention thanks for the so I did mention about the disaster recovery.
A disaster recovery verifying whether that is really working is also one of the things you would do as part of an experiment. It's probably a later stage experiment. I would say you would be starting with testing. High availability. Ha and Dr are two aspects of resilience. Typically, the H is defined as, let's say you have an active and passive system, a passive controller or service residing within your deployment environment, and it's generally associated with inline failover. Active failover. What I mean is you don't really need any manual intervention and things happen on the fly, whereas disaster recovery is generally associated. And these are all the report terms. Disaster recovery in many parts. Many organization talks about having a golden copy which is undergoing some kind of asynchronous replication, and then you promote it to become the primary with some admin approval or some promotion process that you would do either manually or automatically, depending upon what's happening with your primary site. There is a chase more like probably within a site. You have active and passive components and you're bringing up immediately when one thing is going down. So I think you start off with chaos experiments that validate high variability. And there are chaos experiments which also, you know, talk about disaster recovery. Most of these region level failures and what we are talking can sort of be those kind of experiments, the ones that validated, even if there is a manual effort involved in promoting a site is active, there are some is involved there, how quickly as a team you can promote it, and how quickly you get on to the right matrix performance on the the now promoted system. So that's also something that you try to take away from a Chaos experiment. So, yes, that's also something that you would do in a Chaos experiment. Maybe, let's say day two Chaos experiment rather than a a zero a a one kiosk experiment.
Thanks, Karthick. And so we are here. We are everyone in this group, everybody into performance testing and engineering. So how can we become a Chaos engineer? So what kind of skills we need to have? What kind of tools we have to learn? Any insights on the cart?
I think you are already at an advantage because unlike developers, or maybe, you know, the specific QA teams nowadays, the QA teams are very tied with the components developers are building in all the agile world. You are already an advantage because you're looking at the system. You look at it from a system perspective, already closer to the way an end user would look when you're a performance engineer. So that's a great beginning. And there is a lot of tooling available today. There are some tooling which have internal Chaos engineering plugins. For example, take new load or take Dina Trace. And this is a trend that's catching up now. A lot of Observability providers, a lot of APM providers, windows, which is what you might be dealing with on a day to day basis, provide some Chaos engineering hooks where you can launch experiments from the their own dashboards to see what happens. And a system fault is injected as you run your load profiles as you run your performance tests. That's one easy path. But there are a lot of open source tools I understand there are a lot of people even use open source performance tooling, not necessarily enterprise tooling. And not all enterprise tooling has these Chaos plugins. You have a lot of tools today with great documentation, great resources, examples, case studies that you can start utilizing. If you are on Cabernets, you have a lot of them, even if you're not on Cuban Tres. And if you are in a sort of pre Cuban Tes infrastructure, even there, there are a lot of tools available which we sort of help you to start injecting some simple failures. And many of these tools that are available today, it most included, have some kind of hooks into the Observability platform. By this, I am not talking about the ability to launch experiments from those APM tools that I mentioned a few minutes back, but you have the ability to export metrics. Let's say a lot of these tools Cloud Watch or Dina Rasor Grant, which is what you might be using as part of your runs to visualize impact even as you run load from other tools in your sweet Chaos tools, export metrics or export events, or provide some way of visualizing your impact of the Chaos experiment on the application. On these dashboards, we call it as Interleaved Chaos Dashboards Capone dashboards. In Lius, we provide a plugin, but that's true for a lot of other tools as well. So you can sort of run your load, run your fault using these tools, and you make use of these hooks to view things in those dashboards. I think to give a short answer, you can start making use of a lot of open source tooling that is available around CIS engineering. Coming from Litmus, I would sort of request you all to take a look at it. We have something called Chaos workflows, which is a distinct advantage compared to some of the other tools available today. And when workflows, you can actually bring in a load job along with a fault job, tie them together and run it, and then see the impact of scars on her application dashboard. We are in the process of adding more probes we work with. We have skates probes and bash probes, Prometheus probes, all these kind of things to validate steady state hypothesis and trying to add more such probes, where we try to engage with more standardized performance tools that are out there, maybe Asics or Locust or Widget tools like this, where you can go ahead and define profile with that, you probably standard would be using in a standard manner in your test. Now you can define that as part of a Litmus workflow. Also tie together fault with it and then around it. And so you could use a lot of open source tolling. We are very much aligned and very much tuned to the needs of the performance community as the Litmus group, and we are trying to create more capabilities there for you to make use of. Yeah. There are some good. Thanks.
Hey, Cat, I have one last week question, like, so these false injections. Right. So you have categorized into three categories, no knowns, known, unknowns, unknown, unknowns. So you do have a set as standards of faults made for any application or any organization. And second question is, so how often do you think we need to repeat the same fault injection? So once it is mitigated, can we ignore that false injection and we can continue with all other things?
Right? Yeah. So on the first question, Litmus has already made or off the shelf experiments or faults for Kuber Nets and for some sense projects like Open Bars, and also for some applications like Cassandra, Postgres and Kafka, some native faults with some application specific hypothesis health checks that are embedded within the experiments. We are expanding that suite. It also now has the ability to do all our Paws, VMware, VMs and EBS and GPT disks, all these kind of things, data stores in VMware and all that. So there are some ready made for that you can use. On the second question, yeah. As part of a game diver, as part of, let's say, CIOs experimentation, run, you run the fault, see if it is mitigated already, then you move to the next fault. Or if not, you fix things, come back, execute it, then you find out it has been mitigated, then you move on to the next fault. That process is something that you would do in one installment, and then they would actually repeat it because your deployment environment is not going to remain the same. Your application might have undergone upgrades, or your info mate have undergone upgrades. You might have upgraded Kuber Ates versions, or you might have upgraded to a different node type. You might have made some oil patches that might have some impact as the member of kernel version change. Or you might have gone ahead and made some fixes to your application in response to a fault that you had last time. Maybe that is some regression of something else. So this is very similar to testing. So Chaos experimentation. You identify a set of Chaos experiments as part of your sanity run very similar to your regression suites, non failure regression suites that you would do and repeat them in an automated fashion. But you go and keep increasing the level of failure. Let us say you did a single component failure. Mona was asking some time back. You start off with one variable. You find out that your you've been able to identify the right mitigation you've built in self healing, and your automation is also yielding success for that experiment. Every time you run it, you basically level up and go to the next fault where you create a more complex scenario and the hypothesis also correspondingly becoming more complex. And you are basically running that experiment. It sort of has a subset of the previous fault plus another one. Then you identify what happens there and then you go from there. So I would say you need to repeat the Chaos experiment. It also is important. You might also have other incentives for repetition. You might have changed you might have migrated from one cloud to another cloud. Or you might have migrated from one flat dependency, other platform dependency. For example, you might have changed your choice of DB. You might have gone from Mongo to something else. You might have been Mongo. You might have been using one vendor's version of MongoDB, and then you might move to another version of MongoDB. You might have changed the operator that's taken care of Mongo. So many things that cannot keep undergoing changes. So you tend to repeat some experiments that you've identified slowly. Some of these experiments keep getting into the bucket of your a regression or sanity ones. Do you keep identifying newer ones that sort of make use of that fault plus another one to create a more complex scenario, but generally you would to repeat it.
Thanks, Kathy. Kathy, you conduct workshops, right? By any chance, it is scheduled in the soon I'm coming out shops.
Yeah. So we do conduct workshops as Chaos native, the org behind Open Source. Let us and you can register for a workshop here's. The first one that we conducted some days back. There's another one coming up very soon you can go to the Kiosk Native website and you will find the option to register for a workshop. The workshop is mostly going to be about a repetition. A little bit of what we discussed today over Chaos entering its history and practice, but it's also a lot about how you can use the CAS Native Enterprise platform built on Litmus open source project to conduct Chaos experiments and glean information and to get started. So we do conduct workshops. I encourage you to register and we'll be happy to sort of get you kick started on the CS engineering journey.
So how can we contact you if someone has any questions?
I.
Am available on my mail ID. I will probably share it. It's carthis at Chaos Native com and then also on Twitter. My hand layers at K subset that is at Ketch. It please hit me up on LinkedIn as well. I'm Karthik said on LinkedIn. Please feel free to connect. I would love to connect with you all and keep in touch. And there's a lot that we as Chaos engineering community. I know that today was a little bit of a one sided conversation where we were speaking a lot about Chaos and and how it ties in with how performance testing and performance benchmark tools can be leveraged within the CIA's experimentation context. But we stand to learn a lot from hearing you experts on performance. I'm really looking forward to listening in on more more discussions. That and you all are putting together on performance engineering and sort of take those learnings and make a better engineering platform. Take those learnings back to fitness and Chaos Native.
Yeah. Thanks. Excellent. A lot of information to process in the past 90 minutes. So that is why SSN is being recorded. It will be published in my YouTube channel. Probably. You can also send a note or you can put a reference in your community. Also, people can refer the recordings.
Sure, that would be great.
So if no one has any questions, then we can conclude the session. Many thanks, Karthik. It was very insightful. So you brought up a whole new perspective about the testing and chaos, so it will be very helpful for us to do something to get started, at least in our project. So thanks for that. And we will anyway, register for the workshops, and then we'll try to learn a lot of things from the website you referred. I also put a note in my GitHub repository. You can also make a reference. Thank you, everyone. I don't have any questions. We can wind up many thanks, Catholic again. I hope we see you again soon.
Yep. Looking forward to being here. Thank you so much for organizing this and chase, everyone. Bye.
Thank you.
Have a good weekend. Have a good night. You see you. Bye.
Like.
