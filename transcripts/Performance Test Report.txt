Hi Everyone. Good morning. Good afternoon. Good evening. Wherever you are. Thanks for joining. Let us wait for for one or two minutes and then we can get started. Those who want to speak participate in the discussion. Please raise your hand so that I can admit to the stage. Just wait for one more minute, then we can get started. Thank you everyone, for joining this club meeting. This is our weekly session and this session is being recorded. So you can also view the previous recordings in my YouTube channel Insights. So let's get started for this today's topic. So today we are going to discuss about how you can present your performance. Just report to the stakeholders. So what kind of template you have to follow, what details you need to incorporate and based on the audience you are addressing how to a present to your statistics. So those things we are going to discuss, if you want to take part in the discussion, please raise your hand so that I can admit to the stage.
Okay.
So I know many of you will be sharing the report to your stakeholders, slide for architects, Development team, or even C or C based on the audience, you may need to tweak your reporting. So that's what we are going to see. How many of you send a report on a daily basis to your team and what type of template or what type of mode of communication you follow. Any input I can get, how many one whether you use email or slack or something else or just a verbal communication in.
We will send him through. Email in general will be executing the load test. Actually, what is the purpose of the low test? And is there any SLS defined? We'll go with that SLS and compared with those results will will jot down to the Excel spreadsheet and share it with. If there are any on site for an onsite customer or a directory customer will share it with them.
Okay, got it.
So I'm going to share my what I follow from past many years. So I'm going to share my views. And probably we can take the discussion about getting information from other folks as. So first thing what I do is first when the test is completed based on the project I'm working. The email subject should briefly tell about what it is. So the email subject could be like a project you are working on. The release you are working on the title of your type of testing light. For example, it will be load testing, stress testing or endurance testing, and also which run you are going to present. It could be run on or run to some baseline run. So some kind of brief phrase you have to put it in the email subject so that by seeing the email subject itself, people should identify. Okay, this is the report for this particular project under run. So make sure you email subject to clear and concise. And then once you compose the email subject next comes to the the email body. So in the email first thing I have to mention is apart from your usual salutation, right high and project detail. So on. So the very core things I'm going to tell you. So very first thing you have to add is the timestamp timestamp of your run. So that is very much important, including the time zone, which time zone you executed the test and from which start time and the end time and the duration. So this would be my top very first information about my test aggregation, followed by the time zone and time start time, end time and duration. Next would be my the core output. Basically, whether this particular run has any issues are not basically how many SLA has been violated. What is good? What is bad is it would be like a color coding basically. Say for example, if it is everything is fast, it will be in green color. Something is warning, it will be Orange color. Something is bad, it will be red color. So I will just put some four to five points about the run, including the load details. Okay, so and so service has been tested with this much load and the load has been injected in this fashion and we have seen the response time or degradation or improvement. So everything is a very short cut size two six bullet points so that people will just see open the email and just see that they understand what's going on.
Okay.
So this is what I particularly focus on the first part because no one well scroll down everything and see basically the first. Open it and see. Okay, that's a good or bad something is wrong. What is wrong? If something is good, what is good? So your email should focus on that part. Actually, then you can furnish about your a table view details and the other secondary metrics, mainly response time. You can focus on your bullet points on CPU memory, garbage collection, and then the top five. Watch the transactions or top five good transactions and any issues or anything is improved or not. So this five to ten points is only your actual skill comes in the picture, has a performance test arrive. Your skill set is not just executing it is about how you present your results and findings. So make sure you are not writing anything like a paragraph like a story. Don't do that. Everything is a bullet points and appropriately color code and appropriately bold or underline your text. So make sure you compare your previous results. That is also a good thing you can do. Always compare your results with the previous run and also the last week run or last release run. So that comparison you always do. Put some Excel sheet, create some trend, report, a lot of tools or support ending that to run comparison. You can do, and then you can just present your findings. Okay, so let me admit a couple of folks.
Hi.
Don. Thanks for joining. So any questions in this context, like the first top few sections, any input from any other folks about this?
Yeah, he even say here.
Hey, I.
Think you have given almost everything. No need to ask here. But the one thing I thought of mentioning here like it's like a baseline to benchmark comparison. Always. What we do is you have to remember the previous one becomes like a previous one become a baseline and the current one run become benchmark. You always compare and submit that only one point. Apart from that, we don't say based on the test results, it's like a requirement if it's a release major release. Yes, we include all the main metrics like even GC and CPU and app level metrics or any DiNatale metric. But unless until it's a normal load test, just like as you said, only response times and resource metrics. Not in depth level.
Yes, we can talk about APM eventually. First of what I do is right. Whenever I compose the email first, I want to tell what is the purpose of that particular run or a particular release product. One right for tones or architects, they are interested in whether it is good or bad. If it is bad, what is wrong?
Yeah, that part. So we have only first line if it's an Excel sheet that highlight even Excel report. So what we do is what is observation. What is the outcome of it the main point. Unless until if it's good, most of them they won't look into it is if it's anything highlighted, they will dig in. So there was a point where I submitted 95% response time. So somebody asked what is the 95% less time? So I try to example so they don't care about all these things. Don't even submit unnecessary things like you suggested. Unless until it's required and some stakeholders may not understand all the terms and terminologies just abbreviate if it's possible. That is also an re.
Usually I worked on many projects, many clients. Whenever something is good, they don't even care about our emails. But if I say something is wrong, okay, then then every attention will be on the performance, execution or performance emails. Then everything will get screwed. They will just a deep dive. Okay, why they ask so many questions? So then only we get some attention. If everything is good. No one cares about performance.
So we used to follow one standard templates where we used to give all the metrics in the Excel, but my manager strictly suggested don't give you the necessary knowledge unless until it's required. Yes, if it's a problem, you just give the slow transactions and slow queries and metric levels you are analyzing and with crops and everything until required. Don't even post unnecessary just outcome and make sure as simple as that it can be understandable for everyone. That's what we go check it.
Yes.
Ok.
So after these few points, I would focus on the actual statistics from the tool. So that is why the next focus will be. As you know, it depends on the tool you are working. It is whatever I'm going to tell it's a tool. Agnostic it is not only two meter or load or not. So every tool will give you some table statistics. Right. So do not copy and paste assets. People might not be able to understand anything because every tool is different. Every unit is different. So you need to explicitly tell you have to mention the mainly the unit if it is a response time, whether it's in seconds or milliseconds, and if it's a throw put. Right, whether it is a TPS or it is a transfer of bytes received. So you have to explicitly tell everything in the table view. But again, don't use the raw data assess just to take the raw data in your worksheet. Then use slice and dice. Then just put the relevant stuff in the email. So don't. Again, if you had 10,000 rows, do not add 10,000 rows in the email, so just slice and dice. Categorize group it and keep it as short as possible even if it is under transaction just mentioned. Okay. These are the top, the not top 20 and remaining you can attach in an email or you can put it in your hard drive or some Shard location. Then you can have that link in the email. So try to keep your email very concise. Do not write a story or a book in the whole email. So the thing is like a very Con size and it should tell you the story actually by seeing the email. Okay. So how many of you work on this raw data? How many millions of rows you have seen in your experience? Anybody works on tableau, any program you use to slice and dice. So do we get thousands or millions of rows? What would be typical raw data? Anybody can throw some light on that.
Yeah.
Table I used in there will be a monthly, yearly or quarterly. You can chat as a report, response times and everything and even plank. Also when you query it will be like thousands of rows or the key in here a filter. It whatever you want. And don't get confused with so many logs and rows. A.
Tablet is a very powerful tool. So if you have opportunity to work again, you can just learn some basics in Table and then you can slice and dice. So tablet is very powerful. And also if you are into Python, you can use like Pandas Library and then you can write some custom coding and then you can slice and dice on your own. So if you don't have Microsoft products like Excel or Google Spreadsheet, you can go for this open source solutions and also the commercial solutions. Hi Dan. Thanks for joining.
Okay. I would also like to add in what we are trying to do. We are using the Influx and Parma. So we have around 10,000 transactions and what we have done in the final dashboard to slice and dice all the transactions. We have divided segregate into the scenario part. Like if we have a one day and having around 100 transaction with that, then we are trying to show how much total time the case is taken. Suppose logins and searching and logos. So we are summing all these three transactions and trying to show that. So whenever there will be any degradation comparing to any baseline testing, then only we are deep dining into the sub transaction so that there will be no meter report. And we have just a high level of report.
Based on the intuition we can show it the want for execution.
So we are using the JMeter tool and when we are running it, it is storing all the results in the Influx TD and from Influx, the data visualization tool, the.
Okay, so the metric Ie real time you can see. Correct.
Right.
Okay. So what is the delay you will get how much delay will be there?
It is approximately 1010 seconds only. Not more than that.
10 seconds of delay will be there. So when you run your meter after 10 seconds, you can see the actual metric.
Yes. Exactly.
Okay. Profession.
Right.
So we are using the Master Play consideration on so in house build or something like commercial solutions.
No, it is the in house one we have used.
Okay, cool. Thank you.
Just one more thing we have explained as a performance tester. We know that we have the response time, we show the throughput and all that. I am really very much interested at what all the things we can show from the diner at, because if I talk about clients, they are interested in the high level reports. But if I talk about the architect, they really want to see what the diner resistance. So if you can elaborate something.
Yeah, I will tell about Dimitris. Eventually the next 15 minutes we come to APM Tools. Okay. Okay. So after this slicing and dicing, so usually I use Excel Sheet because most of the clients I work at, they have a Microsoft solution. Excel sheet is very powerful and in some cases I use my own Pandas library in Python just to slice and dice. And in JMeter we use some small bench. Sorry, not in a groovy scripting just to filter out the first five minutes and the last few minutes. So it is not possible manually manually. Also, you can do that. But we have written a simple groovy script. It will just take out the first five minutes and last five minutes so that we can have only the steady state in our results. So that is very simple to write it is not very complex. So you have to get the current time stamp and send your five minutes or ten minutes. Whatever the minutes. And you can just ignore that first few minutes. It is very simple to implement in JMeter and we just take the raw data. I'm going to just slice and dice. So the real issue comes into picture when you have millions of records. So when I was walking in Load Runner, once your test is done, it will launch your analysis. Right. So make sure all the data has been loaded in analysis. In the bottom status bar, you can see the progress. I have seen many people what they do, just open the analysis and a site away. They work on the data, but in the background the data still will be Loading for a very large run. So make sure you load everything to the tool you're using. Then you can do your analysis and also a load on a right. They have a feature like a correlation in analysis. I'm talking so you can correlate where the issue is and then you can just Zoom in to a particular area. So make sure you use all the best practices in whichever tool you are working on. So that will help you to deep dive into your statistics. So do not just copy your chart and copy your raw data and paste it in your report. So it is not useful for anybody. So make sure you do your analysis by comparing a lot of things. Compare time versus response time time versus throughput time versus past transactions, time versus file transactions time versus Http status code or build up a multi dimensional charts and bring up everything to one view and see what's going on. So this will help you to deep dive where you need to focus because you need to identify where you are getting the issue, at which point you are response time is degrading or something is failing. Right. So this will help you to deep dive into the back end systems like APM tools or database. So first you identify the timestamp, then you can do your analysis from there. So any best practices in analysis load analysis because I've been working on JMeter for a long time. I think it's been three years. I have not gone into Load Runner area. Anybody working in heavily in Load Runner you can raise your hand to speak to come to the stage. So I assume nobody is working in loader on there nowadays.
No.
They are folks working.
Yeah. Most of the things we covered the best practice apart from that, nothing much unless until you want to.
Yeah, well, don't go to that. But first I want to just talk about the tool perspective. Then we can go to the back and other tools or solutions. So from the two.
Yeah.
Yes.
Sorry when you're telling Jamie right. So Europe is different from the Loading prospective put right. They are talking about transaction throughput, I guess not the Byte size of the data. Right. So one thing that everybody has to come always in the assumption that this route is probably the same. I just submitted like away. So I got wrong. That's a yes.
When it comes to throughput each tool, take us differently. The approach is different. Jmeter is about data. A load is different. And again, load also has a different technology. But you have to go to the health section and see what that particular unit means. Then only you can analyze your test.
Hill.
Okay.
What I would like to share is most people will let's say we are running a load test. So what most people will do is they will they actually will not filter the ramp up and ramp down. Actually, they will completely take the time whatever is there. So it's not the best practice, actually. So in ramp up on ramp done, there will be some like some hits will be there. So we also need to filter. That is what I'm trying to say. If you are running on our load, we need to filter the remaining time. So that's what I like to see.
Yes. That's a very good point. Yes. I have discussed a few minutes before you join. So in JMeter, we use a small groovy script which will filter out automatically first five minutes and the last five minutes. It will keep only the steady state. So manually you may need to do for most of the tools and can also automate so that you can roll out your ramp up and ramp down.
This is a Hi Martin loader. Like, what I would do is apart from the listing, I would analyze the group date also. So if you have any, say 2 seconds or 5 seconds, our outage response time is around the like more than 5 seconds. Five in five or 6 seconds. I would like to analyze the raw data and present to the client. So 50% of the values below 5 seconds, 75% of the value is below 5 seconds. I will give the we can prepare. Nice storm and we can share with client. He'll be more interested. That's what we're doing in the current project.
I am. Thanks, man. I think you are the first person working in louder, I guess.
Yeah, I'm working on Loading about the meter I'm using for API testing with Loading using.
Okay, cool. Thank you, Matt, for sharing.
Thank you.
Hi, Sam. Thanks for joining us.
Hey, thanks for having me. No, not right now. So I'm just listening and learning a lot.
Thanks.
Yeah, sure. Thank you.
Okay.
So let's talk about percentile. So usually I present in most of the projects again, it depends on the architects. Say whatever architect wants. We have to furnish. And I have seen people working in Amazon. They Amazon, Netflix. So they use time as the SLA is the response time baseline and again, I have seen it also. But these three I have seen and you have to get from architects. So what personal they want to see in the report and he dont highlight average average will not give you the full picture. It will give you only a picture about the performance. So I always try to get some percentile from the architect, then furnish the metric. Don't focus on minimum average or maximum. Always stick to the percentile range.
Yeah, I have no point here in this country we have standard deviation. Also place main road right. So that we how our values are scattered. So whether they are closer together or they're completely scattered, the range between min and Max will be too high. Each integrity values will be fluctuating. If it is standard deviation is bad, then we can say the values are fluctuating on the constant. I think that is also a good thing to analyze.
I was about to come to next to that one. Actually a standard deviation. If it is very high value, then you are numbers are scattered basically here and there. So sad division should be close as average so that your numbers are consistent across your execution. So please focus on sin deviation and see everything is consistent or not in.
I have a question in your part. I think we have asked already discussed this one. When you talk about standard division. Is there any upper and lower limit? It's a decimal thing. It's okay if it's in the range of 1.2 points. So we should be thinking about that, right?
Yeah, it should be in decimal. And also if you are using loader, you can observe all the graph data. Raw data. The raw data also will give you the clear picture of how the values are. If you have 100 values so you can see all the other values, how they looks like. But on that also we can.
Yeah, I.
Navin I was asking about finding the deviation is different. Yeah, that's okay. But if you find too much deviation, so what's your approach like generally, apart from the test data?
For example, if the deviation is high first I focus on the first top ten transactions.
Okay.
What is the response time for top ten? And I just go to the back end and where it is contributing more time whether it would be a database or something else or some downstream upstream components. So JMeter like tool cannot help you. In this case we have to deep dive into the back end. Tools. Apm Tools SP tools. Yes. Mainly I would go to Pure Path and see the top ten transactions and I take it from there.
Also, we can highlight in our report in the report saying that the application is not constantly behaving. We can say that right. Application is not responding constantly. Bc high fluctuations between Lake kind of an exact pattern.
Basically application could be it's not stable. It might not be stable. Yes, correct.
Correct. That point. We can highlight it. Pick somewhere in the email.
Yes, I would like to give a one more point here and the application definitely is not behaving as expected. That's true. But if it's default behavior should also concentrate on your transaction level. Which transaction got more response time and just observe the payload. How much is downloaded? That also matter.
The type of this data also matters sometimes sometimes because application may be behaving right.
But you're the app server could be more.
I like to add a.
Yes.
So my go to approach is right for the test data and I re execute the scenario again just to see whether the pattern is coming or not. Otherwise I'll ask the team to restart everything. It might be due to some cash or something. Is some Invalid patch? Something could be there. So we have to take it from that point.
Yeah, that's true. As a best practice, we always need to have multiple cycles with one cycle itself with one round of testing, so we cannot conclude anything. So we need to do multiple cycles to make sure that we are observing the same pattern. And also sometimes I would say for the same transaction, for example, you're using based on the face it, I'm working in an inventor management somewhere. There's a different lot size. Ten K size, 50 kilo size, 100K size. So what I would do is I would get the response time against each data size that I input so that it will be clear for me for ten K size. What is the response time for 100K size? What is the response time? If I see more high response times also for the transaction, the strips will be the same. Only thing is data size will matter. So what I'm doing is I'll get the response time according to the test data that I've input to.
The site. This data is very crucial in execution.
The test is really playing the crucial role. Apart from that, the time zone is like when you are running the test. So sometimes the background activity which has been already seduced is interfering with your test and meeting the numbers. Very worst. So we need to make sure that whenever we are running the test, we have a dedicated slot for that in the pre prod environment so that we will take care of any background activities that you already should do doesn't come during our protest. That is also in the dilution of the and one important thing we are forgetting here.
It's also important to monitor your load runner environment. Is there any glitches set your load generators or networks? Anything that could also if it's a network bandwidth is also a concern that could also sometimes deviate in your response time. There's also another important point to always monitor your host health.
A load generator, CPU and memory applications.
I think load under will throw some error if it is greater than 80%. I automatically throw up in the dashboard direct. We can remove the load generator on the fly or we can add the no.
We cannot remove the load generator on the flight.
No, we can redirect the users not to go to that particular load generator. That thing we can do, we can manually have to do that. But then there's a feature load under that is one of the best things that's but I have not seen in other tools as of now. Probably they might, but that will mess up your tests because there will be a short drop in your load direct.
Otherwise what we need to do is we can stop that complete script and we can add one more script binding to different load generates.
Correct.
I would say if you are finding about any bootlegs and response and divisions or anything. If you're doing analysis better run a clean run. Don't lose the experiments, which may lead for assumptions which could be a potential thing.
Yes, these operas we have only for debugging or for actual run. We need a clean run. As I mentioned. Yes.
I mean in JMeter also can we get the raw data?
Yes, you can get the data.
You can go further to need any customizations or by default is out of box will get it.
No, you can just redirect everything to a file log or on the command line or the properties you can set data you want to write. So there is flexibility in that. Even you can write some correlation correlated value extracted value can write everything to the log file.
Okay, okay.
Okay.
So next after this table view again.
Yeah.
You talk about present, right presented Andy with a talk and after this table view I will focus on the APM tools. So I extensively work on Dina Tris. So anybody working on any other tools apart from Dimitris.
I've done a dynamics dynamics.
Okay. To any modern tools anybody is using like a data dog new leak, any startup tools you're using a scale APM. There a lot of tools nowadays. Everything is observability nowadays. There is no APM. Actually, we should not call APM. We should call us Observability, so only have to schedule one or another session for only on Observability. Probably next two weeks. I will schedule something on that probably also for APM as well.
Because Observability because apart from our testing, there could be a lot prod level. They will be monitoring from a scalability perspective and built to be a real company and real user monitoring. And a lot of things are there, but probably if you could take that session, that would be good.
Yes. I have asked a few industry leaders to take part of this club house, so they are to respond on the Observability space because I'm just getting started to learn those things. So I'm not the right person to my then probably I will.
Apms are observability.
It is a modern. Yeah. Nowadays all APM tools are becoming observability nowadays, even in trees. Right. It was a PM. I have seen intransa, but now it is purely into and useability. Nowadays everything is observability. We need to schedule some session on that. I will do that eventually. In Diner Trace, we create some powerful dashboards. So with the help of a DevOps team and your development team first try to create what things you need to monitor when the test is running and one thing you have to showcase to the stakeholders. So you have to come up with some powerful dashboard. Then in the dashboard, usually I have all the hosts, whichever host I'm testing that one. I have CPU, of course, memory garbage collection on the response time and then we have the what is that? Past transactions, final transactions. That one is another chart and then the database hosts that one. I have another dash. Let and then what else? These are some eight to ten dashes I have in my dashboard. So this will give you some idea when the test is running. If it is something going wrong, it will display in red color so we can just deep dive and dynamic has a lot of features. They have different different views. You can also explore that, but mainly I focus on my dashboard, and if I just filter out by time it will automatically refresh. And then we can see what's going on. And also if you are using tools like JMeter or New Load and Load Runner, you can tag your requests. Many of us will be tagging the request to filter out the traffic, so make sure you tag the requests so that you can easily filter out your request in dynamics. So tagging is very simple in loader. It is very one checkbox in runtime settings, make check box. It will automatically tag everything and in meter you need to write some simple groovy script which will tag your request and the new load. Also, you can easily tag your request in runtime scenario, so this tagging will help you to slice and dice your data in a nitric. So when it comes to dashboard first what I was in the reporting first I will go to the pure path. So pure part is a technology by Diners where you can see where in depth of what is going on in a particular component. So it will give you the call of stack execution. And also it will tell you which component is taking more time, whether it's a database or as a network or something else. It will clearly show you in the pure part. So my first two point is thing. I go always to the pure part and see what is going on for the top ten transactions so you can slice and dice based on the high response time, or you can slice the time filter. So there are a lot of aspects that have improved a lot in the recent version of the Nitric, and this will help you out to deep dive into the particular component where it is taking more time or where it is filing. So in diets itself they have multiple graphs actually. So first thing is, it will show you a response time and if you just click on the next tab, it will show you the file transactions. Again, there are other graph site will show you. So this will help you out to understand what is going on in your load testing. The first thing is pure path. Always go to pure Part and go to the respective application services and see what's there. So how many of you use Pure part? I think everybody is in pure part. Say anything I'm missing in Pure Pat Dinaric, who is working in Dina Tris.
So man working on the dining and yes, apart from the response file path like we are further driving down and going through the service flow. Also there we can see that with services getting hit and what is the extra process which is consuming the more time there? We can also see the top database statement, but the approach if I talk about what I am doing that during the notes, I am really going to the problem part. There is an option. There is a modular problem, the binary where you can see that which host or service purposes taking more time on the ice seeing for some and processes so we can directly see from there and we can jump to that one apart from that. If I talk about the analysis part, then as you said, we can prepare the dashboard, but also there is one view exception analysis there. We have multiple options there that you can use the top database statement and you can see the software request if we are seen. And then what is the exception we are getting during our notice, like the 500 L exception or any locking exceptions, we can let me see from there. So that's what we are approving right now. Yeah.
Thanks. Since you brought up problems, right. I want to talk about the problem section. So problems if you have very few applications. Yes, it will be very helpful, but you have a lot of application like Enterprise. Sometimes it will get noisy because in your environment there are many people who will be accessing your application and there are a lot of problems will be there and you have to roll out which problem is worse. Which problem is others? So if it is very noisy, problems might not be helpful. But again, you need to just deep dive into the problem section. You need to rule out others and you have to focus on yours again. This problem also, it is powered by the back end rules based on your dynamic administrator. Whatever he has set, then only it will replicate it whatever you want. It will not come up unless you can figure so that is also there. So you need to see what's going on in the rules as well. If it is a CPU. Yes, it will throw if it is 80 percentage again, you need to set it up in the rules section. So sometimes problems might be noisy. So you need to take care of that particular section.
So.
I present the results mainly from Pure Path deep dive. I go to the particular code of execution and I just see what's going on. And clearly it will say sometimes error means it will show you where is there or is which line of code or which call which a thread stack has issues. It will clearly show you. Then you can take the necessary action. You can just take a snapshot or you can copy the link and then you can put it in your report. So this is what I do for all the unique file transactions.
Can I ask a quick question?
Yes, I'm sure.
I know you mentioned a tag in request for loan for Dyna tries. Do you know if you can do the same thing for a Dynamics?
Yes, Adam is also tagging support. Yes, you can do that.
Okay.
Stage works like a header. So you are just passing some header information uniquely identify your request, nothing else. There is nothing fancy here. You just pass some unique Tags.
Okay, got it.
And that that will be displayed in your app dynamics.
Hey, Sam, you can also do a pass. No invention. There is an option for App Dynamics in runtime settings to tag the transaction. Apart from that, you can also do just add some Editors like sending the locals will be there from which local you are sending the response time that header, you can add your name. Sam. So and so is sending this transaction like that. Also, you can tag the response I'm a transaction and monitor in a dynamics.
Okay, thanks.
That will give you only your transaction, only your notes transaction. So even if some of the transaction tagged by some of the low test so it will also show one to see only your particular transaction. Just give you a name to the sender host name. It will be more accurate. That's what I'm following right away.
Yeah.
And let's say you did something like that. So I have Dynamics. I'm guessing you can filter the amount of requests, maybe per minute based on those transaction headers.
Yup, Yup.
Okay, okay.
So pure path. So anything I'm missing in pure path, any input from anybody, then place pot or an. I don't know what is there in a dynamic, but there should be some technology. There's some cans name should be there. I don't know what it is. Anybody? No, I said you work in the dynamic, right. So what is the technology term name? No idea.
There is no logic. Like I have a pure part or something. So it will give the same dashboards and everything the same thing. But they won't mention this pure part. So the way we use what are the slow transactions and what are the error transaction if you come up on the dashboard when it's running so you can drill down that's it website it will give the transaction what are the methods and everything that's it apart from that one more thing, you can also see what type of transactions like it's. A synchronous calls are synchronous calls and where in all its communicating. What are the back end features? Pretty much same, but they are not island the few pathname that's.
Yeah, that's true. We can monitor the business transaction level and we can drill down at each method and request level like which particular method or which particular request is causing high response time response. And also we can see the database query SQL queries. Also we can see what are the top time consuming in the.
This level. They have a one term browser metrics. It's like pure part only. From there it will go to the VM level and on each JVM level that they called us.
I would say the pure part is similar like protein complete Java metrics. Also keep safe relegation and a one constant question.
Even so, I have a recent face in PM. I don't know how it could be same in dates or not. So like I said, like I mentioned earlier, also, I work in micro services. So every time it's deployed into the notes different notes, it will be different, different notes. So my app is keep on tracking the notes which are already previously deployed. So that means that your app is deployed to four GMs this time and the next time another for other different. It's showing like the right notes at a time, even though the app is no more presented there. So does anyone face this kind of situation in your dashboard? And yes, of course, what is your mic?
I think you need to change your time filter. No, not because a time filter means it will show up the previous history. If I filter by time the notes which are sit down write it will not show up the update.
Not like that. So even if the notes are shut down, there is not a no point of shutting down. All the notes will be running every time I but it matters. It will also have the works. An agent level agent is installed in that. So I keep on tracking that one. So what I did like I write a query aquarium that if the agent level is 100% up for running, that's it. So even if an agent is down so it's keep on tracking there. So I filter like if agent is 100% up and running then only show those metrics like that. I wrote a query, an API. I don't know how Dina trace work, even I tried with the time frame, particular time I monitor live. I started notes keep on monitor. I started watching in a but still it was showing all the agents which was deployed in all the notes. So the way which I felt is like if the agent is up and running, then only I show those two metrics. That's what I did in I'm not sure how much an identity.
Can I take the question of.
Hey, can you explain can you.
We had similar problems like probably what would have happened. You would have included one a generic node agent across the cluster server clusters. So probably that would be the reason why every time whenever you try to pull the information you're getting for all the notes, all the service in stuff getting from one server. So we face the problem. The way to deal with that one is basically go into the transactions and services and then you select the only the particular Java server that you're interested in. So that's how you can filter it out. Other than that, the there is no solution at the a straightforward solution. I mean to say only the option is filter filtering it up.
Yes.
I also like I mentioned, we don't have the visibility. So when we can trigger the build with four notes, the naming convention is different from the notes versus actual notes, so AMD will give the support number or likewise the node name. But actual server node is different. So like that. So I asked the Ops team to filter out this. They said no, nothing. We can concentrated much. So what I implemented like if its agent is 100% and up and running, just monitor it. That's it. Okay when you configure the dashboard right there is an option for extra settings there you can mention what is the extra filters you want on your servers? Yeah, like that. We have men filtered vacation.
There is also a method and also from the transactions and services. You can also filter on number of requests like a number of request process serviced by a particular server. And there you have an option to choose based on protocol based on dynamic request or static request and based on the response code and etc. Etc. That's another point.
Thing. We have nothing like that. We call transaction score. The number of it is getting any server it will show as a transcript score for your app from there. Also you can filter it out. It's like going backwards. What I mentioned is talking about. I made a dashboard. Whenever I run a load test, it will be popping up there.
The problem related to the dashboard is another feature in Dinaric is synthetic monitors, which is a bit expensive. I'm not sure I don't know the price, but usually based on that, what you can do is you can monitor it in use of behavior. Like for example, let's say there is a search functionality performed by such functionally available application and a user can perform search through multiple options, and some such options will use the less cost and other such options use higher cost. So if you do the back end monitor, you will get to know only the such function is called. But whereas if you have synthetic monitors enabled, you can also trace which direction or which part the end user has taken to reach a particular service call. That is something included in the genetics. But I heard it's a bit expensive or something. It's not a default feature.
It is like a real user monitoring. So what's the difference between a real user motoring and synthetic monitoring Rakesh?
It is a real user monitoring end user monitoring process. Yeah.
Okay.
Because you don't see the right. So that's why I want to they call it a synthetic.
The name is the feature name is synthetic now.
Okay. But again, it's actually from the real users, not actually from the real users.
Yeah. Okay.
I just want to clarify a I have a question like.
Do you have any feature in Dana trace like code level? Which line of code is causing problem? Which method which art sports a.
Yes, we do have you I was mentioning right. Thread stack call stage can see in pure path.
Okay. So will highlight when you drill down.
Yes.
Which line of code as is it will show you.
Okay.
Got it.
It will have the whole stack information.
So probably it also shows like thread States and everything.
Right?
I'm not seen thread state. No, I don't know.
I'm not into showing your date will show like how many Tes are waiting for the method. Water in red level. It will highlight in red if it is any long run crits are waiting there that feature we have an empty when you drill down.
It might be to take dump on dining. As I don't know, I never take.
We can't think that using the diner it's only I've never seen that feature today is talking about. We can see from my horse, but the pain can be able to see that.
Okay. Not from the part. It is from method expert. Right.
So in the level it's like if you find one critical transaction which is taking a high response time, it will it will take the snapshot of the transaction. So there is a two, two notification will come. But if it's partially take screenshot or if it's fully taken screenshot, we should check always fully taken screenshot. Then drill down. Then it will plot all the drill down, whether it's a method level or any query level where it's getting slow down. So it's like going down exactly the particular transaction.
It's a car crap.
Right.
Sit.
From the call graph. You can also see him or which node it is getting cost. And you can also monitor what is the health of that JVM. So that level you can monitor. But if you see any men like a skill thing. You can ignore that car graph.
Yes.
Indian Res. Yes. I have seen that host level metric level metrics I have seen again, we have service back price. We have where we can place the service calls, which services calling upstream and downstream component. So that view will give you that picture re backtrace and they have a lot of HEADtoHEAD comparison.
Yes.
Both the tools will have almost everything. I guess only the terms will defer.
We can say it like that. Stop on approach to.
A and so another metric I take is as I mentioned, right. Cpu memory and garbage collection garbage collection. Again, it will show you what is the response time of each GC that will show you in genetics clearly. And also it will sometimes JVM might crash right. Whenever you do some stress testing. If the JVM is crashing, Ie race will capture that information, then it will show you detail what it scratched and which time and what JVM. So those things also it will clearly show you in the dashboard. So that is also another good thing you can capture and then you can showcase it in the report. I'm sure a Dynamics also will have that feature. I guess. Right.
Yes. And when we have that one and how about the response times would you elaborate with the load first time the response time.
It will be varying reference. There should be some variants because loader captures. If it is a true client, it is end to end transaction. If it is a web based protocol, it will be a backend transaction again if it comes to a PM again where your agent is running so that only it will capture. So there might be variance between each component level response time. But I don't see very huge deviation. If it is very huge, something you need to deep dive, but it should be negligible difference might be there.
Sure.
But when your reporting I mean.
Do you report from the load runner or you report it from the generic?
Both both actually. But my main first preference would be my tool. Yeah, my tool. And then if something there is an issue, I use diaries as a secondary just to validate. And then I present my actually only for the problems and issues deep diving APM tools and for the response time. I use the tool.
Yes, correct. We only present slow aging transaction column in the spreadsheet tab we have what are the slow running transactions and there we can present this ad level redress for metrics. Why it's creating cost that's it not to answer exactly your questions we don't present in response time versus a response. Don't get confused.
Don't make.
Content for the tab where you are doing in depth analysis. It's required if it doesn't or else just ignore it. Otherwise there may be people who will be looking every tap so they will end up asking all the questions you need to teach them to. You have to end up in a situation where you have to explain a to Z.
So usually what I do right. This question comes up right. I just arranged by descending order top ten transactions saying Dina tries. I'm getting 1.5 seconds, but in the tool it is showing like 1.7 seconds then almost matching the angle difference. So in that case you are good. But if it is a very huge difference, then you need to deep dive. There is a huge difference between the two parameters.
Yeah, but that is a win where you submit two reports.
But you always concentrating on your tool report to.
In too. There could be a C you're not presenting all the requests in your report in your request. If at all, any one thing could get high, then you can present that tab as a extra add on. Otherwise I would say apart from the and resource metrics, don't give a the response to this and that.
Yes, it exactly yes, something is sent out there. And here's the top few transactions. Yes. No need to drill down everything and presented in the report. That is not useful to anybody.
Why I'm stretching this point? Because now in because I have ended up in a situation I was thinking like a it could be useful for any other people who will be looking in. But in some situation I ended up in hot spots saying that while Louden is taking this one while Geometer is taking this one. So I said I'm not a lot, but I'm not a geometry. I know how to drive them, but I didn't meet them. I just escape from a situation like that. So stick to the point. Like a Ben said, just give the information what is exactly required. Don't give extra knowledge.
Too much information. Also dangerous sometimes.
Yeah.
Agreed. I didn't remember. I agree with Navin point how the Dinaric installed is more important. We have done a failover testing where servers were shut down and we have noticed the address in performance and load run now. But the time difference where we have observed there is no activity or server shutdowns were there. There was a tier time difference between a Dinaric and the performance center, so I didn't remember exact root cause. But the root cause was something wrong with the way the Dinaric node agent was installed. I don't go with the exact route cause, but what I want to mention is you'll always find some differences the way they were installed and with the configuration and stuff. So I think it's getting one unknown, one one compassion might be difficult, very rare. I think at my level like you will simulate the second correct.
Yes.
And I mean, I have a question like do you people come across a situation like developers or develop guys saying that you APM is the cause of causing some performance deviation?
I have seen overhead is there, but not. I think it's two percentage something like that in the official documentation. So you can see this note, but I have never seen a problem. A person has a bottle like no, I have not come across anything like that.
Something you can reserve this by running one test with ABM to confer and without that pay.
Also you can go.
Yeah, that is like POC and substantially they do. But we have a development environment. They completely removed the ad. So they think that any APM is worded for in their environment like development environment. It makes sense because a they don't anything.
I.
Use a new relate but there is no red. It's similar in milliseconds depends there.
As with dynamic, I see around near total of forehead, maybe hardware relative which I found out. That's why the remote entire API monitoring for one of our main test.
Your head will be there. Any agent based solutions?
Yes.
Even I have taken many heap dumps, so I have seen the objects holding in combiner. So there is a overhead always.
Too many tools redundant because it may be true that you may be observed for your app. In the broad level. Apm is different than some other log monitor tools like C. Even if you take blank, you can do whatever APMs do right away. Two days now. Too many tools definitely will cause overhead. So that is also it could be a reason in perfect performance environment. We use Ad, but in the developers and it they only use the plank. So it depends.
That's why we have solutions like White rash. Right. We had a Ram right a couple of months ago. So the white grass like tools. It does not have any overhead because it run independently, but it monitors your performance. Jvm performance mainly and it will take a dump and it will present some report. You can take some actions on that. So it is not an agent driven solution. You're talking about Ram the one who developed the White crash.
I think he was in Chile.
He joined when he left.
I thought of talking to you because a lot of time from thanks to saying thanks because I read a lot from his yeah.
Actually we are coming up with some Zoom session probably next month.
Yeah.
I will update my post, give some announcement. We're coming up from free session. Everyone can register on an you can join it's about JVM performance.
And one more thing. I think I've gone through your recent video about the algorithms, new algorithms. It's pretty much everything there, but if you take one more session here, that would be more. I mean, if anybody missed that one.
Yes, sure. Yeah, sure. Yes.
It was very nice.
Thank you.
In the Am. Currently I have a question. I currently I'm working on a Micro services which is as a model for the Lambda. So anyone using any APM tools for the currently I'm using only Xray, which has some limitations. So any solutions on.
We are using the internet like I'm also working in micro services, but it's in the Cabernet, but I'm only using this one. Emily, but there is a proposal of primates. They are going to start soon.
Yeah.
So I don't want to be in between PCF and PS like total cloud Foundry versus Net. One thing I observed in violating this one, it got better performance like 100 milliseconds for better performance, same micro services, same node and same everything. We have checked in each method level, but even it is better than another cloud boundary. So they are proposing the families. So maybe I'll let you know once. Yeah.
Our application. It's a new platform so we don't anticipate the load.
So that's why.
I mean initially we are in Lambda. So maybe in the future we'll be moving to EKS or easiest.
What difficulty is like this one the Cabernet anybody using which they found better performance when you or any.
Oh yeah. We are moving to a case. Not your case. As your only we are removing migrate the Kubert.
Something related to the DevOps and orchestration platform. Does it also do the application performance monitoring as well? I have not used Kubert. Yeah.
Somebody mentioned last minute time I asked the question. So they are using premature other or somebody that they are excessively using for monitoring like a cloud monitoring. And it's like containerization. Now you might have heard hearing containerization. Right. More electric to container.
Yes. We also use the sewers involvement or less timeless. But yeah, we have the same issues.
Sorry. We.
Use only. Yeah. And then the restriction we can pull up to 10,000 logs or something like that.
We have another tool is a play which will give a very good insight, but not me. It will be restricted to resource level. Armenian resource level only. So we can get the latency resource is whether it's a gateway Lambda or if you are using any art or something.
Right.
I mean, it will give a resource wise. So compare with a cloud watch. I mean, this will be a very good and I see there is a good development lasts two to three months. They are adding more logging or monitoring letter in the as things. So I think they'll consider.
Taking to the topic. Do you also multi the gateway like the proxy or whatever? The gateway a gateway, the trace in the APM level.
Yes. First the entry point. Right. It will be my gateway, actually. So that gateway. Again, we monitor mainly how much the request is processing and what is the bandwidth they have. So that gateway right. Because that gateway is the responsible to distribute the traffic, which requests should go to which component or which container. Right. So we monitor heavily on the first thing and usually we don't see any issues because it is properly scaled environment and properly maintained so I never see any issues on that. But we do monitor that entry point to give you a tip there.
Look into the load balancer config file gateway country. Sometimes the timeouts and errors which they configure differ from that your app configuration. So there I have faced and I have apart from that, I never looked into the load balancer gateway anytime. Like you said, the mentor to balance the load.
A balancing road and redirecting traffic to the right component. So that is also a thing. It will take care. Yes. I never seen any issues on the networking that distribution. If someone was talking about interrupted or that.
Okay.
Go ahead.
One, you can go ahead to one.
Okay. So I just want to know about the best practices regarding the benefits. Like we have our two types of licensing infrastructure mode and another one is the first step. So if we talk about the record or the first environment in the database, also like everyone is having the infrastructure more only or you are using the full set there. Also, I just want to understand is that in 42 seven on your pending of that just to see if the cost because we are just running the load test and we want to monitor all the things during the Caston. And apart from that, we don't care. So like how you handle that licensing part?
My project is on premise right now. We are using PCF to maintain the instances and everything is 24/7 for me and we don't turn on turn off and it's infrastructure mode. So whenever I want to run some tests, I can just go ahead and then I can monitor. No one will ask any questions. It always 25 B one for me, but we are moving to as you're right. I am selling. So we're currently planning to use Rancher. Rancher is again Kubert management platform. So once you move to Rancher, it might be like based on the usage we may need to spin up or down the enrollment.
Okay, but an improper board. I think you can see the code level. You can see the resources, CPU memory and all that in the first set. Monitoring only. You are able to see all entities like the pure Path and the method at Port.
Okay, that's it. I think it is not based on how many times I don't think Diana is pricing based on that. I think it has a number of agents. The number of notes, right? Anybody can pitch on this pricing part dining place.
I think it's based on the data that you collect. That's what I had something. How many GB of that they got as a quote or something like that.
So you are not based on the number of agents you are installing in a particular node?
No.
I think it's based on, but I will double check on it.
Yeah, I have seen.
I remember a number of agents, basically a number of notes you.
How much Ram you have. Like if you have 16 GB of Ram man, it will cost of two licenses. One is equal. One license is equal to eight GB or 16 GB something like that. And if it is a 32 GB or a one agent will be, then the two licenses will be completed. What the food set monitoring. But if we talk about the introspection mode, even the Ram is 1000 PSI will consider it as a from the license.
But in reaction mode again, a number of how the license works. A number of machines.
Yes.
The scientist is expensive. Very expensive because I have seen some working for previous clients they cannot afford. Then they started developing their own APN station. I have seen that in my previous.
So that's why that's the question. Why? Because we are also getting so a lot. Of course it is in the production alum and free fraud out. A lot of license cost.
Is there how we can do as far as wondering why companies investing in developing a new APM solution when dining trays or new AK is there because the cost is very issues on the five years down the line. If they have their own APM, they can save a lot of money. That's why they started investing developing inhouse solution.
Compared to Dine, I am a little bit less, not a little bit better or lesser. So that is one thing. Most of the companies going for APM if they can't spend much money there in the interest. So same features. But you can get as much as what you get from the dentist and developing a ligating first time. That is a smart way.
That is smart way, but it will take more time. That is not today developing tomorrow I can deploy in production. No, I cannot do that. It will take two or three years. Right. Many iterations. It should be on projects, so it will take some more time. Basically, you are creating a company, a company within a company, right?
Yeah.
On here. I do have some questions. Hi, everyone. Well, first of all, thanks for letting me in to this club accepting my running request. The question that I had is restriction restricting this conversation only to Dina trays and other reagents. Because that is yes. Then obviously we talked about new Relic as well. I just wanted to understand since we have been more centric on Java, JVM based deep diagnostics observability and tracing and so on.
So.
Is there any dot net PM observability irrespective of the tool hack or cheat sheet, especially, you know, when your stack is hosted on, you know, as to cloud environment with cloud formation and all those components in the same place. So is there a, you know, certified and very well known performance PM monitoring strategy in place for Net applications? Because, as we know, you know, there are a plethora of tools, both open source and commercial. Right. So this is just a question that I had.
Dot net. I have no idea. I never worked on dot net enrollment, but I.
I.
Can answer one, right?
Yeah.
Same thing. We can monitor in a dynamics, but I have done a dynamics in Net combination, but exact combination. What you mentioned like EC two Amazon I used for dot Net and it's for Amazon cloud watch. So that was the only PM I was using. Apart from that, I didn't get especially for Net projects. I'm not sure about Dyna trace but App dynamics. Yes, you can monitor Net projects. True and Dynamics. I also monitor like I mentioned failure. I also monitor PKS like Cubans.
So I had a quick look up DeAndre support dot net monitoring.
Sorry. The question that I see in my current project or the exposure that what I would like to call is we have been extensively using new Relic for the end of year for most of the nations, which also includes RDS and no sequel databases. Like, for example, Snow Flake, and we are using some logic for log analysis and its store certain extent wherever that is possible. But challenge again remains in terms of transaction traces to depth transaction tracing, especially pointing from services to the my note sequel hot spots and procedure performance. So I do see that there is still a gap in terms of the traceability irrespective of the depth level of the agent that is being configured to monitor across the notes as the application service. But when you talk about JVM Java diagnostics, it's pretty evident that, you know, you would basically kind of will be in a very flexible I would say a very comfortable place to actually pinpoint what exactly is causing the bottleneck to the Dot. And because dot net is supposed to handle the GC garbage collection memory and all the stuff automatically based on certain algorithms and talking about the latest client side technologies, like for example, if it's Reactor, Kendo, UI and all those other technologies makes it even more difficult. This is just my observability. Still, we are not there in terms of actually saying making the developers more comfortable to understand that this is where the model.
For that art tool will be useful. I also don't work on it, but are hired about it.
Which one.
You R-K-I-T.
Okay, your it is more of a code level profiling tool that is more comfortable from the single user perspective.
Correct. One profiler tool actually profiling I.
Have dynamics. It will give in depth level whatever your set it will give. I'm pretty much sure no.
Really is quite capable enough to actually give you a lot of stats. But the problem here is when has what being talked about, right? I mean, there is a heterogeneous in their architecture where to upstream downstream micro services ideas, especially when if it's on a cloud. Right. When having a a bit of monitoring it through cloud watch. There are lots of metrics are available to measure, but under load it becomes very chaotic. For example, you cannot do an Apple to Apple comparison. The transaction level developer might say that if a developer uses, let's say Locust or any other tool, one the same for his local machine, and when he monitors Itani, his development and went online, you would get a different stack. You're basically trying to test it on your pre production like diameter or any other to load runner or whatever it is will get a difference. Multiple reasons.
Yeah, I would say types here. So first you have to pick your app performance first, if at all your APM or new relic or it's not meant to monitor into end, what is the back end, front end and the DB level, cloud level and even front end whatever signal you said. If I were you, I would say I will monitor the app performance first, then by doing the same road test, I would monitor using cloud watch like back end performance. So if you don't have any big APM which monitor and depend with available resources, you have to monitor separate separate level only.
That is why not observability you are telling you part by part, right?
Can be a this part is like he doesn't have that like any IPM which will monitor into end whenever the problem comes. So he was pointing to our APM or Observability, but seems like he has only new relea right on.
No, actually, I just wanted to not sure whether we have sufficient time, but my point was simple. Point of going for any APM tool is to ensure that we so that all the telemetric have been logged into a centralized location or a dashboard across the components, whether it's TCP or driven or Http driven protocol or whatever it is. The object here is to capture each and every metric across the components and present it in a single dashboard where all the operations and what all the its folders are. Actually they should be able to have an access and look at what is going on. So that is the basic objective of any P two that basically should cover observability should cover the diagnostics, should cover the tracing, and then also what we discussed in this topic where it's all about better reporting and better presentation. But at the end of the day, both the business and then the operation should be comfortable enough to say that this is what the the cost of a particular transaction is. If this particular transaction goes to your production without certain things being addressed, this is the particular cost that will be impacted. And this is the attack score. And how are we going to mitigate it? And what is the cost of impact ratio? All those things should come into play. Otherwise, what is the point in having a hybrid PM solutions like whether it's App Dynamics or Dinars or I mean again, we do this.
I think too many things for one here. So calculating the things. See, the main point of AP is to pinpoint bottlenecks so that's to help to take that help you're using APM and the cost effective are going to mitigate this problem. I think no Observability tool or APM tool will do that thing. You're clubbing all points together. But as per my understanding APMs or Observability to identify the hot spots so that rest of our Arctic and developers performance engineers will come into the picture where they will find out the solutions and do it. I don't think the dynamic will not have any feature like such way that which feature you are talking like whatever he asked, what is the cost effective risk? I will be going to the Pro without doing all these things cost effective?
I don't have access to that level as a contractor, I don't know.
Sorry. Actually, there are a lot of things when we discuss about cost effectiveness. I mean, it depends on depends on I'm not even going to the cost. Actually, all I'm trying to say is even with having flexibility and benefit of, you know, using any hybrid APM tools, let us not put only no link or Dyna rays or a dynamic one. Having a benefit flexibility of using APM tools are supposed to do its job right. Whatever the cost that we have spent. But at the end of the day because ultimately we all know that we are in he lept into the DevOps and all those things. Right. So that particular thing has now been sliced down to Aim Ops at each and every section where detailed minute monitoring of micro services, whatever the monolith to talk about makes sense. Now we need to have a complete and present track of what is happening at the right from the development layer to the integration, the product, the Pro. So there are multiple things that I just wanted to. But again, just stop me when it is out of the boundaries of the scope of this meeting of this composition. That is just the question that I just had is is there a best solution or best recommended APM tool for dot net based applications with the latest trends and technologies from the front end perspective is what I said. Pigs can do UI, obviously JavaScript, so on and so forth. So is there any free solution one.
Is there any background noise from Seattle?
Yes.
Okay.
Is it too much? Is a building? Is it okay you can talk. Go ahead.
Power for your requirement. It is best to offer something open source and have the developers create the capability of tracing the calls or transactions from within the application. For example, if I have to give the example of an open source APM tool, it's elastic APM so it has agents for Net so you can cover the basic tracing over there and on top of that. Your developers can include something like a trace can implement something like a trait that will help you was even further like end to end right from the browser to the database or beyond that. So if you want something that will cover right from your UI to the very end of your application architecture, you will have to develop it in the house. Dignitaries or other tools. Commercial tools may have very good features, but as you said, if you want to cover everything, even these tools commercial tools allow you to extend them. You can write your own integrations and within the application and have the tracing capability built within House. So I guess most of these big companies, including Banks, do it. Open source testing tools come out of them. Like for example, Zip in age. All of these have come out of Hoover and Google, so they develop them inhouse. That is what is required if you want to trace every minutest part of your application.
I did see Navin Ages video and it was quite helpful, but I don't think it was supported on during that time. What is the other tool that you sent?
I can.
Again again, he is also the one I can only go on. Okay, I think I Twitter. I guess Twitter, the developer, Twitter or Uber. I forgot, but it's from their company.
But I'm not sure anyone is using new relic here in this channel, but if someone is using it, I mean it's as good as any tool. I mean, we do have you know, you can certainly do a lot of things right. From we are on to synthetics to browser to application level and all kinds of monitoring have been covered. So when if you want something to be incorporated in house, you know, modeling such a hybrid tool is going to take a lot of development effort, a lot of trial and run effort, a lot of white papers and all those stuff which itself is going to take a lot of years.
The best tool if you ask me right, there will be no best tool. Which tool is fitting your need that one you have to want to concentrate.
I.
Would say we are talking about now how about before APM, how we used to do right? It's like that simple thing. If you're going for without VMs, you have to utilize the free tools available and where it will fit and you have to dissect it and monitor it. That's it.
Many people ask me geometer is good. Loading is good. People ask me most of the time. I don't have answer for that same thing here with APM or its observability. I don't have answer it's.
A.
There is no app.
Yeah, actually no, I don't think it will. There is absolutely no question now that the AP cannot exist, because if that is the case then performance will not exist. No.
What I'm saying is APM has been enhanced to Observability stack. That's what I'm saying. It's more transformed. The two observability APM is giving only for example. What is my heart rate? What is my blood pressure?
Right.
But observe it. You tell me why the heart rate is high. Why is the read Raster design? So that's the difference between APM and Observability?
Yeah, got it. That's correct. But yeah, that's correct. But all I'm trying to say is you know how it has been transformed right now.
Right.
So MLP has been integrated to each and every stuff. Maybe. I think down the line we might even not need certain things attain features.
No.
Correct. Actually starting to observe. Right. Once the session is over, it just go to Honeycomb, IU those guys as neat a blog and so many white peppers on this observability comes. So they have a long history. So it's a good rate. You can just try that on honeycomb.
I made a note.
Honeycomb. And light step again. A light step. Also, you can check it out and then observed in again. This is a new company. So these are the new players in the market. So we are going to they might take this tools like theatres of new relic. Eventually. There's a lot of scope on those tools nowadays.
Can you please share this chat window or something?
Yeah, this session is being recorded, probably in the YouTube channel. I can just add these links. What? I told you right. I can add this in the description. I can check it out.
Yeah, that comes.
Okay, so conclusion is make sure you just present only whatever things is required and be concise. Your reporting part. Don't put all the information. So first part should tell what it is, why it is, how it is on remaining. You can talk about your statistics, entries or whatever collect from the APM tools and you'll be good to go, but make sure you compare with your previous reports. That's what I do.
Sorry, I just had one question before concluding this. So there are two things. One is talked about in some other video. You said this kind of deprecating the support of Beanshell programming in JMeter.
Correct?
Not the intact version. Eventually they will.
No.
BS is still supporting its still supported actually.
Okay, but you said they are planning to do that eventually.
Eventually means I don't know how many places I take, but eventually the not going to focus on the bench because Ruby is the new thing, right? It's performance race. Groovy is the better compared to Bean Chile. Better to start writing in a Groovy than Bianchi.
Okay, plus we have a lot of our test suite running on intel.
It is not performant Orient for high load. That's not the right thing to do. Alright, groovy is being used in many tools like Jenkins. They use Groovy groovy because of the performance angel. Again you need to compile the script every time JMeter has to compile it every time, so it will take some overhead will be there. But groovy. Again, you can compile once you can keep using your Groovy scripts from the cache.
The final question I have is is there? I know it depends on PM to PM, but is there a standard template of how the AP matrix can be represented and how it can be correlated to the tools specific comparison. Is there a template? Let's say, for example. So if you're using JMeter and if you're using Binaries, for example, is there a standard template where we can represent the report and how the ample Temple comparison can be made? Because at this moment I know everyone has their own methodology for reporting, which is more of a.
Why do you want to compare a geometer with Diets? Is there a reason why your comparison is required?
What you talked about earlier, I see. Let's say if I want to break down a transaction response time, say you as you are aware that you run a Lighthouse in single user run for a particular let's say you have an SP application, then you run a single user light house report, and then you do an audit of a particular page, and then let's say it gives you extend it. But then later, again, you do some load testing on it that particular transaction from the UI component perspective, whatever the methodology that you are aware of, then you try to report it from the transaction to the every transaction based response time. Similarly, when you do, for example, let's say you have a particular header where you have a header, name and key value. Pay for neural for that particular transaction, which is kind of hooked into neural. Right. So then due to some reason, there will always be a difference in a lighter note. And then some transactions will have a heavier note of variations. Now then again, as what you said, we will have to keep a close track and download the metrics and then analyze what we normally do is we actually use some logic to kind of centralize the locks and then to analyze the locks later. But again, I just wanted to check if someone does have a standard way of reporting it, where it is quite helpful both to performance architects, technical architects, and then the business analyst, because at the end of the day, you know, we are talking of two different industries. Right. So one is a product driven, and then one is the traditional service driven, but the service to one where we are kind of representing a customer. So that is where the more of flexibility, more transparency and more of now reporting depth comes into play. So that is the reason I wanted to check if there is a standard template or anything.
So are you talking about a performance or just back in performance?
No, I'm talking.
Okay.
Endurance, stack and stack. But then again, I agree that basically depends on the environment, depends on services, and all begin with. Is there any standard template?
Standard template I never seen because probably I can create some base skeleton and then I can chart it my blog. You can check it out, but I stick to one tool to present the report, as I also mentioned. Right. Many of us do that. So if it is a J meter again, present meter and deep dive into the APM tools. That is one strategy. Or if you want to present a head to head comparision between JMeter or Neural. Yes, you can do that. But you will see some variance between the metric. Okay, but when it comes to UI performance, since you mentioned Lighthouse, right. And see Light House, they follow some waterfall method. Correct, because they follow the W Three Console team standards on whichever tool you are using, like a DNA test or Neural. So Dieters and Neroli. Also, they also follow the same W three standards, so you do not see much difference when it comes to that waterfall tracing of all your UA performance. So in this case you can go with Lighthouse or you can go with Dinars or new leak. Okay, because both are following the same standards, so there will not be any difference account. If there is a difference, then something is wrong, mainly due to how they configure your instrumentation. That might be wrong in the APM Tools standard template. I can create some base skeleton and then I can share it in my blog. Probably you can check it out, I can put it in GitHub and somebody else can enhance my reporting structure.
Oh, thanks. Thank you.
Probably we have to conduct another session on UA performance with comparing with JMeter or True client and the Lighthouse and in Trees fully. We can also do that and talk about that.
Can I suggest some other topics as well as Fox? Is that concept pictures like in a dope of a concept and a.
Sorry who kind to the wood? I was just asking usecase.
Everybody is moving to Cr CI pipeline and I just want to understand does the performance is a performance as a scope in CCI pipeline?
I've been doing it. I've been doing CCD performance.
So my question is how frequently you do and what level of test that you just do the benchmark test or do you do the large volume test like.
Yeah, we can talk about. Usually we go with regression testing, test testing or Indians the typical load testing last session.
I think we discussed this part, right? I mean.
Not the yeah.
It'S OK. And it's good to ask in no problem. I like the term you invented it.
So there are some blogs people are using it. So dessert, DevOps and loops and things like that.
Yeah.
To.
Be.
Is the performance testing is not a complete, mature, full end to end performance testing. That's what last time we discussed like now and again. Maybe so their perfect. Maybe there could be a chance of the involving every performance aspect into the CCD.
Right.
Okay. I'm coining a new word devise.
Performs a.
I.
Did find one. I think it was Scott Moore or someone who commented in the LinkedIn saying it is Rick Devise or Pop or something.
Okay, a lot of permutation and combination at the end. I don't know what they are going to make. Performance as holiday series are coming and now there are different different technologies.
So.
Sticking to the point topic, any PMS.
Right.
So two types of requirements. I do see that APM performance engineer like Abang performance engineer. I don't know what they mean. I only stick to the any tool performance engineer or how do you think?
I think its a tool. Agnostic is not stick to the APM tools. Apm plus testing tools. It should be.
No, I have exclusively seen like an.
Administrator kind of role.
No, not even I got it. Let's see. I asked the same question to the recruiter.
No.
So using APM only APM to be monitoring the performance of your app. Maybe the broad level.
I guess so.
I said it's kind of not kind of my style of job.
Got it. I never got any something here.
So then I came to know that we are not only using people as a performance, not only using and some other people like broad level or see some analysis uptake levels. Guys, they will be using the app as well.
Right.
I.
Think we are using a limited part of Performa APM.
Correct this. Now I read on Reddit thread. I think yesterday people who are in re roles, they don't want to do development because it's too much of a stress for them. So they are just selling management. They do only fire fighting the job. They don't involve in any kind of development.
Yeah. It's like a developer becoming everything.
Everyone that's it is this too much, right. They have to develop. They were to fix. They had to monitor. We need to make sure everything is running fine. So it's too much stress for them. So they don't want to do a development. They don't want only focusing on of seated tasks.
Yeah. Yeah.
Yeah.
So one of our managers said when they talked about CCD will be taken care of every performance testing.
Okay.
Let us fire fire of us and everything pretty automated. Then they said, no, we want you to utilize CCD in your as much as possible. That's what the cancellation or.
Okay.
Cool. Thank you everyone for trying this meeting. So I will put the recording on. It is processed, and I will also develop some base skeleton and I can put in my GitHub repository. Yeah. If there's a change is needed, you can just submit a pull request and then I will accept it after reviewing. So next week again, we will meet. I will come up with some other topic. Until then. Have a good night. Have a good day. Thank you.
Thank you. Bye.
Thanks.
Bye bye.
